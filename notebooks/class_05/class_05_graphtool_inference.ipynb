{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdababc",
   "metadata": {},
   "source": [
    "# Class 05: Community Detection as Statistical Inference — `graph-tool` Deep Dive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb47a4d",
   "metadata": {},
   "source": [
    "Goal of today's class:\n",
    "1. Community detection as inference!\n",
    "2. Compare and contrast the abilities and syntax of `networkx` and `graph-tool`\n",
    "3. (further) Highlight shortcomings of modularity\n",
    "\n",
    "__________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057eacde",
   "metadata": {},
   "source": [
    "0. **IMPORTANT** -- If you have not already, follow the installation instructions for the `graph-tool` environment we'll be using.\n",
    "1. Come in. Sit down. Open Teams.\n",
    "2. Find your notebook in your /Class_05/ folder.\n",
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0233fa5b",
   "metadata": {},
   "source": [
    "**Builds on:** Class 04 (modularity, Girvan-Newman, and why naive community detection fails).  \n",
    "**Focus today:** How to use `graph-tool` to do *statistical* community detection (SBMs), with a gentle but thorough inference primer.\n",
    "\n",
    "> `graph-tool` is a high-performance network analysis library (core in C++), with state-of-the-art algorithms for *inference of mesoscopic structure* (communities, hierarchies, overlaps, layers) — largely developed by Tiago Peixoto.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b6c08",
   "metadata": {},
   "source": [
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "1. **Introduction to `graph-tool` graphs**\n",
    "   - Create graphs, add vertices/edges, store attributes with property maps\n",
    "   - Filter and \"view\" subgraphs without copying data\n",
    "   - Draw graphs with good layouts and meaningful styling\n",
    "\n",
    "2. **Translate common `networkx` code into `graph-tool`**\n",
    "   - Load data, convert graphs, keep node labels / attributes\n",
    "   - Compute basic network statistics in both frameworks\n",
    "\n",
    "3. **Preliminary understanding of the statistical inference viewpoint**\n",
    "   - Likelihood, prior, posterior, evidence (marginal likelihood)\n",
    "   - Why community detection can be a *model selection* problem\n",
    "   - What \"description length\" (MDL) means and why `graph-tool` uses it\n",
    "\n",
    "4. **Fit stochastic block models (SBMs) in `graph-tool`**\n",
    "   - Choose between degree-corrected vs not\n",
    "   - Use nested SBMs to infer multi-scale structure\n",
    "   - Quantify uncertainty with MCMC and marginal membership probabilities\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc17d6",
   "metadata": {},
   "source": [
    "Feel free to look over the Statistical Inference Primer section for a review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57269ba",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "0. Notation, logs, and \"what counts as a model?\"\n",
    "1. Setup: a dedicated conda environment (graph-tool + NetworkX + Jupyter)\n",
    "2. Mental model: `networkx` vs `graph-tool`\n",
    "3. Graph basics: vertices, edges, directedness, multigraphs\n",
    "4. Property maps: how attributes work in `graph-tool`\n",
    "5. Building graphs from data (edge lists, pandas)\n",
    "6. Converting `networkx` ↔ `graph-tool`\n",
    "7. Graph views & filtering (largest component, masks, subgraphs)\n",
    "8. Visualization: layouts, styling, exporting\n",
    "9. Core descriptive stats: degrees, clustering, distances\n",
    "10. Null models & simulation: Erdős-Rényi and configuration models\n",
    "11. SBMs: from sociology to modern network inference (plus scaling/runtime comparisons)\n",
    "12. Statistical inference primer: Bayes, evidence, MDL (with code)\n",
    "13. SBM inference in `graph-tool`: `minimize_blockmodel_dl` + nested SBMs\n",
    "14. Uncertainty & MCMC: sampling partitions, marginals\n",
    "15. Case study: PolBlogs + posterior predictive checks (and exercises)\n",
    "16. Community detection gallery: a few very different networks, fit-and-draw\n",
    "17. References and further reading\n",
    "\n",
    "**Appendices (optional deeper dives)**\n",
    "- Appendix A: NetworkX ↔ `graph-tool` cheat sheet\n",
    "- Appendix B: `graph-tool` datasets and Netzschleuder\n",
    "- Appendix C: Weighted graphs in `graph-tool` (distances and SBM inference)\n",
    "- Appendix D: Practical tips, performance, and common gotchas\n",
    "- Appendix E: Modularity, `ModularityState`, and why inference is better\n",
    "- Appendix F: Overlapping communities with `OverlapBlockState`\n",
    "- Appendix G: Multilayer (multiplex) networks with `LayeredBlockState`\n",
    "- Appendix H: Link prediction with SBMs (and a path to uncertain networks)\n",
    "- Appendix I: Network dynamics in `graph-tool` — SIS epidemics + spectral thresholds\n",
    "- Appendix J: Graph algorithms every network scientist should know (with `graph-tool`)\n",
    "- Appendix K: Centrality (PageRank, betweenness, closeness, eigenvector) in `graph-tool`\n",
    "- Appendix L: Motifs and motif significance (triangles are just the start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2232b73",
   "metadata": {},
   "source": [
    "## 0. Notation, logs, and \"what counts as a model?\"\n",
    "\n",
    "We'll use a small amount of notation throughout the notebook so we can connect code ↔ math.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10bacd",
   "metadata": {},
   "source": [
    "### 0.1 Graph notation (you already know this, but just so this notebook can stand alone as a resource!)\n",
    "\n",
    "A (simple) graph is:\n",
    "\n",
    "$$G = (V, E)$$\n",
    "\n",
    "- $V$: set of vertices (nodes), $|V| = N$\n",
    "- $E$: set of edges, $|E| = M$\n",
    "\n",
    "We often represent a graph with an adjacency matrix $A$:\n",
    "\n",
    "$$A_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if there is an edge } i\\to j \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "For **undirected** graphs, $A$ is symmetric.\n",
    "\n",
    "\n",
    "\n",
    "### 0.2 Degrees\n",
    "\n",
    "For an undirected graph, the degree of node $i$ is\n",
    "\n",
    "$$k_i = \\sum_{j} A_{ij}.$$\n",
    "\n",
    "For a directed graph, we distinguish:\n",
    "\n",
    "$$k_i^{\\text{out}} = \\sum_j A_{ij},\\qquad\n",
    "k_i^{\\text{in}}  = \\sum_j A_{ji}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158e87c",
   "metadata": {},
   "source": [
    "### 0.3 Partitions / \"communities\"\n",
    "\n",
    "A (non-overlapping) community assignment is a function\n",
    "\n",
    "$$b: V \\to \\{1,2,\\dots,B\\}$$\n",
    "\n",
    "where $b_i$ is the block/community label for node $i$.\n",
    "\n",
    "> **Important:** The numeric labels $1,2,\\dots,B$ have *no meaning* by themselves (label switching). Only the grouping structure matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c8759",
   "metadata": {},
   "source": [
    "### 0.4 Probabilities vs logs (nats vs bits)\n",
    "\n",
    "`graph-tool` reports many objective functions in **natural log units** (\"nats\").\n",
    "\n",
    "- Natural log: $\\ln(\\cdot)$  \n",
    "- Log base 2: $\\log_2(\\cdot)$ (\"bits\")\n",
    "\n",
    "Conversion: $\\log_2 x = \\frac{\\ln x}{\\ln 2}.$\n",
    "\n",
    "So if `graph-tool` gives you a **description length** $\\Sigma$ in nats, you can convert to bits with: $\\Sigma_{\\text{bits}} = \\frac{\\Sigma}{\\ln 2}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f30d4e9",
   "metadata": {},
   "source": [
    "### 0.5 What is a \"model\" in this notebook?\n",
    "\n",
    "A model is:\n",
    "\n",
    "- a generative story for graphs, like an SBM (stochastic block model)\n",
    "- plus a choice of priors / hyperpriors\n",
    "- plus a choice of which parameters are learned vs fixed (e.g., degree-corrected vs not)\n",
    "\n",
    "Different modeling assumptions can lead to different inferred \"communities\".\n",
    "________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202038ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# log conversion demo (pure math; independent of graph-tool)\n",
    "x = 10.0\n",
    "print(\"ln(x) =\", np.log(x))\n",
    "print(\"log2(x) =\", np.log2(x))\n",
    "print(\"ln(x)/ln(2) =\", np.log(x) / np.log(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2b498",
   "metadata": {},
   "source": [
    "## 1. Setup: a dedicated conda environment (graph-tool + NetworkX + Jupyter)\n",
    "\n",
    "This notebook assumes `graph-tool` is installed and importable.\n",
    "\n",
    "Because `graph-tool` is a compiled C++/Python library, **the safest path is to put it in its own Conda environment** (so you don't accidentally mix binary dependencies from different sources).\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 What we're trying to avoid\n",
    "\n",
    "If you've ever seen errors like:\n",
    "\n",
    "- `ImportError: ... undefined symbol ...`\n",
    "- `libstdc++.so.6: version GLIBCXX_... not found`\n",
    "- `libgomp... version GOMP_... not found`\n",
    "- `Segmentation fault (core dumped)` *right after an import*\n",
    "\n",
    "...those are almost always **binary dependency conflicts**.\n",
    "\n",
    "The fix is boring but reliable:\n",
    "\n",
    "1. Create a **fresh** environment\n",
    "2. Install from **conda-forge**\n",
    "3. Don't \"pile\" unrelated heavy stacks into the same env unless you really need them\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Recommended installer: Miniforge (Conda + conda-forge)\n",
    "\n",
    "`graph-tool` is easiest to install from **conda-forge** (binary builds; no compilation).\n",
    "\n",
    "- macOS (Intel + Apple Silicon): supported\n",
    "- Linux (x86_64 + ARM): supported\n",
    "- Windows: **no native build** → use WSL2 (or Docker)\n",
    "\n",
    "**Install Miniforge** (recommended) or another Conda distribution, then use `mamba` (faster solver).\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 \"One environment for the course\" (macOS / Linux)\n",
    "\n",
    "In a terminal:\n",
    "\n",
    "```bash\n",
    "# 0) (optional but recommended) make sure conda-forge has priority\n",
    "conda config --add channels conda-forge\n",
    "conda config --set channel_priority strict\n",
    "\n",
    "# 1) Create a fresh environment\n",
    "mamba create -n gtcnet5052 python=3.11\n",
    "\n",
    "# 2) Install what we need for this course\n",
    "mamba install -n gtcnet5052 graph-tool networkx numpy pandas scipy matplotlib jupyterlab ipykernel tqdm\n",
    "\n",
    "# 3) Activate\n",
    "conda activate gtcnet5052\n",
    "```\n",
    "\n",
    "If you don't have `mamba`, you can replace `mamba` with `conda` (it's just slower).\n",
    "\n",
    "\n",
    "**macOS note (Apple Silicon):** make sure your terminal is running as `arm64`:\n",
    "\n",
    "```bash\n",
    "uname -m\n",
    "```\n",
    "\n",
    "If it prints `arm64`, you're good. If it prints `x86_64`, you're running an Intel/Rosetta shell (it can work, but you'll be mixing architectures).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4 Windows (recommended): WSL2 + Ubuntu + conda-forge\n",
    "\n",
    "On Windows, the cleanest path is to run `graph-tool` inside **WSL2** (Linux subsystem).\n",
    "\n",
    "**Step A — install WSL2 (PowerShell as Administrator):**\n",
    "```powershell\n",
    "wsl --install\n",
    "```\n",
    "\n",
    "Reboot if prompted, then open your Ubuntu terminal.\n",
    "\n",
    "**Step B — install Miniforge inside WSL2**, then create the same environment as above.\n",
    "\n",
    "**Step C — how to run notebooks**\n",
    "You have two good workflows:\n",
    "\n",
    "1) **VS Code Remote (WSL)**  \n",
    "   - install VS Code on Windows  \n",
    "   - install the \"Remote - WSL\" extension  \n",
    "   - open the folder from WSL  \n",
    "   - run Jupyter notebooks using the WSL Python kernel\n",
    "\n",
    "2) **Run Jupyter in WSL, open it in your Windows browser**\n",
    "```bash\n",
    "conda activate gtcnet5052\n",
    "jupyter lab --no-browser --ip 0.0.0.0 --port 8888\n",
    "```\n",
    "Then open the URL Jupyter prints (usually includes a token) in your Windows browser.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 Register the environment as a Jupyter kernel (recommended)\n",
    "\n",
    "This makes sure your notebook is running the right Python.\n",
    "\n",
    "```bash\n",
    "conda activate gtcnet5052\n",
    "python -m ipykernel install --user --name gtcnet5052 --display-name \"Python (gtcnet5052 / graph-tool)\"\n",
    "```\n",
    "\n",
    "Then, in Jupyter: **Kernel → Change Kernel → Python (gtcnet5052 / graph-tool)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.6 Reproducibility checklist (quick sanity test)\n",
    "\n",
    "From the activated environment:\n",
    "\n",
    "```bash\n",
    "python -c \"import graph_tool.all as gt; import networkx as nx; import numpy as np; print('graph-tool OK')\"\n",
    "```\n",
    "\n",
    "If that succeeds, you are ready.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.7 A fully reproducible `environment.yml` (copy/paste)\n",
    "\n",
    "You can also create environments from an `environment.yml` file:\n",
    "\n",
    "```yaml\n",
    "name: gtcnet5052\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.11\n",
    "  - graph-tool\n",
    "  - networkx\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - scipy\n",
    "  - matplotlib\n",
    "  - jupyterlab\n",
    "  - ipykernel\n",
    "  - tqdm\n",
    "```\n",
    "\n",
    "Then run:\n",
    "\n",
    "```bash\n",
    "mamba env create -f environment.yml\n",
    "conda activate gtcnet5052\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1.8 Troubleshooting heuristics that actually work\n",
    "\n",
    "If installs fail or imports crash:\n",
    "\n",
    "- **Start fresh**: delete the env and recreate it  \n",
    "  `conda env remove -n gtcnet5052`\n",
    "- **Stay on conda-forge** (avoid mixing \"defaults\" + conda-forge for compiled stacks)\n",
    "- **Avoid installing PyTorch + graph-tool into the same env** unless you know how to debug OpenMP / `libgomp` issues  \n",
    "  (make a second env for deep learning)\n",
    "\n",
    "\n",
    "- **Permission errors on macOS** (e.g., `PermissionError: ... /.condatmp`) often happen when your base Conda install lives in a protected location like `/usr/local/anaconda3`.  \n",
    "  Two reliable fixes are:\n",
    "  1) Put Conda's temp + package cache under your home directory:\n",
    "     ```bash\n",
    "     mkdir -p ~/.conda/tmp ~/.conda/pkgs ~/.conda/envs\n",
    "     conda config --add envs_dirs ~/.conda/envs\n",
    "     conda config --add pkgs_dirs ~/.conda/pkgs\n",
    "     export TMPDIR=\"$HOME/.conda/tmp\"\n",
    "     ```\n",
    "     Then retry the install.\n",
    "  2) Install Miniforge under your home directory (recommended for a clean slate).\n",
    "\n",
    "- **macOS Homebrew vs Conda library collisions** can cause warnings like \"Class ... implemented in both ...libgio...\".  \n",
    "  If imports work but you see scary warnings (or occasional crashes), prefer the Conda env's libraries:\n",
    "  ```bash\n",
    "  mkdir -p \"$CONDA_PREFIX/etc/conda/activate.d\" \"$CONDA_PREFIX/etc/conda/deactivate.d\"\n",
    "\n",
    "  cat > \"$CONDA_PREFIX/etc/conda/activate.d/00-dyld-fallback.sh\" << 'EOF'\n",
    "  export _OLD_DYLD_FALLBACK_LIBRARY_PATH=\"$DYLD_FALLBACK_LIBRARY_PATH\"\n",
    "  export DYLD_FALLBACK_LIBRARY_PATH=\"$CONDA_PREFIX/lib:${DYLD_FALLBACK_LIBRARY_PATH:-}\"\n",
    "  EOF\n",
    "\n",
    "  cat > \"$CONDA_PREFIX/etc/conda/deactivate.d/00-dyld-fallback.sh\" << 'EOF'\n",
    "  export DYLD_FALLBACK_LIBRARY_PATH=\"$_OLD_DYLD_FALLBACK_LIBRARY_PATH\"\n",
    "  unset _OLD_DYLD_FALLBACK_LIBRARY_PATH\n",
    "  EOF\n",
    "\n",
    "  conda deactivate\n",
    "  conda activate gtcnet5052\n",
    "  ```\n",
    "\n",
    "- **If your base Conda is very old**, a simple workaround is to make a tiny \"boot\" environment that contains a modern `mamba`, and use it only to create the course env:\n",
    "  ```bash\n",
    "  conda create -n mamba_boot -c conda-forge python=3.11 mamba\n",
    "  conda activate mamba_boot\n",
    "  mamba env create -f environment.yml\n",
    "  ```\n",
    "\n",
    "If all else fails, the official `graph-tool` Docker image is a reliable fallback.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1.9 Fallback: Docker image (if your local install is blocked)\n",
    "\n",
    "If you are in an environment where you *cannot* install Conda packages (or you're debugging a gnarly binary issue), the project maintains a Docker image.\n",
    "\n",
    "At a minimum, you can pull it with:\n",
    "\n",
    "```bash\n",
    "docker pull tiagopeixoto/graph-tool\n",
    "```\n",
    "\n",
    "From there you can mount your course folder into the container and run Python/Jupyter inside it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c06a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python / scientific stack\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "from scipy import special\n",
    "from scipy.sparse import linalg as spla\n",
    "\n",
    "# NetworkX for translation examples\n",
    "import networkx as nx\n",
    "\n",
    "# graph-tool (this notebook assumes it is installed)\n",
    "import graph_tool\n",
    "import graph_tool.all as gt\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "gt.seed_rng(42)\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"NetworkX:\", nx.__version__)\n",
    "print(\"graph-tool:\", getattr(graph_tool, \"__version__\", \"unknown\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c81d86",
   "metadata": {},
   "source": [
    "## 2. Mental model: `networkx` vs `graph-tool`\n",
    "\n",
    "Both libraries let you represent graphs, but they make different design tradeoffs.\n",
    "\n",
    "### 2.1 Key differences between the packages\n",
    "\n",
    "| Idea | `networkx` | `graph-tool` |\n",
    "|---|---|---|\n",
    "| Main goal | flexibility, readability | performance + inference algorithms |\n",
    "| Vertex identity | any hashable Python object (strings, ints, tuples, ...) | vertices are integer indices `0..N-1` |\n",
    "| Attributes | stored in nested Python dicts | stored in \"typed\" property maps (`vp`, `ep`, `gp`) |\n",
    "| Subgraphs | often copied (unless you build views yourself) | GraphView (cheap \"views\" via filters) |\n",
    "| Graph type | simple graph by default | multigraph by default (parallel edges allowed) |\n",
    "| Sweet spot | small-medium graphs, fast prototyping | medium-large graphs, inference + speed |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22993937",
   "metadata": {},
   "source": [
    "`graph-tool` expects you to think a bit more like a systems programmer:\n",
    "- keep a clean separation between structure (e.g. edges) and data (e.g. properties)\n",
    "- be explicit about types (int/float/string/...)\n",
    "- use views/filters to avoid copying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218d248",
   "metadata": {},
   "source": [
    "### 2.2 Why this matters for community detection\n",
    "\n",
    "In Class 04, we learned that modularity maximization can hallucinate, in a sense, communities and/or suffer from resolution issues.\n",
    "\n",
    "`graph-tool` takes a different approach:\n",
    "- define a **generative model** for the graph (e.g., an SBM)\n",
    "- infer the best explanation *and its complexity* from data\n",
    "- quantify uncertainty via Markov-chain Monte Carlo\n",
    "\n",
    "We'll build up to that slowly, starting with the fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7692b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3 Your first translation: a tiny graph in both libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24314495-457e-4ef4-98da-1be1cd1fea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetworkX: tiny undirected graph with labels\n",
    "G_nx = nx.Graph()\n",
    "G_nx.add_edges_from([(\"alice\", \"bob\"), (\"bob\", \"carlos\"), (\"alice\", \"carlos\"), (\"carlos\", \"dana\")])\n",
    "print(\"NetworkX nodes:\", list(G_nx.nodes()))\n",
    "print(\"NetworkX edges:\", list(G_nx.edges()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71210f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph-tool: same graph\n",
    "g_gt = gt.Graph(directed=False)\n",
    "\n",
    "# graph-tool vertices are integers. We'll keep labels in a vertex property map.\n",
    "name = g_gt.new_vertex_property(\"string\")\n",
    "\n",
    "# Make a mapping from label -> vertex\n",
    "label_to_v = {}\n",
    "for lbl in G_nx.nodes():\n",
    "    v = g_gt.add_vertex()\n",
    "    label_to_v[lbl] = v\n",
    "    name[v] = lbl\n",
    "\n",
    "# Add edges using the mapping\n",
    "for u, v in G_nx.edges():\n",
    "    g_gt.add_edge(label_to_v[u], label_to_v[v])\n",
    "\n",
    "g_gt.vp[\"name\"] = name  # store under a nice key\n",
    "\n",
    "print(\"\\ngraph-tool vertices:\", g_gt.num_vertices())\n",
    "print(\"graph-tool edges:\", g_gt.num_edges())\n",
    "# show labels\n",
    "print(\"vertex labels:\", [g_gt.vp.name[v] for v in g_gt.vertices()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527df0e6",
   "metadata": {},
   "source": [
    "### Modularity can be high *simply due to random chance!*\n",
    "\n",
    "Recall: in Class 04, we saw an important observation\n",
    "\n",
    "> Even *random graphs* can have surprisingly high maximum modularity.  \n",
    "> So \"high modularity\" alone is not strong evidence of meaningful communities.\n",
    "\n",
    "Let's reproduce the idea quickly with a small simulation:\n",
    "- compute the best Louvain modularity on the **Karate Club** graph\n",
    "- compute the best Louvain modularity on many **random graphs with the same N and E**\n",
    "- compare the distributions\n",
    "\n",
    "**When in doubt**:\n",
    "- always compare to a null model\n",
    "- don't confuse an optimization objective with \"truth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1aa3c2",
   "metadata": {},
   "source": [
    "________\n",
    "### 2.5 Modularity\n",
    "\n",
    "> Within-group edges minus what we would expect within-group if edges were placed \"at random\", *given the degrees*.\n",
    "\n",
    "Let:\n",
    "\n",
    "- $A_{ij}$ = adjacency matrix (1 if edge between $i$ and $j$, else 0)\n",
    "- $k_i = \\sum_j A_{ij}$ = degree of node $i$\n",
    "- $m$ = number of edges (so $2m = \\sum_i k_i$ in an undirected graph)\n",
    "- $C_i$ = community assignment of node $i$\n",
    "\n",
    "A common modularity definition (undirected, unweighted) is:\n",
    "\n",
    "$$\n",
    "Q(\\mathbf{C}) = \\frac{1}{2m}\\sum_{i,j}\n",
    "\\left(A_{ij} - \\frac{k_i k_j}{2m}\\right)\\,\\mathbf{1}\\{C_i=C_j\\}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac4ffa",
   "metadata": {},
   "source": [
    "#### Where does $\\frac{k_i k_j}{2m}$ come from?\n",
    "\n",
    "This term is the *configuration-model* baseline:\n",
    "\n",
    "- Imagine node $i$ has $k_i$ \"stubs\" (half-edges)\n",
    "- In total, there are $2m$ stubs\n",
    "- A rough approximation: the chance that a randomly chosen stub attaches to node $j$ is $k_j/(2m)$\n",
    "\n",
    "So the expected number of $i\\leftrightarrow j$ connections under random stub-matching is proportional to:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[A_{ij}] \\approx \\frac{k_i k_j}{2m}.\n",
    "$$\n",
    "\n",
    "Modularity is therefore comparing observed vs degree-preserving random expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b91edd",
   "metadata": {},
   "source": [
    "#### Why high modularity can be misleading\n",
    "\n",
    "Even if the graph is random, you can often find *some* partition that makes the within-group count larger than expected — not because the graph has real communities, but because you are optimizing over a huge space of possible partitions and/or the best partition is apt to over-fit on the noise.\n",
    "\n",
    "That's why we compare to null models and/or use statistical inference (SBMs) instead of treating $Q$ as a ground-truth score.\n",
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634357f1-93c7-4cee-b40d-86c50ae332fa",
   "metadata": {},
   "source": [
    "## Your turn! (Refresher) -- 10 minutes\n",
    "\n",
    "Use `networkx` to measure the modularity of the Karate Club graph.\n",
    "\n",
    "1. Run the Louvain algorithm, and store the modularity of the partition it outputs.\n",
    "2. Write a function to randomize your network in two ways (with an input keyword):\n",
    "    * First: Random network, same $n$ and $m$\n",
    "    * Second: Degree-preserving random network.\n",
    "3. For a loop of 500 times, randomize your original network, under both approaches. Each time, run the Louvain community detection algorithm, and store the modularity.\n",
    "4. Plot the two distributions of modularities, along with a vertical line corresponding to the original graph's modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949683ff-682d-411b-805b-6d018a0d30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb94cc1-c0fb-47df-87b4-f62ac1996cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06cd42-960b-4cc0-a3dc-01a1eba66ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f79d09-6a9b-4fc6-977e-9e3b13146187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa597e-a3ff-4fcb-989a-b0fca9a8d7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbeffb-302b-4381-ab73-a31dc8d3aed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69edbd6d-720b-4187-9ba5-98e671a00c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a512f7-093c-4d54-8073-231ae30fbb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a1b93-ed32-41c8-a5c7-4de0db790efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a9dca-44c6-4604-bf16-856ab4d75d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f8a8f-cef1-4d12-abef-b53e28dea9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66b9c2-a384-4ad8-987c-96b4f53b96d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2261b5cf-2f60-43b2-bfe3-b4b89a053a90",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e396b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community.quality import modularity\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "\n",
    "def best_louvain_modularity(G, seed=0):\n",
    "    parts = louvain_communities(G, seed=seed)\n",
    "    return modularity(G, parts)\n",
    "\n",
    "\n",
    "G_obs = nx.karate_club_graph()\n",
    "n = G_obs.number_of_nodes()\n",
    "m = G_obs.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_obs = best_louvain_modularity(G_obs, seed=0)\n",
    "print(\"Observed Karate modularity (Louvain):\", round(q_obs, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e72232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null distribution: random graphs with same N and E\n",
    "qs = []\n",
    "for i in range(500):\n",
    "    G_rand = nx.gnm_random_graph(n, m, seed=i)\n",
    "    qs.append(best_louvain_modularity(G_rand, seed=i))\n",
    "\n",
    "qs = np.array(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec415ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3),dpi=100)\n",
    "\n",
    "plt.hist(qs, bins=15, ec='.8', label='Null model modularity')\n",
    "plt.axvline(q_obs, linewidth=3, color='k', label='Observed modularity')\n",
    "plt.xlabel(\"best modularity (Louvain)\")\n",
    "plt.ylabel(\"null density (approx)\")\n",
    "plt.title(\"Modularity from fluctuations: Karate vs random graphs\")\n",
    "plt.legend(fontsize='small')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Null mean modularity:\", round(qs.mean(), 3))\n",
    "print(\"Null 95% range:\", tuple(np.round(np.quantile(qs, [0.025, 0.975]), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654a9de",
   "metadata": {},
   "source": [
    "#### 2.6 In case you're interested... we can compute modularity from scratch like in Class 04.\n",
    "\n",
    "We'll take the Louvain partition on Karate and compute:\n",
    "\n",
    "$$\n",
    "Q = \\frac{1}{2m}\\sum_{i,j}\n",
    "\\left(A_{ij} - \\frac{k_i k_j}{2m}\\right)\\,\\mathbf{1}\\{g_i=g_j\\}\n",
    "$$\n",
    "\n",
    "in plain NumPy, then verify it matches `networkx.algorithms.community.quality.modularity()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e92046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the Louvain partition explicitly\n",
    "parts = louvain_communities(G_obs, seed=0)\n",
    "q_nx = modularity(G_obs, parts)\n",
    "\n",
    "nodes = list(G_obs.nodes())\n",
    "idx = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "A = nx.to_numpy_array(G_obs, nodelist=nodes)\n",
    "k = A.sum(axis=1)\n",
    "m = k.sum() / 2.0\n",
    "\n",
    "# community labels aligned to our matrix ordering\n",
    "g = np.empty(len(nodes), dtype=int)\n",
    "for ci, group in enumerate(parts):\n",
    "    for node in group:\n",
    "        g[idx[node]] = ci\n",
    "\n",
    "B = A - np.outer(k, k) / (2 * m)           # modularity matrix\n",
    "S = (g[:, None] == g[None, :]).astype(float)  # same-block indicator\n",
    "q_manual = (1 / (2 * m)) * np.sum(B * S)\n",
    "\n",
    "print(\"NetworkX modularity:\", q_nx)\n",
    "print(\"Manual modularity  :\", q_manual)\n",
    "print(\"Absolute diff      :\", abs(q_nx - q_manual))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d200a9",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c39eff",
   "metadata": {},
   "source": [
    "### Recap done / check-in\n",
    "\n",
    "1. What is the biggest *practical* difference between a `networkx` graph and a `graph-tool` graph?\n",
    "2. Why does `graph-tool` use property maps for attributes instead of storing them directly on Python objects?\n",
    "3. Suppose you need to compute a heavy statistic (like betweenness) on a graph with millions of edges. What parts of the stack become the bottleneck in pure-Python workflows?\n",
    "4. When might `networkx` still be the right choice even if it's slower?\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbaf54",
   "metadata": {},
   "source": [
    "## 3. Graph basics in `graph-tool`\n",
    "\n",
    "### 3.1 Creating graphs\n",
    "\n",
    "In `graph-tool`, the central object is `gt.Graph`.\n",
    "\n",
    "```python\n",
    "g = gt.Graph(directed=False)   # undirected\n",
    "g = gt.Graph(directed=True)    # directed\n",
    "```\n",
    "\n",
    "A few gotchas:\n",
    "- A `Graph` is a **multigraph by default** (parallel edges allowed).\n",
    "- Vertices are always indexed contiguously from `0` to `N-1`.\n",
    "- Vertices and edges are \"descriptors\" — lightweight handles that refer into the graph.\n",
    "\n",
    "### 3.2 Adding vertices and edges\n",
    "\n",
    "- `v = g.add_vertex()` adds one vertex\n",
    "- `g.add_edge(u, v)` adds an edge\n",
    "- You can add many edges quickly via `g.add_edge_list(...)`\n",
    "\n",
    "### 3.3 Iterating\n",
    "\n",
    "- `for v in g.vertices(): ...`\n",
    "- `for e in g.edges(): ...`\n",
    "- `for u in v.out_neighbors(): ...` (directed graphs)\n",
    "- `v.out_degree()` / `v.in_degree()` / `v.all_neighbors()`\n",
    "\n",
    "### 3.4 Parallel edges and self-loops\n",
    "\n",
    "Because graphs are multigraphs by default, the following are possible unless you remove them:\n",
    "- multiple edges between the same pair of vertices\n",
    "- edges from a vertex to itself\n",
    "\n",
    "You can remove them with:\n",
    "```python\n",
    "gt.remove_self_loops(g)\n",
    "gt.remove_parallel_edges(g)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5 Hands-on: build a directed graph and inspect it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b25b5-de9e-4a2e-b146-99f0ca3586d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gt.Graph(directed=True)\n",
    "g.add_vertex(5)  # add 5 vertices: indices 0..4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4931135-419b-4bda-9fe9-801a9f5a92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some directed edges\n",
    "g.add_edge(g.vertex(0), g.vertex(1))\n",
    "g.add_edge(g.vertex(0), g.vertex(2))\n",
    "g.add_edge(g.vertex(2), g.vertex(1))\n",
    "g.add_edge(g.vertex(1), g.vertex(3))\n",
    "g.add_edge(g.vertex(3), g.vertex(3))  # self-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10496994-2716-4670-aece-fd678dd86d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel edge (same endpoints)\n",
    "g.add_edge(g.vertex(0), g.vertex(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bce415-5f5b-43cd-8945-f69859ac59ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"directed?\", g.is_directed())\n",
    "print(\"V =\", g.num_vertices(), \"E =\", g.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f319c7b-cd9e-4432-95cc-9a5b65471408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degrees\n",
    "for v in g.vertices():\n",
    "    print(int(v), \"out-degree =\", v.out_degree(), \"in-degree =\", v.in_degree())\n",
    "\n",
    "print(\"\\nAll edges:\")\n",
    "for e in g.edges():\n",
    "    print(int(e.source()), \"->\", int(e.target()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "gt.remove_self_loops(g)\n",
    "gt.remove_parallel_edges(g)\n",
    "\n",
    "print(\"\\nAfter removing self-loops + parallel edges:\")\n",
    "print(\"E =\", g.num_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4033e4a",
   "metadata": {},
   "source": [
    "## 4. Property maps: how attributes work in `graph-tool`\n",
    "\n",
    "In `networkx`, attributes live in Python dictionaries:\n",
    "\n",
    "```python\n",
    "G.nodes[\"alice\"][\"age\"] = 19\n",
    "G.edges[\"alice\", \"bob\"][\"weight\"] = 0.7\n",
    "```\n",
    "\n",
    "In `graph-tool`, attributes live in **property maps**, which are:\n",
    "- typed (e.g., `\"int\"`, `\"double\"`, `\"string\"`, `\"bool\"`, `\"vector<double>\"`)\n",
    "- stored *separately* from the adjacency structure (for speed)\n",
    "- accessible via:\n",
    "  - `g.vp` (vertex properties)\n",
    "  - `g.ep` (edge properties)\n",
    "  - `g.gp` (graph properties)\n",
    "\n",
    "### 4.1 Create property maps\n",
    "\n",
    "```python\n",
    "age   = g.new_vertex_property(\"int\")\n",
    "name  = g.new_vertex_property(\"string\")\n",
    "w     = g.new_edge_property(\"double\")\n",
    "label = g.new_graph_property(\"string\")\n",
    "```\n",
    "\n",
    "Then assign values by indexing with a vertex/edge descriptor:\n",
    "```python\n",
    "age[v] = 19\n",
    "w[e]   = 0.7\n",
    "```\n",
    "\n",
    "### 4.2 Store property maps inside the graph\n",
    "\n",
    "You can keep property maps as standalone variables, **and/or** store them in the graph:\n",
    "\n",
    "```python\n",
    "g.vp[\"age\"] = age\n",
    "g.ep[\"weight\"] = w\n",
    "g.gp[\"dataset\"] = label\n",
    "```\n",
    "\n",
    "Then you can access them as:\n",
    "- `g.vp.age` (if the key is a valid Python identifier)\n",
    "- `g.vp[\"age\"]` (always works)\n",
    "\n",
    "### 4.3 Fast numpy access (`.a` and `.fa`)\n",
    "\n",
    "Many property maps expose numpy arrays for speed:\n",
    "- `prop.a` is typically a numpy array view of the values\n",
    "- `prop.fa` is a \"fast array\" optimized for internal storage (often the one to use)\n",
    "\n",
    "You'll see this in graph-tool examples like:\n",
    "```python\n",
    "deg = g.degree_property_map(\"total\").a\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Hands-on: attach names to vertices and weights to edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4d0171-8722-425f-96a3-50ab68804b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gt.Graph(directed=False)\n",
    "g.add_vertex(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66963201-7dbc-4736-bf26-15243d9ed1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make edges\n",
    "edges = [(0, 1), (1, 2), (2, 3), (0, 2)]\n",
    "g.add_edge_list(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a51f1-f317-4298-807c-9e13e826641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertex properties\n",
    "name = g.new_vp(\"string\")\n",
    "for i, v in enumerate(g.vertices()):\n",
    "    name[v] = [\"alice\", \"bob\", \"carlos\", \"dana\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343018bc-768f-43b2-9f57-7b2bd472e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge properties\n",
    "w = g.new_ep(\"double\")\n",
    "for e in g.edges():\n",
    "    w[e] = np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bafe1c-9e5d-438c-a960-70bfd5050367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store them\n",
    "g.vp[\"name\"] = name\n",
    "g.ep[\"weight\"] = w\n",
    "g.gp[\"dataset\"] = g.new_gp(\"string\", val=\"toy_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph property dataset:\", g.gp[\"dataset\"])\n",
    "for e in g.edges():\n",
    "    print(g.vp.name[e.source()], \"--\", g.vp.name[e.target()], \" weight =\", round(g.ep.weight[e], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db0223d",
   "metadata": {},
   "source": [
    "### Check-in:\n",
    "\n",
    "1. What is the type of `g.vp[\"name\"]` (a **property map**) versus the type of `g.vp[\"name\"][v]` (a **value**)?\n",
    "2. `deg = g.degree_property_map(\"total\")` gives a property map. What does `deg.a` return, and why is that convenient?\n",
    "3. What happens to vertex/edge property maps when you create a `GraphView`? (Think: *view* vs *copy*.)\n",
    "4. If you filter a graph and then call `gt.Graph(view, prune=True)`, what changes, and what stays the same?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dcdc9e",
   "metadata": {},
   "source": [
    "__________\n",
    "## 5. Building graphs from data (edge lists, pandas, file formats)\n",
    "\n",
    "### 5.1 Edge lists are the universal \"exchange format\"\n",
    "\n",
    "Most real-world network datasets can be reduced to something like:\n",
    "\n",
    "| source | target | weight | timestamp | ... |\n",
    "|---|---|---:|---|---|\n",
    "| alice | bob | 0.2 | 2024-01-01 | ... |\n",
    "| bob | carlos | 1.1 | 2024-01-02 | ... |\n",
    "\n",
    "In `graph-tool`, the most efficient way to build a graph from data is usually:\n",
    "\n",
    "- turn your edge list into a numpy array or list of tuples\n",
    "- call `g.add_edge_list(...)`\n",
    "\n",
    "### 5.2 `add_edge_list(..., hashed=True)` for string node IDs\n",
    "\n",
    "If your nodes are strings, you can let `graph-tool` build the mapping automatically:\n",
    "\n",
    "```python\n",
    "g = gt.Graph(directed=False)\n",
    "vprop_id = g.add_edge_list(edge_array, hashed=True, hash_type=\"string\")\n",
    "g.vp[\"id\"] = vprop_id\n",
    "```\n",
    "\n",
    "This creates:\n",
    "- vertices with integer indices `0..N-1`\n",
    "- a vertex property map that stores the original IDs\n",
    "\n",
    "### 5.3 Reading and writing graphs\n",
    "\n",
    "`graph-tool` supports multiple formats:\n",
    "- `.gt` (native binary, fast)\n",
    "- `.graphml` / `.xml`\n",
    "- `.gml`, `.dot`\n",
    "\n",
    "```python\n",
    "g.save(\"mygraph.gt\")\n",
    "g2 = gt.load_graph(\"mygraph.gt\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Hands-on: make a graph from a pandas edge list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a tiny \"realistic\" edge list as a DataFrame\n",
    "df_edges = pd.DataFrame({\n",
    "    \"src\": [\"alice\", \"alice\", \"bob\", \"carlos\", \"carlos\", \"dana\"],\n",
    "    \"dst\": [\"bob\", \"carlos\", \"carlos\", \"dana\", \"alice\", \"bob\"],\n",
    "    \"weight\": [1.0, 0.2, 0.8, 2.0, 0.5, 1.3],\n",
    "})\n",
    "\n",
    "df_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90024241-13e3-4282-8e26-be3f9724cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge list as list of tuples (src, dst, weight)\n",
    "edge_list = list(df_edges.itertuples(index=False, name=None))\n",
    "edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb343457-b0b7-40dc-bb60-26ff0a73ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gt.Graph(directed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d6064-3fcb-44c5-b289-87298e0cfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will store weights in an edge property map.\n",
    "eweight = g.new_ep(\"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bdd79f-594b-4b74-8fd7-0bc0b1247af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_edge_list can fill edge properties if we pass eprops=[...]\n",
    "# IMPORTANT: When hashed=True, it will map string IDs -> vertex indices.\n",
    "vprop_id = g.add_edge_list(edge_list, hashed=True, hash_type=\"string\", eprops=[eweight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72093c92-3c8a-4677-be0c-3bf51e8e3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store properties\n",
    "g.vp[\"id\"] = vprop_id\n",
    "g.ep[\"weight\"] = eweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f239bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V =\", g.num_vertices(), \"E =\", g.num_edges())\n",
    "print(\"Some vertex IDs:\", [g.vp.id[v] for v in list(g.vertices())[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e915de8-c9bd-4cdf-a25b-32ecfc852fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first few edges with weights\n",
    "for e in list(g.edges())[:5]:\n",
    "    print(g.vp.id[e.source()], \"-\", g.vp.id[e.target()], \"w =\", g.ep.weight[e])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd354f",
   "metadata": {},
   "source": [
    "________\n",
    "## 6. Converting `networkx` ↔ `graph-tool` (interoperability)\n",
    "\n",
    "You will often:\n",
    "- prototype in `networkx` (quick + flexible)\n",
    "- scale or infer structure in `graph-tool` (fast + statistical inference)\n",
    "- export results back to `networkx` for compatibility with other tooling\n",
    "\n",
    "### 6.1 The core challenge: node labels\n",
    "\n",
    "- In `networkx`, nodes can be strings, ints, tuples, etc.\n",
    "- In `graph-tool`, vertices are **integer indices**.\n",
    "\n",
    "So any conversion needs a mapping. A common pattern:\n",
    "- store original node labels in a vertex property map named `\"id\"` or `\"name\"`\n",
    "\n",
    "### 6.2 Practical conversion utilities\n",
    "\n",
    "Below is a conversion function that:\n",
    "- preserves node labels (as `vp[\"id\"]`)\n",
    "- carries over node/edge attributes when possible\n",
    "\n",
    "> These are \"teaching implementations\", not industrial-strength ETL.  \n",
    "> For messy real datasets, you'll want to be explicit about which columns/attributes you keep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896965b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _infer_gt_type(value):\n",
    "    # Infer a graph-tool property type for common Python values.\n",
    "    if isinstance(value, bool):\n",
    "        return \"bool\"\n",
    "    if isinstance(value, int) and not isinstance(value, bool):\n",
    "        return \"int\"\n",
    "    if isinstance(value, float):\n",
    "        return \"double\"\n",
    "    if isinstance(value, str):\n",
    "        return \"string\"\n",
    "    # fallback: stores arbitrary Python objects (less portable)\n",
    "    return \"object\"\n",
    "\n",
    "def nx_to_gt(G_nx, node_id_key=\"id\"):\n",
    "    '''\n",
    "    Convert a NetworkX graph -> graph-tool Graph.\n",
    "\n",
    "    - Keeps original node labels in vertex property `node_id_key`.\n",
    "    - Copies node and edge attributes when possible.\n",
    "    '''\n",
    "    g = gt.Graph(directed=G_nx.is_directed())\n",
    "\n",
    "    # --- Collect attribute keys\n",
    "    node_attr_keys = set()\n",
    "    edge_attr_keys = set()\n",
    "\n",
    "    for _, data in G_nx.nodes(data=True):\n",
    "        node_attr_keys |= set(data.keys())\n",
    "    for _, _, data in G_nx.edges(data=True):\n",
    "        edge_attr_keys |= set(data.keys())\n",
    "\n",
    "    # Create vertex property maps\n",
    "    vp = {}\n",
    "    vp[node_id_key] = g.new_vp(\"string\")  # always keep labels as strings\n",
    "    for k in sorted(node_attr_keys):\n",
    "        sample = None\n",
    "        for _, data in G_nx.nodes(data=True):\n",
    "            if k in data:\n",
    "                sample = data[k]\n",
    "                break\n",
    "        vp[k] = g.new_vp(_infer_gt_type(sample) if sample is not None else \"string\")\n",
    "\n",
    "    # Create edge property maps\n",
    "    ep = {}\n",
    "    for k in sorted(edge_attr_keys):\n",
    "        sample = None\n",
    "        for _, _, data in G_nx.edges(data=True):\n",
    "            if k in data:\n",
    "                sample = data[k]\n",
    "                break\n",
    "        ep[k] = g.new_ep(_infer_gt_type(sample) if sample is not None else \"string\")\n",
    "\n",
    "    # --- Add vertices and mapping\n",
    "    nxnode_to_v = {}\n",
    "    for n, data in G_nx.nodes(data=True):\n",
    "        v = g.add_vertex()\n",
    "        nxnode_to_v[n] = v\n",
    "\n",
    "        vp[node_id_key][v] = str(n)\n",
    "\n",
    "        for k, pmap in vp.items():\n",
    "            if k == node_id_key:\n",
    "                continue\n",
    "            if k in data:\n",
    "                try:\n",
    "                    pmap[v] = data[k]\n",
    "                except Exception:\n",
    "                    pmap[v] = str(data[k])\n",
    "\n",
    "    # --- Add edges\n",
    "    for u, v, data in G_nx.edges(data=True):\n",
    "        e = g.add_edge(nxnode_to_v[u], nxnode_to_v[v])\n",
    "        for k, pmap in ep.items():\n",
    "            if k in data:\n",
    "                try:\n",
    "                    pmap[e] = data[k]\n",
    "                except Exception:\n",
    "                    pmap[e] = str(data[k])\n",
    "\n",
    "    # Attach property maps\n",
    "    for k, pmap in vp.items():\n",
    "        g.vp[k] = pmap\n",
    "    for k, pmap in ep.items():\n",
    "        g.ep[k] = pmap\n",
    "\n",
    "    return g\n",
    "\n",
    "def gt_to_nx(g, node_id_key=\"id\"):\n",
    "    '''\n",
    "    Convert graph-tool Graph -> NetworkX graph.\n",
    "\n",
    "    If g.vp[node_id_key] exists, uses it as NetworkX node labels.\n",
    "    Otherwise uses integer vertex indices.\n",
    "    '''\n",
    "    G_nx = nx.DiGraph() if g.is_directed() else nx.Graph()\n",
    "\n",
    "    use_id = (node_id_key in g.vp)\n",
    "    v_to_label = {}\n",
    "\n",
    "    for v in g.vertices():\n",
    "        lbl = g.vp[node_id_key][v] if use_id else int(v)\n",
    "        v_to_label[v] = lbl\n",
    "        G_nx.add_node(lbl)\n",
    "\n",
    "        # copy vertex properties (excluding id)\n",
    "        for k, pmap in g.vp.items():\n",
    "            if k == node_id_key:\n",
    "                continue\n",
    "            try:\n",
    "                G_nx.nodes[lbl][k] = pmap[v]\n",
    "            except Exception:\n",
    "                G_nx.nodes[lbl][k] = str(pmap[v])\n",
    "\n",
    "    # edges + properties\n",
    "    for e in g.edges():\n",
    "        u = v_to_label[e.source()]\n",
    "        v = v_to_label[e.target()]\n",
    "        G_nx.add_edge(u, v)\n",
    "\n",
    "        for k, pmap in g.ep.items():\n",
    "            try:\n",
    "                G_nx.edges[u, v][k] = pmap[e]\n",
    "            except Exception:\n",
    "                G_nx.edges[u, v][k] = str(pmap[e])\n",
    "\n",
    "    return G_nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de4126",
   "metadata": {},
   "source": [
    "### 6.3 Demo: convert the Karate Club graph\n",
    "\n",
    "1. load the classic Zachary Karate Club network in NetworkX\n",
    "2. convert to `graph-tool`\n",
    "3. compute a layout and draw it (graph-tool)\n",
    "4. convert back to NetworkX (so you can keep using your old workflows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_k = nx.karate_club_graph()\n",
    "print(\"NetworkX karate:\", G_k.number_of_nodes(), \"nodes,\", G_k.number_of_edges(), \"edges\")\n",
    "\n",
    "g_k = nx_to_gt(G_k, node_id_key=\"id\")\n",
    "print(\"graph-tool karate:\", g_k.num_vertices(), \"vertices,\", g_k.num_edges(), \"edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b0e70-3280-4812-8725-0a5156abf7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = gt.sfdp_layout(g_k)\n",
    "gt.graph_draw(\n",
    "    g_k, pos=pos,\n",
    "    vertex_text=g_k.vp.id,\n",
    "    vertex_font_size=14,\n",
    "    output_size=(500, 500)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd20600-de55-40d0-a9af-9b4a40c2e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back\n",
    "G_back = gt_to_nx(g_k, node_id_key=\"id\")\n",
    "print(\"Back to NetworkX:\", G_back.number_of_nodes(), \"nodes,\", G_back.number_of_edges(), \"edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331569c9",
   "metadata": {},
   "source": [
    "## 7. Graph views & filtering (largest component, masks, subgraphs)\n",
    "\n",
    "One of the most powerful ideas in `graph-tool` is the **GraphView**:\n",
    "\n",
    "- You can create a *view* of a subset of vertices/edges\n",
    "- Most algorithms operate on the view transparently\n",
    "- This avoids expensive copying\n",
    "\n",
    "### 7.1 The two-step pattern\n",
    "\n",
    "1) Create a boolean filter (a property map of type `\"bool\"`):\n",
    "```python\n",
    "vfilt = g.new_vp(\"bool\")\n",
    "vfilt[v] = True  # keep\n",
    "vfilt[v] = False # hide\n",
    "```\n",
    "\n",
    "2) Create a view:\n",
    "```python\n",
    "gv = gt.GraphView(g, vfilt=vfilt)\n",
    "```\n",
    "\n",
    "If you later need an independent graph (no filters), you can copy/prune:\n",
    "```python\n",
    "g2 = gt.Graph(gv, prune=True)\n",
    "```\n",
    "\n",
    "### 7.2 Largest connected component\n",
    "\n",
    "Community detection and inference are usually best done on the **giant component**.\n",
    "\n",
    "Two common patterns:\n",
    "- `gt.extract_largest_component(g, directed=False)` (quick)\n",
    "- `GraphView + label_largest_component()` (more control)\n",
    "\n",
    "---\n",
    "\n",
    "### 7.3 Hands-on: filter to the largest component and to \"high-degree\" vertices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9411193-d1cd-44b1-95a0-626c10c754a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a built-in dataset (Karate Club)\n",
    "g = nx_to_gt(nx.karate_club_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1c981-985a-4575-b5ce-737c968de3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original: V =\", g.num_vertices(), \"E =\", g.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d3087-4113-4160-a1e2-47d451e59e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largest component (Karate is connected, but this is the pattern)\n",
    "g_lcc = gt.extract_largest_component(g, directed=False)\n",
    "print(\"Largest component: V =\", g_lcc.num_vertices(), \"E =\", g_lcc.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: keep only vertices with degree >= 4\n",
    "deg = g_lcc.degree_property_map(\"total\")\n",
    "vfilt = g_lcc.new_vp(\"bool\")\n",
    "vfilt.a = deg.a >= 4  # vectorized assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c03a3-8296-4770-9c10-aa4307a98c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gv = gt.GraphView(g_lcc, vfilt=vfilt)\n",
    "g_deg4 = gt.Graph(gv, prune=True)\n",
    "\n",
    "print(\"Degree>=4 view (pruned): V =\", g_deg4.num_vertices(), \"E =\", g_deg4.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe4e41-61c9-40bc-8f70-48804201b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = gt.sfdp_layout(g_deg4)\n",
    "gt.graph_draw(\n",
    "    g_deg4, pos=pos,\n",
    "    vertex_text=g_deg4.vp.id,\n",
    "    vertex_font_size=14,\n",
    "    output_size=(500, 500),\n",
    "    vertex_fill_color='lightblue'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b43cf4",
   "metadata": {},
   "source": [
    "## 8. Visualization: layouts, styling, exporting\n",
    "\n",
    "`graph-tool` can draw graphs *directly* (via Cairo), and its layout algorithms are excellent.\n",
    "\n",
    "### 8.1 Layouts (where do nodes go?)\n",
    "\n",
    "A layout is usually a **vertex property map** with a 2D position per node:\n",
    "- type often looks like `\"vector<double>\"` under the hood\n",
    "\n",
    "A common default:\n",
    "```python\n",
    "pos = gt.sfdp_layout(g)\n",
    "```\n",
    "\n",
    "### 8.2 Styling (colors, sizes, labels)\n",
    "\n",
    "Most style parameters can be:\n",
    "- a constant (e.g., `vertex_size=10`)\n",
    "- **or** a property map (e.g., size proportional to degree)\n",
    "\n",
    "Example:\n",
    "```python\n",
    "deg = g.degree_property_map(\"total\")\n",
    "vsize = gt.prop_to_size(deg, mi=5, ma=20)\n",
    "gt.graph_draw(g, pos=pos, vertex_size=vsize)\n",
    "```\n",
    "\n",
    "### 8.3 Exporting\n",
    "\n",
    "You can render directly to a file:\n",
    "```python\n",
    "gt.graph_draw(g, pos=pos, output=\"figure.svg\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8.4 Hands-on: draw the Karate Club graph with degree-based sizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acee566-92d0-45b6-aef1-6cc19293836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx_to_gt(nx.karate_club_graph())\n",
    "\n",
    "# layout\n",
    "pos = gt.sfdp_layout(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4071b636-4dd9-4f4e-b6b7-f9b2e0410964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree -> size\n",
    "deg = g.degree_property_map(\"total\")\n",
    "vsize = gt.prop_to_size(deg, mi=20, ma=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f8999-ba3d-48f5-8353-a0aac4888210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color by degree (continuous)\n",
    "vcolor = gt.prop_to_size(deg, mi=0.1, ma=0.9)  # values mapped to [0.1,0.9] for colormap usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.graph_draw(\n",
    "    g, pos=pos,\n",
    "    vertex_size=vsize,\n",
    "    vertex_fill_color=vcolor,\n",
    "    vertex_text=g.vp.id if \"id\" in g.vp else g.vertex_index,\n",
    "    vertex_font_size=14,\n",
    "    output_size=(700, 650)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18894c10-0454-4aee-82b4-09d5d0e52a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceecba16-b10b-4bef-b238-e6de2a0a8d50",
   "metadata": {},
   "source": [
    "## Your turn! (15 minutes)\n",
    "In pairs*, explore the graph-tool cookbook, and try pasting in an interesting routine / module / analysis.\n",
    "\n",
    "https://graph-tool.skewed.de/static/docs/stable/demos/index.html\n",
    "\n",
    "At the end, each group will share screen and show off their technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4247bf-9cf8-495d-946b-c74827ef1a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0656d2-06d1-4cb3-a75b-5525c1476dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1584d7-b5ff-4e63-b60b-ed9de6df9c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b51f6-7e4b-4b4a-80a4-88babb528bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b5190-4201-4616-a581-3ad28b65d023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41bbe2ce",
   "metadata": {},
   "source": [
    "## 9. Core descriptive stats: degrees, clustering, distances (and translations)\n",
    "\n",
    "Before we infer communities, we should be comfortable with basic network summaries.\n",
    "\n",
    "We'll use the **Zachary Karate Club** network as a running example.\n",
    "\n",
    "> Why this matters: when you later fit an SBM, you should *always* sanity-check the result against basic summaries (degree distribution, clustering, path lengths, assortativity, etc.).  \n",
    "> Otherwise, you can end up \"discovering\" communities that are really just artifacts of degree heterogeneity, disconnected components, or measurement issues.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9b97b-f78b-499e-ab79-015deac45c75",
   "metadata": {},
   "source": [
    "### 9.1 Degree (distribution and averages)\n",
    "\n",
    "For an undirected simple graph with adjacency matrix $A$:\n",
    "\n",
    "$$k_i = \\sum_j A_{ij}.$$\n",
    "\n",
    "For a directed graph:\n",
    "\n",
    "$$k_i^{\\text{out}} = \\sum_j A_{ij},\\qquad k_i^{\\text{in}} = \\sum_j A_{ji}.$$\n",
    "\n",
    "**NetworkX**\n",
    "- `dict(G.degree())`\n",
    "- `np.mean([d for _, d in G.degree()])`\n",
    "\n",
    "**graph-tool**\n",
    "- `deg = g.degree_property_map(\"total\").a`\n",
    "- `gt.vertex_average(g, \"total\")`\n",
    "- `gt.vertex_hist(g, \"total\")`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ef6e8-5806-438b-8a58-b1e8b6a03dcd",
   "metadata": {},
   "source": [
    "### 9.2 Clustering coefficient (triangles)\n",
    "\n",
    "A triangle is a 3-cycle: $i\\to j\\to k\\to i$.\n",
    "\n",
    "A common definition of the **local clustering coefficient** is:\n",
    "\n",
    "$$C_i = \\frac{\\text{number of triangles touching } i}{\\text{number of triples centered at } i}\n",
    "= \\frac{2T_i}{k_i(k_i-1)},$$\n",
    "\n",
    "where $T_i$ is the number of triangles incident to $i$, and $k_i$ is the degree.\n",
    "\n",
    "A common **global** clustering / transitivity is:\n",
    "\n",
    "$$C = \\frac{3\\times\\text{\\# triangles}}{\\text{\\# connected triples}}.$$\n",
    "\n",
    "**NetworkX**\n",
    "- `nx.clustering(G)` (local)\n",
    "- `nx.average_clustering(G)` (global average of local $C_i$)\n",
    "\n",
    "**graph-tool**\n",
    "- `gt.local_clustering(g)` (local)\n",
    "- `gt.global_clustering(g)` (global transitivity)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b8ffe-4d43-498e-a260-03cbf4475687",
   "metadata": {},
   "source": [
    "### 9.3 Shortest-path distance\n",
    "\n",
    "The shortest path distance between $i$ and $j$ is:\n",
    "\n",
    "$$d(i,j) = \\min_{\\pi:i\\to j} |\\pi|,$$\n",
    "\n",
    "i.e., the minimum number of edges along any path $\\pi$ from $i$ to $j$.\n",
    "\n",
    "**NetworkX**\n",
    "- `nx.shortest_path_length(G, source, target)`\n",
    "\n",
    "**graph-tool**\n",
    "- `gt.shortest_distance(g, source=v)` returns distances to all vertices\n",
    "- `gt.shortest_distance(g, source=v, target=u)` returns one distance\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc600e-6825-425c-863a-90db5da4c870",
   "metadata": {},
   "source": [
    "### 9.4 Betweenness centrality (Freeman)\n",
    "\n",
    "Betweenness of a vertex $v$ measures how often it lies on shortest paths:\n",
    "\n",
    "$$\\mathrm{BC}(v) = \\sum_{s\\neq v\\neq t}\\frac{\\sigma_{st}(v)}{\\sigma_{st}},$$\n",
    "\n",
    "where:\n",
    "- $\\sigma_{st}$ is the number of shortest paths from $s$ to $t$\n",
    "- $\\sigma_{st}(v)$ is the number of those paths that pass through $v$\n",
    "\n",
    "**NetworkX**\n",
    "- `nx.betweenness_centrality(G)` (can be slow for large graphs)\n",
    "\n",
    "**graph-tool**\n",
    "- `vb, eb = gt.betweenness(g)` (optimized; can use OpenMP)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8167202a",
   "metadata": {},
   "source": [
    "<!-- #### 9.0 (Optional) A tiny \"by hand\" check of the formulas\n",
    "\n",
    "Before we use library functions, it can be helpful to sanity-check the definitions on a toy graph.\n",
    "\n",
    "We'll build a 4-node graph consisting of:\n",
    "- a triangle on nodes $\\{0,1,2\\}$\n",
    "- plus a \"tail\" edge $(2,3)$\n",
    "\n",
    "Then we'll compute:\n",
    "- degrees from $k_i = \\sum_j A_{ij}$\n",
    "- triangles via $(A^3)_{ii}$\n",
    "- local clustering via $C_i = 2T_i / (k_i(k_i-1))$\n",
    "\n",
    "> This is **not** the fastest way to do things — it's a pedagogical check that the code matches the math. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03827fe",
   "metadata": {},
   "source": [
    "<!-- ##### 9.0.1 Why does $A^3$ \"know about\" triangles?\n",
    "\n",
    "This is one of the most useful linear-algebra facts in network science.\n",
    "\n",
    "Start with an adjacency matrix $A$ for an **undirected simple** graph.\n",
    "\n",
    "**Claim 1:** $(A^2)_{ij}$ counts the number of length-2 walks from $i$ to $j$.\n",
    "\n",
    "Proof sketch:\n",
    "\n",
    "$$\n",
    "(A^2)_{ij} = \\sum_k A_{ik}A_{kj}.\n",
    "$$\n",
    "\n",
    "A term $A_{ik}A_{kj}$ equals 1 exactly when:\n",
    "\n",
    "- $i$ is connected to $k$\n",
    "- and $k$ is connected to $j$\n",
    "\n",
    "So each intermediate node $k$ that forms a 2-step path $i\\to k\\to j$ contributes 1.\n",
    "\n",
    "**Claim 2:** $(A^3)_{ii}$ counts the number of length-3 closed walks that start and end at $i$.\n",
    "\n",
    "$$\n",
    "(A^3)_{ii} = \\sum_{j,k} A_{ij}A_{jk}A_{ki}.\n",
    "$$\n",
    "\n",
    "Now look at a triangle $(i,j,k)$.\n",
    "\n",
    "In an undirected triangle, you can traverse the triangle and return to $i$ in **two** distinct 3-step directions:\n",
    "\n",
    "- $i\\to j\\to k\\to i$\n",
    "- $i\\to k\\to j\\to i$\n",
    "\n",
    "So **each triangle incident to $i$ contributes 2** to $(A^3)_{ii}$.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "T_i = \\frac{(A^3)_{ii}}{2}\n",
    "$$\n",
    "\n",
    "where $T_i$ is the number of triangles that touch vertex $i$.\n",
    "\n",
    "Finally, if you sum over all vertices:\n",
    "\n",
    "- each triangle has 3 vertices\n",
    "- at each vertex, it contributes 2 closed walks\n",
    "\n",
    "So the total triangle count is:\n",
    "\n",
    "$$\n",
    "\\#\\text{triangles} = \\frac{\\mathrm{tr}(A^3)}{6}.\n",
    "$$ -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af50bf-2045-4d35-b835-119e371e2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy graph: triangle + tail\n",
    "G_tiny = nx.Graph()\n",
    "G_tiny.add_edges_from([(0, 1), (1, 2), (2, 0), (2, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdf2fb-0d12-41b5-92a0-f41bab41bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency matrix (node order: 0,1,2,3)\n",
    "A = nx.to_numpy_array(G_tiny, nodelist=sorted(G_tiny.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa09fa-9918-40f0-8fa6-6ae17e36fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree by definition: k_i = sum_j A_ij\n",
    "k = A.sum(axis=1)\n",
    "print(\"Degrees k:\", k.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d6e3f0-e28c-401b-baf8-d70997b261ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triangle counts in an undirected simple graph:\n",
    "# (A^3)_{ii} counts twice the number of triangles incident to i.\n",
    "A3 = np.linalg.matrix_power(A, 3)\n",
    "T = np.diag(A3) / 2\n",
    "print(\"Triangles incident to node i (T_i):\", T.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30a9b3-009a-4c02-8ec2-bf396d26f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local clustering: C_i = 2T_i / (k_i (k_i - 1)), with C_i = 0 if k_i < 2\n",
    "C = np.zeros(len(k), dtype=float)\n",
    "for i in range(len(k)):\n",
    "    if k[i] >= 2:\n",
    "        C[i] = (2 * T[i]) / (k[i] * (k[i] - 1))\n",
    "print(\"Local clustering (by formula):\", np.round(C, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to NetworkX\n",
    "print(\"NetworkX clustering:\", nx.clustering(G_tiny))\n",
    "\n",
    "# Total number of triangles = tr(A^3) / 6 (undirected simple graphs)\n",
    "num_triangles = np.trace(A3) / 6\n",
    "print(\"Total triangles (trace formula):\", int(num_triangles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09183fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "\n",
    "# NetworkX degrees\n",
    "deg_nx = np.array([d for _, d in G.degree()])\n",
    "print(\"NetworkX mean degree:\", deg_nx.mean())\n",
    "\n",
    "g = nx_to_gt(G)\n",
    "\n",
    "deg_gt = g.degree_property_map(\"total\").a\n",
    "print(\"graph-tool mean degree:\", deg_gt.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c3515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare full distributions\n",
    "plt.figure(figsize=(6,3),dpi=100)\n",
    "\n",
    "plt.hist(deg_nx, bins=range(deg_nx.max()+2), alpha=0.6, label=\"NetworkX\", ls='--', ec='k')\n",
    "plt.hist(deg_gt, bins=range(int(deg_gt.max())+2), alpha=0.6, label=\"graph-tool\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx_to_gt(nx.karate_club_graph())\n",
    "\n",
    "# Clustering\n",
    "c_local = gt.local_clustering(g)       # vertex property map\n",
    "c_global = gt.global_clustering(g)     # (value, std) by default\n",
    "\n",
    "print(\"graph-tool global clustering (estimate):\", c_global)\n",
    "\n",
    "# Distances from a source vertex\n",
    "source = g.vertex(0)\n",
    "dist = gt.shortest_distance(g, source=source)  # vertex property map of distances\n",
    "print(\"Distances from vertex 0 (first 10):\", dist.a[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a38d3-cf26-41e6-9e7f-026047152446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betweenness\n",
    "vb, eb = gt.betweenness(g)\n",
    "print(\"Top 5 betweenness vertices:\", np.argsort(vb.a)[-5:][::-1])\n",
    "\n",
    "# Visualize betweenness as node size\n",
    "pos = gt.sfdp_layout(g)\n",
    "vsize = gt.prop_to_size(vb, mi=12, ma=30)\n",
    "gt.graph_draw(g, pos=pos, vertex_size=vsize, output_size=(650, 650))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e80c0",
   "metadata": {},
   "source": [
    "## 10. Null models & simulation: Erdős-Rényi vs configuration models in `graph-tool`\n",
    "\n",
    "A **null model** answers:\n",
    "\n",
    "> *\"What patterns would we expect even if nothing interesting is happening?\"*\n",
    "\n",
    "This is absolutely central to statistical thinking:\n",
    "- we observe a statistic $T(G)$ on the real graph $G$\n",
    "- we compare it to the distribution of $T(\\tilde G)$ for graphs $\\tilde G$ generated from a null model\n",
    "\n",
    "---\n",
    "\n",
    "### 10.1 Erdős-Rényi $G(N,p)$ / $G(N,E)$\n",
    "\n",
    "Two common ER variants:\n",
    "\n",
    "1) **$G(N,p)$**: each possible edge is present independently with probability $p$.  \n",
    "2) **$G(N,E)$**: choose exactly $E$ edges uniformly at random from all $\\binom{N}{2}$ possibilities.\n",
    "\n",
    "For an undirected $G(N,p)$:\n",
    "\n",
    "$$\\mathbb{E}[k_i] = p(N-1),\n",
    "\\qquad\n",
    "k_i \\sim \\mathrm{Binomial}(N-1, p).$$\n",
    "\n",
    "A common \"plug-in\" estimate for $p$ given an observed graph with $N$ nodes and $E$ edges is:\n",
    "\n",
    "$$\\hat p = \\frac{2E}{N(N-1)}.$$\n",
    "\n",
    "In `graph-tool`, the fast way to sample something like $G(N,E)$ while keeping $N$ and $E$ fixed is rewiring:\n",
    "\n",
    "```python\n",
    "gt.random_rewire(g, model=\"erdos\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10.2 Configuration model (degree sequence preserved)\n",
    "\n",
    "Many network statistics are heavily influenced by the degree sequence $\\{k_i\\}$.\n",
    "A stronger null model keeps degrees fixed while randomizing everything else.\n",
    "\n",
    "A configuration-model null preserves the degrees but randomizes the pairing of \"stubs\" (half-edges).\n",
    "\n",
    "In `graph-tool`, you can sample from a configuration-model-like ensemble via:\n",
    "\n",
    "```python\n",
    "gt.random_rewire(g, model=\"configuration\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10.3 Why this matters for community detection\n",
    "\n",
    "If you compare a real graph only to ER, you might mistake \"heavy-tailed degrees\" for \"community structure\".\n",
    "\n",
    "Configuration-model comparisons help you ask a sharper question:\n",
    "\n",
    "> \"Does this graph have structure beyond what degrees alone would produce?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 10.4 A simple hypothesis test by simulation (permutation test)\n",
    "\n",
    "Let $T(G)$ be a statistic (e.g., global clustering).\n",
    "\n",
    "- **Null hypothesis:** the graph was generated from a null model (ER or configuration)\n",
    "- **Test statistic:** $T$\n",
    "- **Monte Carlo p-value:**\n",
    "\n",
    "$$\\hat p = \\frac{1 + \\sum_{s=1}^{S} \\mathbb{1}\\{T(\\tilde G_s) \\ge T(G)\\}}{S+1}.$$\n",
    "\n",
    "We add 1 in numerator and denominator for a small-sample correction (avoids $p=0$).\n",
    "\n",
    "---\n",
    "\n",
    "### 10.5 Hands-on: compare clustering after rewiring\n",
    "\n",
    "We'll start with one rewire (quick intuition), then do a simulation-based p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d24d4",
   "metadata": {},
   "source": [
    "### 10.2.1 Configuration model: the key expectation $\\mathbb{E}[A_{ij}] \\approx k_i k_j/(2m)$\n",
    "\n",
    "The configuration model is the degree-preserving null model sitting underneath a lot of network methods.\n",
    "\n",
    "A quick \"back of the envelope\" derivation:\n",
    "\n",
    "- Node $i$ has $k_i$ stubs.\n",
    "- There are $2m$ stubs in total.\n",
    "- Pick one stub from $i$. The probability it connects to a stub at node $j$ is roughly $k_j/(2m-1) \\approx k_j/(2m)$.\n",
    "\n",
    "Since $i$ has $k_i$ stubs, the expected number of $i\\leftrightarrow j$ connections is approximately:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[A_{ij}] \\approx \\frac{k_i k_j}{2m}.\n",
    "$$\n",
    "\n",
    "This is exactly the baseline term inside modularity.\n",
    "\n",
    "**Takeaway:** the configuration model is not just a random-graph curiosity — it's a concrete \"null story\" about what patterns degrees alone can create.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0af95-64c8-40e0-a5b8-83b43258b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the Karate Club graph\n",
    "g0 = nx_to_gt(nx.karate_club_graph())\n",
    "\n",
    "# Copy graphs before rewiring\n",
    "g_erdos = gt.Graph(g0)\n",
    "g_conf  = gt.Graph(g0)\n",
    "\n",
    "gt.random_rewire(g_erdos, model=\"erdos\", n_iter=10, edge_sweep=True)\n",
    "gt.random_rewire(g_conf,  model=\"configuration\", n_iter=10, edge_sweep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c3e532-535b-4ede-a4a0-7b9c9557d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree sanity-check: configuration rewiring preserves degrees (ER does not)\n",
    "deg0 = g0.degree_property_map(\"total\").a\n",
    "degE = g_erdos.degree_property_map(\"total\").a\n",
    "degC = g_conf.degree_property_map(\"total\").a\n",
    "print(\"Degrees preserved under configuration rewire?\", np.all(deg0 == degC))\n",
    "print(\"Degrees preserved under ER rewire?           \", np.all(deg0 == degE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123142a4-696d-47ac-8ca7-9449abddb49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare global clustering\n",
    "c0 = gt.global_clustering(g0)[0]\n",
    "cE = gt.global_clustering(g_erdos)[0]\n",
    "cC = gt.global_clustering(g_conf)[0]\n",
    "\n",
    "print(\"Global clustering:\")\n",
    "print(\"  original        :\", round(c0, 3))\n",
    "print(\"  ER (G(N,E))     :\", round(cE, 3))\n",
    "print(\"  configuration   :\", round(cC, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2796ae3-f856-415b-8123-82a1876fefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "g0 = nx_to_gt(nx.karate_club_graph())\n",
    "c_obs = gt.global_clustering(g0)[0]\n",
    "\n",
    "S = 10000\n",
    "c_erdos = []\n",
    "c_conf  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2c7c2-473a-4d88-b2d8-69b407a320c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(S):\n",
    "    gE = gt.Graph(g0)\n",
    "    gt.random_rewire(gE, model=\"erdos\", n_iter=10, edge_sweep=True)\n",
    "    c_erdos.append(gt.global_clustering(gE)[0])\n",
    "\n",
    "    gC = gt.Graph(g0)\n",
    "    gt.random_rewire(gC, model=\"configuration\", n_iter=10, edge_sweep=True)\n",
    "    c_conf.append(gt.global_clustering(gC)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1a015-1533-48bf-a04a-0eef1d8a3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_erdos = np.array(c_erdos)\n",
    "c_conf  = np.array(c_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384984e-82ec-4cfc-a315-d056b33c8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_pvalue(null_samples, obs):\n",
    "    return (1 + np.sum(null_samples >= obs)) / (len(null_samples) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44213f0c-3801-4ca6-87f6-afc8ca23b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pE = mc_pvalue(c_erdos, c_obs)\n",
    "pC = mc_pvalue(c_conf,  c_obs)\n",
    "\n",
    "print(f\"Observed global clustering: {c_obs:.4f}\")\n",
    "print(f\"Monte Carlo p-value under ER rewiring         : {pE:.4f}\")\n",
    "print(f\"Monte Carlo p-value under configuration model : {pC:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the null distributions\n",
    "plt.figure(figsize=(7, 4), dpi=100)\n",
    "\n",
    "plt.hist(c_erdos, bins=20, alpha=0.6, label=\"ER rewiring\", ec='.2')\n",
    "plt.hist(c_conf,  bins=20, alpha=0.6, label=\"Configuration rewiring\", ec='.2')\n",
    "plt.axvline(c_obs, linewidth=2, label=\"Observed\", color='k')\n",
    "plt.xlabel(\"Global clustering\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(fontsize='small')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a6b93",
   "metadata": {},
   "source": [
    "### Check-in\n",
    "\n",
    "1. In the Erdős-Rényi $G(n,p)$ model, what do we match when we choose $p$ from an observed graph?\n",
    "2. In the configuration model (degree-preserving rewiring), what *structure* is explicitly preserved, and what is allowed to change?\n",
    "3. If you compute a community score (like modularity) on an observed graph, what is the right baseline question to ask before interpreting it?\n",
    "4. When you choose a null model, what is the trade-off between \"too simple\" and \"too constrained\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03df1b-c00f-4930-b439-ab31d1e815fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8a0798d-1b9e-4b08-b900-27b4862a7afd",
   "metadata": {},
   "source": [
    "## Break!\n",
    "\n",
    "<p>\n",
    "<img src=\"images/graphtool.png\" width=\"2000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73211601-ab47-4f44-b6b1-24e399a06ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83c3cc-4bf4-465d-b6cc-6e53b2158325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b6e32-0b45-4acd-b166-b547f2010f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7ab10ed",
   "metadata": {},
   "source": [
    "________\n",
    "## 11. Stochastic block models (SBMs): from sociology to modern inference\n",
    "\n",
    "Before we talk about *inference*, we need a clear mental model of what an SBM is.\n",
    "\n",
    "### 11.1 The core idea\n",
    "\n",
    "An SBM assumes:\n",
    "1. Each node belongs to a latent group (block) $b_i \\in \\{1,2,\\dots,B\\}$\n",
    "2. Edge probabilities depend on groups:\n",
    "   $$P(A_{ij}=1 \\mid b_i=r, b_j=s) = p_{rs}$$\n",
    "3. We infer the partition $\\mathbf{b}$ (and sometimes $B$) from the observed graph.\n",
    "\n",
    "That is: **communities are parameters in a generative model**, not just \"clusters\".\n",
    "\n",
    "### 11.2 A short sociology history (why this is a social science model!)\n",
    "\n",
    "SBMs originated in social science and statistics to model *roles* and *positions*:\n",
    "- not only \"assortative communities\" (dense within groups),\n",
    "- but also:\n",
    "  - core-periphery\n",
    "  - bipartite structure\n",
    "  - hierarchical roles\n",
    "  - disassortative mixing\n",
    "\n",
    "### 11.3 Why nested SBMs?\n",
    "\n",
    "Real networks often have structure at *multiple scales*:\n",
    "- a few big clusters, each containing subclusters, etc.\n",
    "\n",
    "Nested SBMs encode this with a hierarchy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee6ab2",
   "metadata": {},
   "source": [
    "### 11.4 The canonical SBM likelihood (edges independent given blocks)\n",
    "\n",
    "For this, it'll helps to write down the likelihood explicitly. Assume an undirected simple graph with adjacency matrix $A$ and a block assignment $b_i \\in \\{1,\\dots,B\\}$. A (canonical) SBM says:\n",
    "\n",
    "- For each pair $i<j$, the edge indicator $A_{ij}$ is Bernoulli\n",
    "- The Bernoulli parameter depends only on the blocks $(b_i, b_j)$\n",
    "\n",
    "$$\n",
    "P(A_{ij}=1 \\mid b_i=r, b_j=s) = p_{rs}.\n",
    "$$\n",
    "\n",
    "Assuming conditional independence across pairs $(i,j)$, the likelihood is:\n",
    "\n",
    "$$\n",
    "P(A \\mid \\mathbf{b}, \\mathbf{p})\n",
    "= \\prod_{i<j} p_{b_i b_j}^{A_{ij}}\\,(1-p_{b_i b_j})^{1-A_{ij}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361795b3-fdde-4885-af19-a48ac84ec782",
   "metadata": {},
   "source": [
    "#### Sufficient statistics: edge counts between blocks\n",
    "\n",
    "Define:\n",
    "\n",
    "- $m_{rs}$ = number of observed edges between blocks $r$ and $s$\n",
    "- $n_{rs}$ = number of *possible* pairs between blocks $r$ and $s$\n",
    "\n",
    "For an undirected graph with block sizes $n_r$:\n",
    "\n",
    "- if $r \\neq s$: $n_{rs} = n_r n_s$\n",
    "- if $r = s$: $n_{rr} = \\binom{n_r}{2}$\n",
    "\n",
    "Then the log-likelihood can be written (up to constants) as:\n",
    "\n",
    "$$\n",
    "\\log P(A \\mid \\mathbf{b}, \\mathbf{p})\n",
    "= \\sum_{r\\le s} \\Bigl[\n",
    "m_{rs}\\log p_{rs} + (n_{rs}-m_{rs})\\log(1-p_{rs})\n",
    "\\Bigr].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c2029-8946-40b2-b86a-0c112a39a25b",
   "metadata": {},
   "source": [
    "#### MLE of $p_{rs}$\n",
    "\n",
    "Differentiate w.r.t. $p_{rs}$ and set to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial p_{rs}}\n",
    "\\Bigl[m_{rs}\\log p_{rs} + (n_{rs}-m_{rs})\\log(1-p_{rs})\\Bigr]\n",
    "= \\frac{m_{rs}}{p_{rs}} - \\frac{n_{rs}-m_{rs}}{1-p_{rs}} = 0.\n",
    "$$\n",
    "\n",
    "Solve for $p_{rs}$:\n",
    "\n",
    "$$\n",
    "\\hat p_{rs} = \\frac{m_{rs}}{n_{rs}}.\n",
    "$$\n",
    "\n",
    "So: **given a partition**, the best-fitting block probabilities are just \"edge density between the blocks.\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e7ed2-9eb5-48dc-a01e-074cb55823d0",
   "metadata": {},
   "source": [
    "### 11.5 Degree Correction\n",
    "\n",
    "A \"vanilla\" SBM can accidentally use blocks to explain degree variation (i.e., hubs become their own communities).\n",
    "\n",
    "A degree-corrected SBM adds per-vertex parameters (often written $\\theta_i$) so the model can match heavy-tailed degrees *without* inventing extra blocks.\n",
    "\n",
    "One common formulation (for multigraphs / Poisson edges) is:\n",
    "\n",
    "$$\n",
    "A_{ij} \\sim \\text{Poisson}(\\theta_i\\theta_j\\,\\omega_{b_i b_j}),\n",
    "$$\n",
    "\n",
    "where $\\omega_{rs}$ is a block-level connectivity parameter and $\\theta_i$ is a node-specific propensity.\n",
    "\n",
    "The high-level point is simple:\n",
    "\n",
    "- $\\theta_i$ explains \"how active\" node $i$ is\n",
    "- blocks explain \"who connects to whom beyond degree effects\"\n",
    "\n",
    "`graph-tool` supports degree correction and (crucially) uses MDL to decide whether the extra complexity is worth it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4fd848",
   "metadata": {},
   "source": [
    "A **degree-corrected SBM** separates:\n",
    "- community/block structure\n",
    "- individual node degree propensity\n",
    "\n",
    "In `graph-tool`, degree correction is commonly controlled with:\n",
    "```python\n",
    "state_args=dict(deg_corr=True)   # degree-corrected SBM\n",
    "state_args=dict(deg_corr=False)  # non-degree-corrected SBM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 11.6 Hands-on: generate a synthetic SBM with `graph-tool`\n",
    "\n",
    "We'll generate a 3-block assortative network and visualize it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c9fe7b",
   "metadata": {},
   "source": [
    "**Important implementation detail (graph-tool vs \"textbook\" SBM):** `gt.generate_sbm()` does *not* take a matrix of Bernoulli probabilities $p_{rs}$.\n",
    "\n",
    "Instead, its `probs[r, s]` parameter is an **edge propensity**: the *expected number of edges* between blocks $r$ and $s$ under a Poisson/multigraph SBM.\n",
    "\n",
    "If you want the familiar Bernoulli SBM probabilities $p_{rs}$, you can still use them — you just convert them to expected edge counts using the group sizes.\n",
    "\n",
    "Let $n_r$ be the number of vertices in block $r$.\n",
    "\n",
    "- For $r \\neq s$ (between blocks), the number of possible pairs is $n_r n_s$, so\n",
    "  $$\\mathbb{E}[m_{rs}] = p_{rs}\\,n_r n_s,$$\n",
    "  and you should pass `probs[r, s] = p_rs * n_r * n_s`.\n",
    "\n",
    "- For $r = s$ (within a block), the number of possible pairs is $\\binom{n_r}{2}$, so\n",
    "  $$\\mathbb{E}[m_{rr}] = p_{rr}\\,\\binom{n_r}{2}.$$\n",
    "  In the `graph-tool` convention for undirected graphs, you pass **twice** that value on the diagonal:\n",
    "  $$\\texttt{probs[r, r]} = 2\\,\\mathbb{E}[m_{rr}] = p_{rr}\\,n_r(n_r-1).$$\n",
    "\n",
    "For sparse graphs (small $p_{rs}$), the Poisson and Bernoulli formulations behave very similarly. If you push $p_{rs}$ into the dense regime, the Poisson model can generate multi-edges (which `graph-tool` can represent). If you need a strictly simple graph, you can use the `condensed=True` option or sample from a microcanonical model (see Appendix A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5699f-a258-4c36-90ec-33ac64855d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a planted partition: 3 blocks of equal size\n",
    "n_per_block = 50\n",
    "b = np.repeat(np.arange(3), n_per_block)\n",
    "\n",
    "p_in_pair  = 0.125   # per-pair within-block probability\n",
    "p_out_pair = 0.005   # per-pair between-block probability\n",
    "\n",
    "n = n_per_block\n",
    "\n",
    "p_in  = p_in_pair  * n * (n - 1)   # = 2 * E[m_rr] for undirected\n",
    "p_out = p_out_pair * n * n         # = E[m_rs] for r != s\n",
    "\n",
    "probs = np.array([\n",
    "    [p_in,  p_out, p_out],\n",
    "    [p_out, p_in,  p_out],\n",
    "    [p_out, p_out, p_in ],\n",
    "], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41dde73",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gt.generate_sbm(b, probs, directed=False)\n",
    "g.vp[\"block_true\"] = g.new_vp(\"int\", vals=b)\n",
    "\n",
    "print(\"Generated SBM: V =\", g.num_vertices(), \"E =\", g.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe5b08-93be-46bb-8d9d-419fbc42b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layout guided by true groups (helps drawing)\n",
    "pos = gt.sfdp_layout(g, groups=g.vp.block_true)\n",
    "\n",
    "gt.graph_draw(\n",
    "    g, pos=pos,\n",
    "    vertex_fill_color=g.vp.block_true,\n",
    "    vertex_size=8,\n",
    "    output_size=(500, 500)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a3da8",
   "metadata": {},
   "source": [
    "### 11.6 Estimate $\\hat p_{rs} = m_{rs}/n_{rs}$ from a generated SBM\n",
    "\n",
    "We generated the graph using a known block probability matrix `probs`.\n",
    "\n",
    "Now we'll treat the graph as \"observed data\" and (pretending we know the true partition) compute:\n",
    "\n",
    "- $m_{rs}$: edge counts between blocks\n",
    "- $n_{rs}$: possible pairs between blocks\n",
    "- $\\hat p_{rs} = m_{rs}/n_{rs}$: the canonical SBM MLE\n",
    "\n",
    "This is a nice sanity check that the math and code agree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a8ccdf-4016-4d5d-889d-33636b1168a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count edges between blocks in the generated graph\n",
    "b_true = g.vp[\"block_true\"].a\n",
    "B = int(b_true.max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbff33f-24e9-46f4-8e86-87b88887a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block sizes\n",
    "n_r = np.array([(b_true == r).sum() for r in range(B)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52511f1f-a997-4cf0-a8ee-2d4b5fb6d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible pairs n_rs\n",
    "n_rs = np.zeros((B, B), dtype=float)\n",
    "for r in range(B):\n",
    "    for s in range(B):\n",
    "        if r == s:\n",
    "            n_rs[r, s] = n_r[r] * (n_r[r] - 1) / 2\n",
    "        else:\n",
    "            n_rs[r, s] = n_r[r] * n_r[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e894b9-00e9-4ed2-b970-3cb77cdc19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed edges m_rs (undirected, count each edge once)\n",
    "m_rs = np.zeros((B, B), dtype=float)\n",
    "for e in g.edges():\n",
    "    r = int(b_true[int(e.source())])\n",
    "    s = int(b_true[int(e.target())])\n",
    "    m_rs[r, s] += 1\n",
    "    m_rs[s, r] += 1  # symmetric fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b41b1-4656-44d9-9087-803671240666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For undirected graphs, the diagonal was double-counted above (because r==s adds twice).\n",
    "for r in range(B):\n",
    "    m_rs[r, r] /= 2.0\n",
    "\n",
    "p_hat = m_rs / n_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True p matrix used to generate:\")\n",
    "print(np.round(probs, 3))\n",
    "print(\"\\nEstimated p_hat = m_rs / n_rs:\")\n",
    "print(np.round(p_hat, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09dd49f",
   "metadata": {},
   "source": [
    "### 11.7 Runtime and scaling: which community detection algorithms you can actually run\n",
    "\n",
    "When people say \"community detection,\" they often mean *very different* computational problems.\n",
    "\n",
    "It helps to sort algorithms into a few big families:\n",
    "\n",
    "1. **Edge-removal / divisive methods** (classic Girvan-Newman)\n",
    "2. **Objective maximization heuristics** (modularity: greedy, Louvain, Leiden)\n",
    "3. **Flow / coding methods** (Infomap / map equation)\n",
    "4. **Spectral relaxations** (eigenvectors of Laplacians / modularity matrices)\n",
    "5. **Generative-model inference** (SBMs: canonical/microcanonical, nested, overlapping)\n",
    "\n",
    "These families differ in:\n",
    "- what they optimize (a score? a likelihood? a code length?)\n",
    "- what they assume (null models, flow models, generative models)\n",
    "- and very practically: **what graph sizes they can handle**.\n",
    "\n",
    "#### A rough scaling comparison (rule-of-thumb, sparse graphs)\n",
    "\n",
    "Let $n$ = number of nodes and $m$ = number of edges.\n",
    "\n",
    "| Method family | Typical goal | Scaling intuition | What it's good for | What breaks |\n",
    "|---|---|---:|---|---|\n",
    "| Girvan-Newman (edge betweenness removal) | \"cut\" bridges between groups | **very expensive** (repeated betweenness computations) | teaching + tiny networks | anything beyond a few thousand edges |\n",
    "| Modularity heuristics (Louvain/Leiden) | maximize $Q$ | **fast** in practice (often near-linear in $m$ per pass) | very large graphs, quick partitions | resolution limit, degeneracy, \"high $Q$ by chance,\" no uncertainty |\n",
    "| Spectral clustering | relax discrete partition into eigenvectors | sparse eigensolvers can scale well, but still heavier than Louvain | clean blocky structure, embeddings | must choose $B$; can be brittle in noisy graphs |\n",
    "| Infomap / map equation | compress random-walk flow | typically fast in practice | flow-based communities | sensitive to dynamics assumptions |\n",
    "| SBM inference (`graph-tool`) | maximize posterior / minimize description length | often scales well in $m$, but with larger constant factors | principled model selection; uncertainty; \"no communities\" is allowed | model mismatch; heavier than pure heuristics |\n",
    "\n",
    "#### \"Explicit\" big-$O$ intuition (don't take these as promises)\n",
    "\n",
    "These are the most common back-of-the-envelope statements you'll see:\n",
    "\n",
    "- **Girvan-Newman (edge betweenness):**  \n",
    "  computing edge betweenness via Brandes is $O(nm)$ for unweighted graphs.  \n",
    "  If you recompute it after each edge removal for up to $m$ removals, you get roughly $O(nm^2)$ in the worst case.\n",
    "\n",
    "- **Greedy modularity (agglomerative):**  \n",
    "  can be around $O(m\\log n)$ to $O(mn)$ depending on implementation details (and how much bookkeeping is maintained).  \n",
    "  In practice it's often \"fine\" for medium graphs and then suddenly not fine.\n",
    "\n",
    "- **Louvain / Leiden:**  \n",
    "  usually described as approximately $O(m)$ per pass for sparse graphs (with a small number of passes), which is why they're popular at scale.\n",
    "\n",
    "- **Spectral methods:**  \n",
    "  computing the top $k$ eigenvectors of a sparse matrix can be about $O(km)$ per iteration of Lanczos/Arnoldi, times the number of iterations to converge.\n",
    "\n",
    "- **SBM inference (graph-tool):**  \n",
    "  the inference moves are designed so that many operations scale roughly with the number of affected edges, and a full sweep is often described as \"about $O(m)$\" (again: with constant factors and convergence time that depend on the data and the move set).\n",
    "\n",
    "Two things to internalize:\n",
    "\n",
    "- **There is no free lunch.** A method that returns uncertainty, chooses $B$, and avoids overfitting *must* do more work than a simple greedy score maximizer.\n",
    "- **Implementation matters.** `graph-tool` is fast because the heavy lifting is in C++ and the inference moves are carefully engineered.\n",
    "\n",
    "#### Micro-benchmark: how runtimes grow with graph size\n",
    "\n",
    "Below is a classroom-scale benchmark.  \n",
    "It is not a \"publishable benchmark,\" but it will give you a feel for how runtimes respond to $n$ and $m$.\n",
    "\n",
    "> If this runs slowly on your machine, reduce `sizes` or reduce the number of trials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f39c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------\n",
    "# Helper: generate a planted SBM in NetworkX\n",
    "# (We keep average degree roughly constant as n grows.)\n",
    "# -------------------------\n",
    "def planted_sbm_nx(n, B=4, c_in=12, c_out=2, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sizes = [n // B] * B\n",
    "    for i in range(n - sum(sizes)):\n",
    "        sizes[i] += 1\n",
    "\n",
    "    # Probabilities scale like 1/n to keep expected degree O(1)\n",
    "    p_in  = c_in / n\n",
    "    p_out = c_out / n\n",
    "    p = [[p_in if r == s else p_out for s in range(B)] for r in range(B)]\n",
    "\n",
    "    G = nx.stochastic_block_model(sizes, p, seed=seed)\n",
    "    return G\n",
    "\n",
    "# -------------------------\n",
    "# Timing wrappers\n",
    "# -------------------------\n",
    "def time_call(fn, *args, **kwargs):\n",
    "    t0 = time.perf_counter()\n",
    "    out = fn(*args, **kwargs)\n",
    "    t1 = time.perf_counter()\n",
    "    return (t1 - t0), out\n",
    "\n",
    "def nx_louvain_partition(G):\n",
    "    # NetworkX built-in Louvain (if available); returns list of sets\n",
    "    parts = louvain_communities(G, seed=0)\n",
    "    return parts\n",
    "\n",
    "def nx_greedy_partition(G):\n",
    "    # Greedy modularity (Clauset-Newman-Moore style)\n",
    "    parts = list(nx.algorithms.community.greedy_modularity_communities(G))\n",
    "    return parts\n",
    "\n",
    "def nx_girvan_one_split(G):\n",
    "    # Girvan-Newman is expensive; here we only compute the FIRST split.\n",
    "    gen = nx.algorithms.community.girvan_newman(G)\n",
    "    first = next(gen)\n",
    "    return list(first)\n",
    "\n",
    "def gt_nested_sbm(g_gt):\n",
    "    state = gt.minimize_nested_blockmodel_dl(g_gt, state_args=dict(deg_corr=True))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309c883-004f-4b27-ab7d-432176b54075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Benchmark settings\n",
    "# -------------------------\n",
    "sizes = [300, 600, 1200]\n",
    "sizes = np.logspace(2,4.2,11).astype(int)\n",
    "results = []\n",
    "\n",
    "for n in sizes:\n",
    "    print(n)\n",
    "    G = planted_sbm_nx(n, B=4, c_in=12, c_out=2, seed=0)\n",
    "    m = G.number_of_edges()\n",
    "\n",
    "    row = dict(n=n, m=m)\n",
    "\n",
    "    t, _ = time_call(nx_louvain_partition, G)\n",
    "    row[\"nx_louvain_sec\"] = t\n",
    "\n",
    "    # Greedy modularity (usually slower than Louvain, still OK for moderate n)\n",
    "    t, _ = time_call(nx_greedy_partition, G)\n",
    "    row[\"nx_greedy_mod_sec\"] = t\n",
    "\n",
    "    # Girvan-Newman (very expensive): only run on the smallest size\n",
    "    if n == min(sizes):\n",
    "        t, _ = time_call(nx_girvan_one_split, G)\n",
    "        row[\"nx_girvan_firstsplit_sec\"] = t\n",
    "    else:\n",
    "        row[\"nx_girvan_firstsplit_sec\"] = np.nan\n",
    "    # graph-tool nested SBM inference (convert graph first)\n",
    "    g_gt = nx_to_gt(G)\n",
    "    t, _ = time_call(gt_nested_sbm, g_gt)\n",
    "    row[\"gt_nested_sbm_sec\"] = t\n",
    "    results.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ef70c-c2c2-44f3-90ca-ef49b17bee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = pd.DataFrame(results)\n",
    "df_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fdbb36-c508-42fe-bb8e-3bab2d99523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick log-log plot (runtime vs edges)\n",
    "plt.figure(figsize=(7,4),dpi=100)\n",
    "\n",
    "plt.plot(df_bench[\"m\"], df_bench[\"nx_louvain_sec\"], marker=\"o\", label=\"NetworkX Louvain\")\n",
    "plt.plot(df_bench[\"m\"], df_bench[\"nx_greedy_mod_sec\"], marker=\"o\", label=\"NetworkX greedy modularity\")\n",
    "plt.plot(df_bench[\"m\"], df_bench[\"gt_nested_sbm_sec\"], marker=\"o\", label=\"graph-tool nested SBM (deg-corr)\")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"m = # edges\")\n",
    "plt.ylabel(\"runtime (seconds)\")\n",
    "plt.title(\"Micro-benchmark: runtime scaling (very rough)\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49bacd7",
   "metadata": {},
   "source": [
    "## 12. Statistical inference primer: Bayes, evidence, and MDL (with code)\n",
    "\n",
    "This section is intentionally **slow and explicit**.  \n",
    "If you've never taken a statistics class, that's okay — read carefully and run the code.\n",
    "\n",
    "We'll build an intuition that transfers directly to SBMs in `graph-tool`.\n",
    "\n",
    "---\n",
    "\n",
    "### 12.1 A data-generating story (a.k.a. a model)\n",
    "\n",
    "A statistical model is a **story** for how data are generated.\n",
    "\n",
    "- Data: $D$ (for networks, typically the adjacency matrix $A$ or edge list)\n",
    "- Parameters: $\\theta$ (numbers we don't know)\n",
    "- Model class: $M$ (assumptions + which distributions are used)\n",
    "\n",
    "The model gives you probabilities like:\n",
    "\n",
    "$$P(D \\mid \\theta, M).$$\n",
    "\n",
    "---\n",
    "\n",
    "### 12.2 Likelihood and log-likelihood\n",
    "\n",
    "The **likelihood function** is:\n",
    "\n",
    "$$L(\\theta) = P(D \\mid \\theta, M).$$\n",
    "\n",
    "We often work with the **log-likelihood**:\n",
    "\n",
    "$$\\ell(\\theta) = \\log L(\\theta),$$\n",
    "\n",
    "because:\n",
    "- products of probabilities become sums\n",
    "- underflow is avoided\n",
    "- gradients are easier\n",
    "\n",
    "---\n",
    "\n",
    "### 12.3 Maximum likelihood (MLE)\n",
    "\n",
    "The MLE is:\n",
    "\n",
    "$$\\hat \\theta_{\\mathrm{MLE}} = \\arg\\max_\\theta P(D \\mid \\theta, M)\n",
    "= \\arg\\max_\\theta \\ell(\\theta).$$\n",
    "\n",
    "**Big idea:** MLE focuses on *fit* only (can overfit).\n",
    "\n",
    "---\n",
    "\n",
    "### 12.4 Prior beliefs (Bayesian modeling)\n",
    "\n",
    "A Bayesian model adds a **prior**:\n",
    "\n",
    "$$P(\\theta \\mid M).$$\n",
    "\n",
    "This encodes what values of $\\theta$ are plausible *before seeing data*.\n",
    "\n",
    "---\n",
    "\n",
    "### 12.5 Posterior (Bayes' rule)\n",
    "\n",
    "After observing data $D$, we update our beliefs:\n",
    "\n",
    "$$P(\\theta \\mid D, M) = \\frac{P(D \\mid \\theta, M)\\,P(\\theta \\mid M)}{P(D \\mid M)}.$$\n",
    "\n",
    "Where the denominator $P(D \\mid M)$ is the **evidence**.\n",
    "\n",
    "---\n",
    "\n",
    "### 12.6 Evidence (marginal likelihood): why it matters for \"communities\"\n",
    "\n",
    "$$P(D \\mid M) = \\int P(D \\mid \\theta, M)\\,P(\\theta \\mid M)\\,d\\theta.$$\n",
    "\n",
    "Evidence is the probability your model assigns to the data **after averaging over parameter uncertainty**.\n",
    "\n",
    "A flexible model can fit many datasets, but that means its probability mass is spread thinly → evidence can be small unless the data truly support the complexity.\n",
    "\n",
    "This is one formalization of **Occam's razor**.\n",
    "\n",
    "---\n",
    "\n",
    "### 12.7 Model comparison (Bayes factors)\n",
    "\n",
    "For two models $M_0, M_1$:\n",
    "\n",
    "$$\\mathrm{BF}_{10} = \\frac{P(D \\mid M_1)}{P(D \\mid M_0)}.$$\n",
    "\n",
    "If $\\mathrm{BF}_{10} \\gg 1$, the data support $M_1$ over $M_0$ (assuming equal prior odds).\n",
    "\n",
    "---\n",
    "\n",
    "### 12.8 Bits, nats, and description length\n",
    "\n",
    "Information-theoretic coding says:\n",
    "\n",
    "- a probability $P$ corresponds to a code length $-\\log P$\n",
    "\n",
    "If we use natural logs:\n",
    "\n",
    "$$\\text{code length (nats)} = -\\ln P.$$\n",
    "\n",
    "If we use base-2 logs:\n",
    "\n",
    "$$\\text{code length (bits)} = -\\log_2 P.$$\n",
    "\n",
    "Conversion:\n",
    "\n",
    "$$\\text{bits} = \\frac{\\text{nats}}{\\ln 2}.$$\n",
    "\n",
    "---\n",
    "\n",
    "### 12.9 Minimum description length (MDL): inference as compression\n",
    "\n",
    "MDL reframes inference as compression:\n",
    "\n",
    "> The best model is the one that lets you describe the data in the fewest bits.\n",
    "\n",
    "A useful mental model:\n",
    "\n",
    "$$\\Sigma(M, \\theta, D) \\approx -\\log P(D, \\theta \\mid M)\n",
    "= -\\log P(D\\mid\\theta, M) - \\log P(\\theta\\mid M),$$\n",
    "\n",
    "and if the model also has latent structure (like communities) you include that too:\n",
    "\n",
    "$$-\\log P(D, \\theta, b \\mid M).$$\n",
    "\n",
    "So:\n",
    "- smaller description length ↔ higher posterior probability\n",
    "- choosing $B$ (number of blocks) becomes **model selection**\n",
    "\n",
    "In `graph-tool`, you will see this directly:\n",
    "\n",
    "- `state.entropy()` returns a description length $\\Sigma$ (in nats)\n",
    "- `minimize_blockmodel_dl()` searches for partitions minimizing $\\Sigma$\n",
    "\n",
    "---\n",
    "\n",
    "### 12.10 A toy Bayesian inference problem: coin flips\n",
    "\n",
    "We'll now do a fully worked example with equations and code.\n",
    "\n",
    "This will give you:\n",
    "- a concrete posterior you can visualize\n",
    "- an evidence calculation you can compute in closed form\n",
    "- an Occam factor you can *see*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96281939-a2a8-4110-8e9c-212e0d6395b5",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img src=\"images/mdl_spectrum.png\" width=\"650\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d1cb7",
   "metadata": {},
   "source": [
    "### 12.10.1 Breakaway: likelihood is not a probability distribution (and why conjugate priors feel \"magical\")\n",
    "\n",
    "One conceptual jump that trips people up:\n",
    "\n",
    "- A **likelihood** $L(p) = P(D \\mid p)$ is a function of $p$ given fixed data $D$.\n",
    "- A **probability distribution** over $p$ must integrate to 1.\n",
    "\n",
    "These are not the same object.\n",
    "\n",
    "For coin flips, the likelihood is:\n",
    "\n",
    "$$\n",
    "L(p) = P(k \\mid p) \\propto p^k (1-p)^{n-k}.\n",
    "$$\n",
    "\n",
    "If you *normalize* that function over $p \\in [0,1]$, you get a proper density over $p$.\n",
    "\n",
    "And here is the key observation:\n",
    "\n",
    "> With a uniform prior $p \\sim \\mathrm{Beta}(1,1)$, the **posterior density over $p$ is proportional to the likelihood**.\n",
    "\n",
    "So \"posterior = normalized likelihood\" is literally true *in that special case*.\n",
    "\n",
    "Let's watch this happen numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18acdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "# Fix some fake coin flip data:\n",
    "n = 20\n",
    "k = 13\n",
    "\n",
    "p_grid = np.linspace(1e-4, 1-1e-4, 2000)\n",
    "\n",
    "# Likelihood as a function of p (up to a constant binomial coefficient)\n",
    "L = (p_grid**k) * ((1 - p_grid)**(n - k))\n",
    "\n",
    "# This is NOT a probability distribution over p:\n",
    "area_L = np.trapezoid(L, p_grid)\n",
    "print(\"Integral of likelihood over p (not 1):\", area_L)\n",
    "\n",
    "# Normalize it by hand\n",
    "L_norm = L / area_L\n",
    "area_L_norm = np.trapezoid(L_norm, p_grid)\n",
    "print(\"Integral after normalization:\", area_L_norm)\n",
    "\n",
    "# Under a uniform prior Beta(1,1), posterior is Beta(k+1, n-k+1)\n",
    "post_pdf = beta_dist.pdf(p_grid, k + 1, n - k + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ec2fe9-2463-4303-9a6d-80f04c8721cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare shapes\n",
    "plt.figure(figsize=(7, 3.5),dpi=100)\n",
    "plt.plot(p_grid, L_norm, label=\"normalized likelihood (shape only)\")\n",
    "plt.plot(p_grid, post_pdf, label=\"Beta(k+1, n-k+1) posterior (uniform prior)\", ls='--')\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"density (scaled)\")\n",
    "plt.title(\"Normalized likelihood matches Beta posterior (uniform prior)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# MLE is k/n\n",
    "print(\"MLE p_hat = k/n =\", k/n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87174103",
   "metadata": {},
   "source": [
    "### 12.11 Worked example: coin flips (Bernoulli + Beta)\n",
    "\n",
    "This example is deliberately simple, but the logic transfers to SBMs.\n",
    "\n",
    "We observe flips $x_1, \\dots, x_n$ with $x_i \\in \\{0,1\\}$.\n",
    "\n",
    "Let:\n",
    "- $k = \\sum_i x_i$ be the number of heads\n",
    "- $n-k$ be the number of tails\n",
    "\n",
    "#### Model (likelihood)\n",
    "\n",
    "$$x_i \\sim \\mathrm{Bernoulli}(p)$$\n",
    "\n",
    "So the likelihood is:\n",
    "\n",
    "$$P(x \\mid p) = \\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}\n",
    "= p^k (1-p)^{n-k}.$$\n",
    "\n",
    "This is basically the Binomial likelihood written in \"product form\".\n",
    "\n",
    "A small but important move: take logs.\n",
    "\n",
    "$$\n",
    "\\ell(p) = \\log P(x\\mid p)\n",
    "= k\\log p + (n-k)\\log(1-p).\n",
    "$$\n",
    "\n",
    "This turns a product into a sum, and it also makes the geometry visible:\n",
    "- $\\ell(p)$ is **concave** (one hump)\n",
    "- the maximum occurs at $\\hat p = k/n$ (we'll show this in code)\n",
    "\n",
    "#### Prior: why a Beta prior?\n",
    "\n",
    "Use a Beta prior:\n",
    "\n",
    "$$p \\sim \\mathrm{Beta}(\\alpha, \\beta),\n",
    "\\qquad\n",
    "P(p) = \\frac{1}{B(\\alpha,\\beta)}\\,p^{\\alpha-1}(1-p)^{\\beta-1}.$$\n",
    "\n",
    "Why this prior shows up constantly:\n",
    "\n",
    "1. **Right support.** $p$ lives on $[0,1]$, and Beta lives on $[0,1]$.\n",
    "2. **Conjugacy.** The Bernoulli/Binomial likelihood has the form $p^k(1-p)^{n-k}$, and the Beta prior has the form $p^{\\alpha-1}(1-p)^{\\beta-1}$.  \n",
    "   When you multiply them, you just add exponents → the posterior is Beta again.\n",
    "3. **Pseudo-count interpretation.** A useful mental model:\n",
    "   - $\\alpha$ behaves like \"prior heads + 1\"\n",
    "   - $\\beta$ behaves like \"prior tails + 1\"\n",
    "   - $\\alpha+\\beta$ controls how strongly the prior pulls on the posterior (an \"equivalent sample size\")\n",
    "4. **Convenient evidence.** The Beta prior makes the evidence integral doable in closed form (Beta-Binomial).  \n",
    "   This is exactly the kind of move you want when doing model selection.\n",
    "\n",
    "Two common \"default\" choices:\n",
    "- **Uniform** prior: $\\mathrm{Beta}(1,1)$ (flat on $[0,1]$)\n",
    "- **Jeffreys** prior: $\\mathrm{Beta}(1/2, 1/2)$ (often used as a reparameterization-invariant default)\n",
    "\n",
    "#### Posterior (conjugacy)\n",
    "\n",
    "Multiply likelihood × prior:\n",
    "\n",
    "$$P(p\\mid x) \\propto p^{k+\\alpha-1}(1-p)^{(n-k)+\\beta-1}$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$p \\mid x \\sim \\mathrm{Beta}(\\alpha + k,\\; \\beta + n - k).$$\n",
    "\n",
    "#### Posterior summaries you should know\n",
    "\n",
    "Posterior mean:\n",
    "\n",
    "$$\\mathbb{E}[p\\mid x] = \\frac{\\alpha+k}{\\alpha+\\beta+n}.$$\n",
    "\n",
    "Posterior mode (MAP), when $\\alpha+k>1$ and $\\beta+n-k>1$:\n",
    "\n",
    "$$p_{\\mathrm{MAP}} = \\frac{\\alpha+k-1}{\\alpha+\\beta+n-2}.$$\n",
    "\n",
    "Posterior predictive for a new flip:\n",
    "\n",
    "$$P(x_{\\mathrm{new}}=1 \\mid x) = \\mathbb{E}[p\\mid x].$$\n",
    "\n",
    "Next we compute all of these in code, and then we'll play with different priors to see what changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate coin flips (synthetic data)\n",
    "rng = np.random.default_rng(3)\n",
    "\n",
    "n = 40\n",
    "p_true = 0.65\n",
    "x = rng.binomial(1, p_true, size=n)\n",
    "\n",
    "k = int(x.sum())\n",
    "print(f\"{k} heads out of {n} flips (true p={p_true})\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Frequentist MLE\n",
    "# ---------------------------\n",
    "p_mle = k / n\n",
    "print(\"MLE p_hat = k/n =\", round(p_mle, 3))\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Bayesian prior + posterior\n",
    "# ---------------------------\n",
    "alpha, beta = 1, 1  # Beta(1,1) = uniform prior on [0,1]\n",
    "alpha_post = alpha + k\n",
    "beta_post  = beta + (n - k)\n",
    "\n",
    "# Posterior mean\n",
    "p_mean = alpha_post / (alpha_post + beta_post)\n",
    "\n",
    "# Posterior MAP (mode), defined only if alpha_post>1 and beta_post>1\n",
    "if alpha_post > 1 and beta_post > 1:\n",
    "    p_map = (alpha_post - 1) / (alpha_post + beta_post - 2)\n",
    "else:\n",
    "    p_map = np.nan\n",
    "\n",
    "# Posterior variance\n",
    "p_var = (alpha_post * beta_post) / (((alpha_post + beta_post) ** 2) * (alpha_post + beta_post + 1))\n",
    "\n",
    "print(\"Posterior mean E[p|x] =\", round(p_mean, 3))\n",
    "print(\"Posterior MAP  p_MAP  =\", round(p_map, 3))\n",
    "print(\"Posterior sd          =\", round(np.sqrt(p_var), 3))\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Posterior credible interval\n",
    "# ---------------------------\n",
    "p_samps = rng.beta(alpha_post, beta_post, size=50_000)\n",
    "ci = np.quantile(p_samps, [0.025, 0.975])\n",
    "print(\"95% credible interval:\", tuple(np.round(ci, 3)))\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Posterior predictive check\n",
    "# ---------------------------\n",
    "# Predict number of heads in m future flips by sampling p ~ posterior, then Binomial(m,p)\n",
    "m = 40\n",
    "p_post = rng.beta(alpha_post, beta_post, size=20_000)\n",
    "k_future = rng.binomial(m, p_post)\n",
    "\n",
    "print(\"Posterior predictive: E[#heads in next 40 flips] ≈\", round(k_future.mean(), 1))\n",
    "\n",
    "# ---------------------------\n",
    "# Plot posterior over p\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(7, 3.5))\n",
    "plt.hist(p_samps, bins=80, density=True)\n",
    "plt.axvline(p_true, linewidth=2, label=\"true p\")\n",
    "plt.axvline(p_mle,  linewidth=2, label=\"MLE\")\n",
    "plt.axvline(p_mean, linewidth=2, label=\"posterior mean\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"posterior density (approx)\")\n",
    "plt.title(\"Posterior over p for a Bernoulli coin\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot posterior predictive distribution over future heads\n",
    "plt.figure(figsize=(7, 3.5))\n",
    "plt.hist(k_future, bins=np.arange(m+2)-0.5, density=True)\n",
    "plt.axvline(k, linewidth=2, label=\"observed k (in current data)\")\n",
    "plt.xlabel(f\"# heads in next {m} flips\")\n",
    "plt.ylabel(\"probability (approx)\")\n",
    "plt.title(\"Posterior predictive distribution\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5737eb",
   "metadata": {},
   "source": [
    "#### 12.11.1 Micro-illustration: priors as pseudo-counts (and how \"strong\" a prior is)\n",
    "\n",
    "In Beta-Bernoulli models, $\\alpha$ and $\\beta$ can be read as *pseudo-counts*:\n",
    "\n",
    "- prior \"heads\" $\\approx \\alpha - 1$\n",
    "- prior \"tails\" $\\approx \\beta - 1$\n",
    "- prior strength $\\approx \\alpha + \\beta$ (bigger means the prior matters more)\n",
    "\n",
    "Let's keep the *same data* and compare several priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d930c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "# We'll re-use the observed k and n from the previous cell.\n",
    "# If you rerun cells out of order, recompute them:\n",
    "try:\n",
    "    k\n",
    "    n\n",
    "except NameError:\n",
    "    n = 40\n",
    "    k = 26\n",
    "\n",
    "priors = [\n",
    "    (\"Uniform  Beta(1,1)\", 1, 1),\n",
    "    (\"Jeffreys Beta(1/2,1/2)\", 0.5, 0.5),\n",
    "    (\"Mild     Beta(2,2)\", 2, 2),\n",
    "    (\"Strong   Beta(20,20)\", 20, 20),\n",
    "    (\"Skewed   Beta(10,2)\", 10, 2),\n",
    "]\n",
    "\n",
    "p_grid = np.linspace(1e-4, 1 - 1e-4, 2000)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "for label, a, b in priors:\n",
    "    a_post = a + k\n",
    "    b_post = b + (n - k)\n",
    "\n",
    "    pdf = beta_dist.pdf(p_grid, a_post, b_post)\n",
    "    plt.plot(p_grid, pdf, label=f\"{label} → post Beta({a_post:.1f},{b_post:.1f})\")\n",
    "\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"posterior density\")\n",
    "plt.title(\"Posterior depends on the prior most when data are scarce\")\n",
    "plt.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show posterior means numerically\n",
    "rows = []\n",
    "for label, a, b in priors:\n",
    "    a_post = a + k\n",
    "    b_post = b + (n - k)\n",
    "    post_mean = a_post / (a_post + b_post)\n",
    "    prior_mean = a / (a + b)\n",
    "    rows.append(dict(prior=label, prior_mean=prior_mean, post_mean=post_mean, prior_strength=a+b))\n",
    "\n",
    "pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f7e020",
   "metadata": {},
   "source": [
    "### 12.12 Evidence and \"Occam's razor\" in a tiny model comparison\n",
    "\n",
    "We'll compare two models:\n",
    "\n",
    "- $M_0$: **fair coin**, fixed $p_0 = 0.5$ (no free parameters)\n",
    "- $M_1$: **unknown** $p$, with prior $p \\sim \\mathrm{Beta}(\\alpha,\\beta)$\n",
    "\n",
    "Let $k$ be the number of heads out of $n$ flips.\n",
    "\n",
    "#### Evidence under the fair-coin model $M_0$\n",
    "\n",
    "The probability of observing exactly $k$ heads is:\n",
    "\n",
    "$$P(k \\mid M_0) = \\binom{n}{k} p_0^k (1-p_0)^{n-k}.$$\n",
    "\n",
    "(If you treat the full sequence $x_1,\\dots,x_n$ as the data, you can drop the $\\binom{n}{k}$ term.  \n",
    "Using $k$ as a sufficient statistic keeps it clean.)\n",
    "\n",
    "#### Evidence under the unknown-$p$ model $M_1$\n",
    "\n",
    "Start with:\n",
    "\n",
    "$$P(k \\mid p) = \\binom{n}{k} p^k (1-p)^{n-k},\n",
    "\\qquad\n",
    "p \\sim \\mathrm{Beta}(\\alpha,\\beta).$$\n",
    "\n",
    "Then integrate out $p$:\n",
    "\n",
    "$$P(k \\mid M_1)\n",
    "= \\int_0^1 P(k \\mid p)\\,P(p)\\,dp.$$\n",
    "\n",
    "Because the Beta is conjugate to the Binomial, this integral has a closed form:\n",
    "\n",
    "$$P(k \\mid M_1)\n",
    "= \\binom{n}{k}\\,\\frac{B(k+\\alpha,\\; n-k+\\beta)}{B(\\alpha,\\beta)},$$\n",
    "\n",
    "where the Beta function is:\n",
    "\n",
    "$$B(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}.$$\n",
    "\n",
    "#### Bayes factor\n",
    "\n",
    "$$\\mathrm{BF}_{10} = \\frac{P(k\\mid M_1)}{P(k\\mid M_0)}.$$\n",
    "\n",
    "A useful habit: compute this in log space:\n",
    "\n",
    "$$\\log \\mathrm{BF}_{10} = \\log P(k\\mid M_1) - \\log P(k\\mid M_0).$$\n",
    "\n",
    "---\n",
    "\n",
    "Now we compute these quantities in code, and interpret the Bayes factor as an \"Occam factor\":  \n",
    "$M_1$ has flexibility, but it only wins if the data *demand* that flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ce3b9",
   "metadata": {},
   "source": [
    "#### 12.12.1 (Math breakaway) Doing the evidence integral step by step\n",
    "\n",
    "We claimed (for the unknown-$p$ model):\n",
    "\n",
    "$$\n",
    "P(k \\mid M_1) = \\binom{n}{k}\\,\\frac{B(k+\\alpha,\\; n-k+\\beta)}{B(\\alpha,\\beta)}.\n",
    "$$\n",
    "\n",
    "Here is the algebra that gets you there.\n",
    "\n",
    "Start with:\n",
    "\n",
    "- Likelihood: $P(k\\mid p) = \\binom{n}{k} p^k (1-p)^{n-k}$\n",
    "- Prior: $P(p) = \\frac{1}{B(\\alpha,\\beta)}p^{\\alpha-1}(1-p)^{\\beta-1}$\n",
    "\n",
    "Evidence is the integral:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(k\\mid M_1)\n",
    "&= \\int_0^1 P(k\\mid p)P(p)\\,dp \\\\\n",
    "&= \\int_0^1 \\binom{n}{k} p^k (1-p)^{n-k}\n",
    "\\cdot \\frac{1}{B(\\alpha,\\beta)}p^{\\alpha-1}(1-p)^{\\beta-1}\\,dp \\\\\n",
    "&= \\binom{n}{k}\\,\\frac{1}{B(\\alpha,\\beta)}\n",
    "\\int_0^1 p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1}\\,dp.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But the Beta function is defined as:\n",
    "\n",
    "$$\n",
    "B(a,b) = \\int_0^1 t^{a-1}(1-t)^{b-1}\\,dt.\n",
    "$$\n",
    "\n",
    "So our integral is **exactly**:\n",
    "\n",
    "$$\n",
    "\\int_0^1 p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1}\\,dp\n",
    "= B(k+\\alpha,\\; n-k+\\beta).\n",
    "$$\n",
    "\n",
    "Plug it back in and we're done:\n",
    "\n",
    "$$\n",
    "P(k \\mid M_1) = \\binom{n}{k}\\,\\frac{B(k+\\alpha,\\; n-k+\\beta)}{B(\\alpha,\\beta)}.\n",
    "$$\n",
    "\n",
    "Next: let's verify the closed form with a numerical integral in code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "\n",
    "# Pick a small example (use the same notation as above)\n",
    "n = 20\n",
    "k = 13\n",
    "alpha, beta = 2.0, 2.0\n",
    "\n",
    "binom = special.comb(n, k, exact=False)\n",
    "B_ab = special.beta(alpha, beta)\n",
    "\n",
    "def integrand(p):\n",
    "    prior = (p**(alpha - 1) * (1 - p)**(beta - 1)) / B_ab\n",
    "    like  = binom * (p**k) * ((1 - p)**(n - k))\n",
    "    return like * prior\n",
    "\n",
    "num_int, num_err = integrate.quad(integrand, 0, 1)\n",
    "\n",
    "closed_form = binom * special.beta(k + alpha, n - k + beta) / B_ab\n",
    "\n",
    "print(\"Numerical integral:\", num_int, \"(±\", num_err, \")\")\n",
    "print(\"Closed form      :\", closed_form)\n",
    "print(\"Absolute diff    :\", abs(num_int - closed_form))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3730c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll compute log-evidences to avoid underflow.\n",
    "\n",
    "def log_choose(n, k):\n",
    "    # log binomial coefficient using Gamma functions\n",
    "    return special.gammaln(n + 1) - special.gammaln(k + 1) - special.gammaln(n - k + 1)\n",
    "\n",
    "def log_evidence_fair(n, k, p0=0.5):\n",
    "    # P(k | M0) = C(n,k) p0^k (1-p0)^(n-k)\n",
    "    return log_choose(n, k) + k * np.log(p0) + (n - k) * np.log(1 - p0)\n",
    "\n",
    "def log_evidence_beta_binomial(n, k, alpha=1, beta=1):\n",
    "    # P(k | M1) = C(n,k) * B(k+alpha, n-k+beta) / B(alpha,beta)\n",
    "    return (\n",
    "        log_choose(n, k)\n",
    "        + special.betaln(k + alpha, n - k + beta)\n",
    "        - special.betaln(alpha, beta)\n",
    "    )\n",
    "\n",
    "# Use the k, n from the previous cell (coin flips)\n",
    "le0 = log_evidence_fair(n, k, p0=0.5)\n",
    "le1 = log_evidence_beta_binomial(n, k, alpha=1, beta=1)\n",
    "\n",
    "log_bf_10 = le1 - le0\n",
    "bf_10 = float(np.exp(log_bf_10))\n",
    "\n",
    "print(\"log evidence M0 (fair):\", round(le0, 3))\n",
    "print(\"log evidence M1 (unknown p with Beta(1,1)):\", round(le1, 3))\n",
    "print(\"log Bayes factor log BF_10 =\", round(log_bf_10, 3))\n",
    "print(\"Bayes factor BF_10 =\", round(bf_10, 3))\n",
    "\n",
    "# Convert log BF to bits (sometimes easier to interpret as \"compression gain\")\n",
    "log_bf_bits = log_bf_10 / np.log(2)\n",
    "print(\"log BF_10 in bits =\", round(log_bf_bits, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366feab",
   "metadata": {},
   "source": [
    "### 12.13 Translating this to SBMs (networks)\n",
    "\n",
    "For SBMs, the \"coin flip\" example generalizes like this:\n",
    "\n",
    "- **Data** $D$: the graph $G$, often represented by an adjacency matrix $A$\n",
    "- **Latent structure** $b$: a partition of nodes into blocks/communities\n",
    "- **Parameters** $\\theta$: quantities controlling edge probabilities/rates between blocks, and (sometimes) node propensities\n",
    "\n",
    "A very simple **(canonical) SBM** story for an undirected simple graph:\n",
    "\n",
    "1. Each node $i$ belongs to a block $b_i \\in \\{1,\\dots,B\\}$\n",
    "2. For each pair $(i,j)$, generate an edge:\n",
    "\n",
    "$$A_{ij} \\sim \\mathrm{Bernoulli}(p_{b_i b_j}),\n",
    "\\qquad i<j$$\n",
    "\n",
    "so the likelihood is:\n",
    "\n",
    "$$P(A \\mid b, p)\n",
    "= \\prod_{i<j} p_{b_i b_j}^{A_{ij}} (1-p_{b_i b_j})^{1-A_{ij}}.$$\n",
    "\n",
    "A common (and important) extension is the **degree-corrected SBM (DC-SBM)**,\n",
    "which introduces node-level propensities so high-degree nodes don't force spurious communities.\n",
    "One convenient DC-SBM form uses Poisson edges:\n",
    "\n",
    "$$A_{ij} \\sim \\mathrm{Poisson}(\\theta_i \\theta_j \\,\\omega_{b_i b_j}).$$\n",
    "\n",
    "(There are several equivalent formulations; `graph-tool` uses microcanonical ensembles under the hood.)\n",
    "\n",
    "---\n",
    "\n",
    "### 12.14 Evidence and MDL in SBMs\n",
    "\n",
    "The Bayesian goal is conceptually:\n",
    "\n",
    "$$P(b \\mid G, M) \\propto P(G \\mid b, M)\\,P(b \\mid M),$$\n",
    "\n",
    "where\n",
    "\n",
    "$$P(G \\mid b, M) = \\int P(G \\mid b, \\theta, M)\\,P(\\theta \\mid M)\\,d\\theta.$$\n",
    "\n",
    "In SBMs, the parameter space $\\theta$ can be large, so `graph-tool` uses an MDL/Bayesian formulation where it minimizes a **description length**.\n",
    "\n",
    "A useful heuristic you will use constantly:\n",
    "\n",
    "- `state.entropy()` returns a description length $\\Sigma$ (in nats)\n",
    "- smaller $\\Sigma$ means a more plausible partition/model\n",
    "\n",
    "For comparing two modeling choices $M_1$ vs $M_0$ (e.g., degree-corrected vs not):\n",
    "\n",
    "$$\\frac{P(G\\mid M_1)}{P(G\\mid M_0)} \\approx \\exp\\bigl(-(\\Sigma_1-\\Sigma_0)\\bigr).$$\n",
    "\n",
    "So a difference $\\Delta\\Sigma = \\Sigma_1-\\Sigma_0$ translates directly into an evidence ratio.\n",
    "\n",
    "Next we actually do the inference in `graph-tool`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2374e",
   "metadata": {},
   "source": [
    "### 12.15 Breakaway: a canonical SBM is \"many coin flips\" (and the Beta prior comes back)\n",
    "\n",
    "In the simple (canonical) SBM we wrote:\n",
    "\n",
    "$$\n",
    "P(A \\mid b, p) = \\prod_{i<j} p_{b_i b_j}^{A_{ij}} (1-p_{b_i b_j})^{1-A_{ij}}.\n",
    "$$\n",
    "\n",
    "A useful re-expression groups terms by **block pairs**.\n",
    "\n",
    "Let $N_r$ be the number of nodes in block $r$.\n",
    "\n",
    "Define the number of *possible* edges between blocks:\n",
    "\n",
    "- if $r \\neq s$: $n_{rs} = N_r N_s$\n",
    "- if $r = s$: $n_{rr} = \\binom{N_r}{2}$\n",
    "\n",
    "and the number of *observed* edges between blocks:\n",
    "\n",
    "$$m_{rs} = \\sum_{i<j} A_{ij}\\,\\mathbf{1}\\{b_i=r,\\;b_j=s\\}.$$\n",
    "\n",
    "Then the likelihood can be rewritten as:\n",
    "\n",
    "$$\n",
    "P(A \\mid b, p)\n",
    "= \\prod_{r \\le s} p_{rs}^{m_{rs}} (1-p_{rs})^{n_{rs}-m_{rs}}.\n",
    "$$\n",
    "\n",
    "Now you can see the analogy:\n",
    "\n",
    "- For each block pair $(r,s)$, you have $n_{rs}$ Bernoulli trials\n",
    "- with $m_{rs}$ successes\n",
    "- and unknown success probability $p_{rs}$\n",
    "\n",
    "So it's literally \"coin flips,\" repeated for every block pair.\n",
    "\n",
    "#### Why the Beta prior is a natural default here\n",
    "\n",
    "If we set independent priors\n",
    "\n",
    "$$p_{rs} \\sim \\mathrm{Beta}(\\alpha,\\beta),$$\n",
    "\n",
    "then for each block pair the integral\n",
    "\n",
    "$$\n",
    "\\int_0^1 p_{rs}^{m_{rs}}(1-p_{rs})^{n_{rs}-m_{rs}}\n",
    "\\,\\frac{p_{rs}^{\\alpha-1}(1-p_{rs})^{\\beta-1}}{B(\\alpha,\\beta)}\\,dp_{rs}\n",
    "$$\n",
    "\n",
    "has a closed form:\n",
    "\n",
    "$$\n",
    "\\frac{B(m_{rs}+\\alpha,\\; n_{rs}-m_{rs}+\\beta)}{B(\\alpha,\\beta)}.\n",
    "$$\n",
    "\n",
    "This is the same conjugacy story as the coin flip example — just repeated many times.\n",
    "\n",
    "> `graph-tool` uses a *microcanonical* SBM under the hood (hard constraints on edge counts), which changes some constants and bookkeeping, but the core intuition is the same: integrate out nuisance parameters → get an automatic complexity penalty → choose $B$ by model selection.\n",
    "\n",
    "Let's compute a toy version of this \"integrated likelihood score\" for two different partitions of the *same* graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ea6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy canonical-SBM scoring function:\n",
    "# - compute block-pair counts (m_rs and n_rs)\n",
    "# - compute an integrated likelihood under Beta priors (log scale)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def block_pair_counts_undirected(G, b, B=None):\n",
    "    \"\"\"Return (m_rs, n_rs) for an undirected simple graph under partition b.\n",
    "\n",
    "    - G: NetworkX graph with nodes 0..n-1\n",
    "    - b: list/array of block labels length n (values 0..B-1)\n",
    "    \"\"\"\n",
    "    n = G.number_of_nodes()\n",
    "    if B is None:\n",
    "        B = int(np.max(b)) + 1\n",
    "\n",
    "    # block sizes\n",
    "    N = np.bincount(np.asarray(b), minlength=B)\n",
    "\n",
    "    # n_rs: number of potential edges between block pairs\n",
    "    n_rs = np.zeros((B, B), dtype=int)\n",
    "    for r in range(B):\n",
    "        for s in range(r, B):\n",
    "            if r == s:\n",
    "                n_rs[r, s] = N[r] * (N[r] - 1) // 2\n",
    "            else:\n",
    "                n_rs[r, s] = N[r] * N[s]\n",
    "\n",
    "    # m_rs: observed edges between block pairs\n",
    "    m_rs = np.zeros((B, B), dtype=int)\n",
    "    for u, v in G.edges():\n",
    "        r = int(b[u]); s = int(b[v])\n",
    "        if r > s:\n",
    "            r, s = s, r\n",
    "        m_rs[r, s] += 1\n",
    "\n",
    "    return m_rs, n_rs\n",
    "\n",
    "def log_marginal_canonical_sbm(G, b, alpha=1.0, beta=1.0):\n",
    "    \"\"\"Log P(A | b) under a canonical SBM with independent Beta(alpha,beta) priors on p_rs.\n",
    "    This uses the *exact graph likelihood* (no binomial coefficient), grouped by block pairs.\n",
    "    \"\"\"\n",
    "    m_rs, n_rs = block_pair_counts_undirected(G, b)\n",
    "    B = m_rs.shape[0]\n",
    "\n",
    "    # log marginal = sum_{r<=s} log B(m+alpha, n-m+beta) - log B(alpha,beta)\n",
    "    total = 0.0\n",
    "    for r in range(B):\n",
    "        for s in range(r, B):\n",
    "            m = m_rs[r, s]\n",
    "            n = n_rs[r, s]\n",
    "            if n == 0:\n",
    "                continue\n",
    "            total += special.betaln(m + alpha, (n - m) + beta) - special.betaln(alpha, beta)\n",
    "    return float(total)\n",
    "\n",
    "def log_likelihood_mle_canonical_sbm(G, b, eps=1e-12):\n",
    "    \"\"\"Plug-in log-likelihood using MLE p_hat = m_rs/n_rs for each block pair.\"\"\"\n",
    "    m_rs, n_rs = block_pair_counts_undirected(G, b)\n",
    "    B = m_rs.shape[0]\n",
    "\n",
    "    ll = 0.0\n",
    "    for r in range(B):\n",
    "        for s in range(r, B):\n",
    "            m = m_rs[r, s]\n",
    "            n = n_rs[r, s]\n",
    "            if n == 0:\n",
    "                continue\n",
    "\n",
    "            # MLE p_hat = m/n, handle boundary cases carefully\n",
    "            if m == 0:\n",
    "                p = 0.0\n",
    "            elif m == n:\n",
    "                p = 1.0\n",
    "            else:\n",
    "                p = m / n\n",
    "\n",
    "            # Contribution: m log p + (n-m) log(1-p)\n",
    "            # (with careful handling of 0*log(0))\n",
    "            if 0 < p < 1:\n",
    "                ll += m * np.log(p) + (n - m) * np.log(1 - p)\n",
    "            elif p == 0.0:\n",
    "                ll += 0.0  # since m=0 and log(1)=0\n",
    "            elif p == 1.0:\n",
    "                ll += 0.0  # since n-m=0 and log(1)=0\n",
    "\n",
    "    return float(ll)\n",
    "\n",
    "# -------------------------\n",
    "# Generate a small SBM graph with a known partition\n",
    "# -------------------------\n",
    "n1, n2 = 30, 30\n",
    "sizes = [n1, n2]\n",
    "p_in, p_out = 0.25, 0.02\n",
    "p = [[p_in, p_out],\n",
    "     [p_out, p_in]]\n",
    "\n",
    "G = nx.stochastic_block_model(sizes, p, seed=1)\n",
    "\n",
    "# True partition (2 blocks)\n",
    "b_true = np.array([0]*n1 + [1]*n2)\n",
    "\n",
    "# An over-split partition (4 blocks): split each true block in half\n",
    "b_over = np.array([0]*(n1//2) + [1]*(n1 - n1//2) + [2]*(n2//2) + [3]*(n2 - n2//2))\n",
    "\n",
    "print(\"Graph: n =\", G.number_of_nodes(), \"m =\", G.number_of_edges())\n",
    "\n",
    "# Compare plug-in log-likelihoods (fit only)\n",
    "ll_true = log_likelihood_mle_canonical_sbm(G, b_true)\n",
    "ll_over = log_likelihood_mle_canonical_sbm(G, b_over)\n",
    "\n",
    "# Compare integrated log marginal likelihoods (fit + complexity penalty)\n",
    "lm_true = log_marginal_canonical_sbm(G, b_true, alpha=1, beta=1)\n",
    "lm_over = log_marginal_canonical_sbm(G, b_over, alpha=1, beta=1)\n",
    "\n",
    "print(\"\\nPlug-in log-likelihood (MLE p_rs):\")\n",
    "print(\"  2 blocks:\", round(ll_true, 2))\n",
    "print(\"  4 blocks:\", round(ll_over, 2), \"(should be >= 2-block, since it's more flexible)\")\n",
    "\n",
    "print(\"\\nIntegrated log marginal likelihood (Beta priors on p_rs):\")\n",
    "print(\"  2 blocks:\", round(lm_true, 2))\n",
    "print(\"  4 blocks:\", round(lm_over, 2), \"(may go up or down; this is the Occam effect)\")\n",
    "\n",
    "print(\"\\nDifference (4 - 2):\")\n",
    "print(\"  Δ log-likelihood:\", round(ll_over - ll_true, 2))\n",
    "print(\"  Δ log-marginal  :\", round(lm_over - lm_true, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ce25c",
   "metadata": {},
   "source": [
    "### Checks for understanding (after §12)\n",
    "\n",
    "1. In Bayes' rule $P(\\theta\\mid D) \\propto P(D\\mid \\theta) P(\\theta)$, which term is the likelihood and which is the prior? What does the proportionality hide?\n",
    "2. Why does the evidence $P(D)$ tend to penalize overly flexible models, even when they can fit the data well?\n",
    "3. In the Beta-Bernoulli example, interpret $(\\alpha,\\beta)$ as pseudo-counts. What changes when you make $(\\alpha,\\beta)$ very large?\n",
    "4. \"Likelihood is not a probability distribution.\" What does that sentence mean, concretely, in the coin-flip micro-example?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bd403a",
   "metadata": {},
   "source": [
    "## 13. SBM inference in `graph-tool`: `minimize_blockmodel_dl` and nested SBMs\n",
    "\n",
    "Now we do the thing that modularity *wanted* to do, but in a statistically grounded way.\n",
    "\n",
    "`graph-tool` treats community detection as **latent-variable inference**.\n",
    "\n",
    "- latent variable: partition $b$\n",
    "- observed data: graph $G$ (adjacency $A$)\n",
    "- model class: SBM (with options like degree correction, overlap, layers, hierarchy)\n",
    "\n",
    "---\n",
    "\n",
    "### 13.1 The \"one-liner\" (don't worry, we'll unpack it)\n",
    "\n",
    "```python\n",
    "state = gt.minimize_blockmodel_dl(g)\n",
    "```\n",
    "\n",
    "This returns a `BlockState` object that contains:\n",
    "- a partition of vertices into blocks (`state.get_blocks()`)\n",
    "- inferred block structure (e.g., a block matrix via `state.get_matrix()`)\n",
    "- a description length / entropy score (`state.entropy()`)\n",
    "\n",
    "---\n",
    "\n",
    "### 13.2 What is the objective function?\n",
    "\n",
    "Conceptually, the Bayesian goal is to maximize:\n",
    "\n",
    "$$P(b \\mid G, M) \\propto P(G \\mid b, M)P(b\\mid M).$$\n",
    "\n",
    "In the MDL view, we minimize a description length:\n",
    "\n",
    "$$\\Sigma(b) \\approx -\\ln P(G, b \\mid M)\n",
    "= -\\ln P(G\\mid b,M) - \\ln P(b\\mid M).$$\n",
    "\n",
    "So:\n",
    "- **lower $\\Sigma$** = better (higher posterior probability)\n",
    "- the method automatically balances \"fit\" vs \"complexity\"\n",
    "- the number of blocks $B$ is *selected* rather than fixed\n",
    "\n",
    "---\n",
    "\n",
    "### 13.3 What does the result mean?\n",
    "\n",
    "- The partition is **not** \"the truth\" — it's a model-based explanation.\n",
    "- The inferred $B$ is the model's *preferred complexity* given the priors.\n",
    "- Many partitions may have similar posterior probability → uncertainty is real.\n",
    "\n",
    "---\n",
    "\n",
    "### 13.4 Degree-corrected vs non-degree-corrected\n",
    "\n",
    "A practical workflow is:\n",
    "\n",
    "1. fit with `deg_corr=True`\n",
    "2. fit with `deg_corr=False`\n",
    "3. compare description lengths\n",
    "\n",
    "Lower description length (entropy) usually indicates the better explanation.\n",
    "\n",
    "> In many empirical networks, degree correction is crucial: without it, \"communities\" can become proxies for \"high-degree vs low-degree\".\n",
    "\n",
    "---\n",
    "\n",
    "### 13.5 Hands-on: recover planted communities in a synthetic SBM\n",
    "\n",
    "We start with a synthetic graph where the \"ground truth\" is known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb33e0c",
   "metadata": {},
   "source": [
    "### 13.2.1 Interpreting description length differences as evidence (a practical trick)\n",
    "\n",
    "A `graph-tool` SBM fit gives you a description length (in **nats**) via:\n",
    "\n",
    "```python\n",
    "Sigma = state.entropy()\n",
    "```\n",
    "\n",
    "The key exponential relationship is:\n",
    "\n",
    "$$\n",
    "P(b \\mid G) \\propto \\exp(-\\Sigma(b)).\n",
    "$$\n",
    "\n",
    "So if you have **two** candidate partitions $b_1$ and $b_2$ (or two model variants), their posterior odds satisfy:\n",
    "\n",
    "$$\n",
    "\\frac{P(b_1\\mid G)}{P(b_2\\mid G)}\n",
    "\\approx \\exp\\bigl(-[\\Sigma(b_1)-\\Sigma(b_2)]\\bigr).\n",
    "$$\n",
    "\n",
    "That means:\n",
    "\n",
    "- If $\\Delta\\Sigma = \\Sigma_1-\\Sigma_2 = 10$, then $b_1$ is downweighted by a factor of $e^{-10} \\approx 0.000045$ relative to $b_2$.\n",
    "- If you want **bits** instead of nats: $\\Delta\\Sigma_{\\text{bits}} = \\Delta\\Sigma/\\ln 2$.\n",
    "\n",
    "This is the same logic as Bayes factors in Section 12 — just expressed in the MDL coding language.\n",
    "\n",
    "We'll use this repeatedly:\n",
    "\n",
    "- comparing `deg_corr=True` vs `deg_corr=False`\n",
    "- comparing nested vs non-nested SBMs\n",
    "- comparing \"model explains the data\" vs \"model is overfitting\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b7f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_odds_from_delta_sigma(delta_sigma):\n",
    "    \"\"\"Return odds = P(model1|data) / P(model2|data) given ΔΣ = Σ1 - Σ2 (nats).\"\"\"\n",
    "    return math.exp(-delta_sigma)\n",
    "\n",
    "# Example: interpret some common ΔΣ values\n",
    "for dS in [0, 1, 2, 5, 10]:\n",
    "    odds = posterior_odds_from_delta_sigma(dS)\n",
    "    print(f\"ΔΣ={dS:>2} nats -> odds ≈ e^(-ΔΣ) = {odds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72826923",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "# Synthetic planted partition\n",
    "n_per_block = 60\n",
    "B = 3\n",
    "b_true = np.repeat(np.arange(B), n_per_block)\n",
    "\n",
    "# These are PER-PAIR probabilities (the intuitive thing)\n",
    "p_in_pair, p_out_pair = 0.12, 0.01\n",
    "\n",
    "# Convert to graph-tool's \"probs\" = expected edge counts between blocks\n",
    "n = n_per_block\n",
    "p_in  = p_in_pair  * n * (n - 1)   # = 2 * E[m_rr] for undirected\n",
    "p_out = p_out_pair * n * n         # = E[m_rs] for r != s\n",
    "\n",
    "probs = np.array([[p_in,  p_out, p_out],\n",
    "                  [p_out, p_in,  p_out],\n",
    "                  [p_out, p_out, p_in]], dtype=float)\n",
    "\n",
    "g = gt.generate_sbm(b_true, probs, directed=False)\n",
    "g.vp[\"b_true\"] = g.new_vp(\"int\", vals=b_true)\n",
    "\n",
    "\n",
    "print(\"Generated SBM: V =\", g.num_vertices(), \"E =\", g.num_edges())\n",
    "\n",
    "# Inference: non-degree-corrected SBM (fine for this toy)\n",
    "state = gt.minimize_blockmodel_dl(g, state_args=dict(deg_corr=False))\n",
    "\n",
    "b_hat = state.get_blocks()\n",
    "B_hat = state.get_nonempty_B()\n",
    "dl = state.entropy()\n",
    "\n",
    "print(\"Inferred B:\", B_hat)\n",
    "print(\"Description length (entropy):\", round(dl, 2))\n",
    "\n",
    "pos = gt.sfdp_layout(g)\n",
    "state.draw(pos=pos, output_size=(700, 650));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0c4237",
   "metadata": {},
   "source": [
    "### 13.5 Comparing degree-corrected vs not\n",
    "\n",
    "Even on the *same* graph, different SBM variants can yield different explanations.\n",
    "\n",
    "Let's compare:\n",
    "- non-degree-corrected SBM\n",
    "- degree-corrected SBM\n",
    "\n",
    "and look at description lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec359cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_plain = gt.minimize_blockmodel_dl(g, state_args=dict(deg_corr=False))\n",
    "state_dc    = gt.minimize_blockmodel_dl(g, state_args=dict(deg_corr=True))\n",
    "\n",
    "print(\"Plain SBM:\")\n",
    "print(\"  B =\", state_plain.get_nonempty_B(), \"entropy =\", round(state_plain.entropy(), 2))\n",
    "print(\"Degree-corrected SBM:\")\n",
    "print(\"  B =\", state_dc.get_nonempty_B(), \"entropy =\", round(state_dc.entropy(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117658d4",
   "metadata": {},
   "source": [
    "### 13.6 Turning description length differences into evidence ratios\n",
    "\n",
    "A powerful trick: compare two modeling choices (e.g., degree-corrected vs not) via:\n",
    "\n",
    "$$\\Delta \\Sigma = \\Sigma_1 - \\Sigma_0.$$\n",
    "\n",
    "If $\\Sigma$ is a negative log probability (in nats), then:\n",
    "\n",
    "$$\\frac{P(G\\mid M_1)}{P(G\\mid M_0)} \\approx \\exp(-\\Delta\\Sigma).$$\n",
    "\n",
    "So:\n",
    "\n",
    "- if $\\Delta\\Sigma < 0$, model 1 is favored (higher evidence)\n",
    "- if $\\Delta\\Sigma > 0$, model 0 is favored\n",
    "\n",
    "You can also translate $\\Delta\\Sigma$ into bits:\n",
    "\n",
    "$$\\Delta\\Sigma_{\\mathrm{bits}} = \\frac{\\Delta\\Sigma}{\\ln 2}.$$\n",
    "\n",
    "A difference of just **10 bits** corresponds to an evidence ratio of about $2^{10} \\approx 1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53704a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume state_plain and state_dc exist from the previous cell.\n",
    "S_plain = state_plain.entropy()\n",
    "S_dc    = state_dc.entropy()\n",
    "\n",
    "# Evidence ratio P(G|DC) / P(G|plain) ≈ exp(-(S_dc - S_plain))\n",
    "dS = S_dc - S_plain\n",
    "bf = float(np.exp(-dS))\n",
    "dS_bits = dS / np.log(2)\n",
    "\n",
    "print(\"Σ_plain =\", round(S_plain, 3))\n",
    "print(\"Σ_dc    =\", round(S_dc, 3))\n",
    "print(\"ΔΣ = Σ_dc - Σ_plain =\", round(dS, 3), \"nats =\", round(dS_bits, 3), \"bits\")\n",
    "print(\"Approx evidence ratio  P(G|DC) / P(G|plain) ≈ exp(-ΔΣ) =\", round(bf, 3))\n",
    "\n",
    "# Inspect block matrices\n",
    "best = state_dc if S_dc < S_plain else state_plain\n",
    "mat = best.get_matrix()  # sparse matrix of edge counts between blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd06046-9014-4639-9911-a3c7ce800bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4),dpi=100)\n",
    "plt.imshow(mat.todense(), aspect=\"auto\")\n",
    "plt.colorbar(label=\"edges between blocks\")\n",
    "plt.title(\"Block matrix (edge counts) of best model\")\n",
    "plt.xlabel(\"block s\")\n",
    "plt.ylabel(\"block r\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c61b69",
   "metadata": {},
   "source": [
    "### 13.7 Nested SBMs: multi-scale community structure\n",
    "\n",
    "For many real networks, a flat partition is not enough.\n",
    "\n",
    "`graph-tool` can infer nested/hierarchical structure with:\n",
    "```python\n",
    "state = gt.minimize_nested_blockmodel_dl(g)\n",
    "```\n",
    "\n",
    "This returns a `NestedBlockState`.\n",
    "You can:\n",
    "- print a summary\n",
    "- draw the hierarchy\n",
    "- access block memberships at each level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04053a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A slightly larger synthetic example with 6 groups (2 super-groups)\n",
    "n = 300\n",
    "B = 6\n",
    "b_true = np.repeat(np.arange(B), n//B)\n",
    "\n",
    "# Make two super-communities: {0,1,2} and {3,4,5}\n",
    "p_in  = 0.10\n",
    "p_mid = 0.03\n",
    "p_out = 0.005\n",
    "\n",
    "# Build the PER-PAIR probability matrix p_rs\n",
    "p = np.full((B, B), p_out, dtype=float)\n",
    "np.fill_diagonal(p, p_in)\n",
    "\n",
    "# within each super-group, between different blocks use p_mid\n",
    "for r in range(3):\n",
    "    for s in range(3):\n",
    "        if r != s:\n",
    "            p[r, s] = p_mid\n",
    "for r in range(3, 6):\n",
    "    for s in range(3, 6):\n",
    "        if r != s:\n",
    "            p[r, s] = p_mid\n",
    "\n",
    "# Convert per-pair probabilities p_rs -> graph-tool propensities lambda_rs\n",
    "sizes = np.array([(b_true == r).sum() for r in range(B)], dtype=int)\n",
    "lam = np.zeros((B, B), dtype=float)\n",
    "\n",
    "for r in range(B):\n",
    "    for s in range(B):\n",
    "        if r == s:\n",
    "            # undirected convention: generate_sbm expects lam_rr = 2 * E[m_rr]\n",
    "            # E[m_rr] = p_rr * C(n_r, 2)\n",
    "            lam[r, r] = p[r, r] * sizes[r] * (sizes[r] - 1)\n",
    "        else:\n",
    "            # E[m_rs] = p_rs * n_r * n_s\n",
    "            lam[r, s] = p[r, s] * sizes[r] * sizes[s]\n",
    "\n",
    "g = gt.generate_sbm(b_true, lam, directed=False)\n",
    "g.vp[\"b_true\"] = g.new_vp(\"int\", vals=b_true)\n",
    "\n",
    "print(\"Generated SBM: V =\", g.num_vertices(), \"E =\", g.num_edges())\n",
    "\n",
    "# Nested inference (degree correction often helps in real graphs)\n",
    "nstate = gt.minimize_nested_blockmodel_dl(g, state_args=dict(deg_corr=True))\n",
    "\n",
    "nstate.print_summary()\n",
    "print(\"Total hierarchy description length:\", round(nstate.entropy(), 2))\n",
    "\n",
    "# Draw hierarchy\n",
    "nstate.draw(output_size=(900, 600))\n",
    "\n",
    "# Get block memberships at each level\n",
    "bs = nstate.get_bs()\n",
    "print(\"Number of levels:\", len(bs))\n",
    "print(\"B at level 0:\", len(np.unique(bs[0])))\n",
    "print(\"B at top level:\", len(np.unique(bs[-1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60696f8f",
   "metadata": {},
   "source": [
    "### Checks for understanding (after §13)\n",
    "\n",
    "1. What does `deg_corr=True` change in an SBM, and why does it matter for heavy-tailed degree distributions?\n",
    "2. What is the difference between `minimize_blockmodel_dl(...)` and `minimize_nested_blockmodel_dl(...)` in terms of what the model is allowed to represent?\n",
    "3. In `graph-tool`, why is the quantity $\\Sigma$ (description length) a natural model selection score?\n",
    "4. If the best-fitting model has $B=1$ block (no community structure), how should you interpret that result?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1282bcd2",
   "metadata": {},
   "source": [
    "## 14. Uncertainty & MCMC: sampling partitions and getting marginals\n",
    "\n",
    "A single inferred partition (an MDL optimum) can be misleading:\n",
    "\n",
    "- many partitions may fit almost equally well\n",
    "- block labels are arbitrary (\"label switching\")\n",
    "- you often want *marginal* probabilities, not a single hard assignment\n",
    "\n",
    "`graph-tool` gives you tools to:\n",
    "\n",
    "1. **sample** partitions from the posterior (MCMC)\n",
    "2. **summarize** the posterior:\n",
    "   - vertex membership marginals (\"soft\" assignments)\n",
    "   - consensus partitions (\"hard\" summaries)\n",
    "   - uncertainty visualizations (pie charts, entropy, co-assignment matrices)\n",
    "\n",
    "---\n",
    "\n",
    "### 14.1 The posterior over partitions (conceptual)\n",
    "\n",
    "Think of the SBM objective as a (negative log) posterior:\n",
    "\n",
    "$$P(b \\mid G) \\propto \\exp\\bigl(-\\Sigma(b)\\bigr),$$\n",
    "\n",
    "where $\\Sigma(b)$ is the description length / entropy reported by `state.entropy()`.\n",
    "\n",
    "---\n",
    "\n",
    "### 14.2 MCMC and the Metropolis idea (high level)\n",
    "\n",
    "MCMC constructs a Markov chain whose stationary distribution is the posterior.\n",
    "\n",
    "A simplified Metropolis acceptance rule is:\n",
    "\n",
    "$$\\text{accept move } b\\to b' \\text{ with prob } \n",
    "\\alpha = \\min\\left(1,\\; \\exp(-\\beta[\\Sigma(b')-\\Sigma(b)])\\right),$$\n",
    "\n",
    "where:\n",
    "- $\\Delta\\Sigma = \\Sigma(b')-\\Sigma(b)$ is the description-length change\n",
    "- $\\beta$ is an inverse temperature:\n",
    "  - $\\beta=1$: sample from the posterior\n",
    "  - $\\beta\\to\\infty$: greedy downhill search (optimization)\n",
    "\n",
    "(Real implementations include proposal probabilities and extra move types, but this captures the core idea.)\n",
    "\n",
    "---\n",
    "\n",
    "### 14.3 Label switching (why \"block 0\" is not meaningful)\n",
    "\n",
    "If you permute labels (rename blocks), the partition is the same grouping structure.\n",
    "This means raw MCMC samples can \"jump\" in label space even when they haven't changed in meaning.\n",
    "\n",
    "`PartitionModeState` solves this by aligning labels across samples so we can compute meaningful marginals.\n",
    "\n",
    "---\n",
    "\n",
    "### 14.4 Hands-on: posterior marginals as \"pie charts\" (Karate Club)\n",
    "\n",
    "We will:\n",
    "\n",
    "1. fit a degree-corrected SBM\n",
    "2. run MCMC to sample partitions\n",
    "3. compute vertex marginals\n",
    "4. visualize uncertainty as pie charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a25e7f6",
   "metadata": {},
   "source": [
    "### 14.2.1 Why Metropolis works (the one equation: detailed balance)\n",
    "\n",
    "We want our Markov chain to have stationary distribution:\n",
    "\n",
    "$$\n",
    "\\pi(b) \\propto \\exp(-\\beta\\,\\Sigma(b)).\n",
    "$$\n",
    "\n",
    "A common sufficient condition is **detailed balance**:\n",
    "\n",
    "$$\n",
    "\\pi(b)\\,q(b\\to b')\\,\\alpha(b\\to b')\n",
    "=\n",
    "\\pi(b')\\,q(b'\\to b)\\,\\alpha(b'\\to b),\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $q(b\\to b')$ is the proposal probability\n",
    "- $\\alpha(b\\to b')$ is the acceptance probability\n",
    "\n",
    "If the proposal is symmetric ($q(b\\to b')=q(b'\\to b)$), detailed balance holds if we choose:\n",
    "\n",
    "$$\n",
    "\\alpha(b\\to b') = \\min\\left(1, \\frac{\\pi(b')}{\\pi(b)}\\right)\n",
    "= \\min\\left(1, \\exp\\bigl(-\\beta[\\Sigma(b')-\\Sigma(b)]\\bigr)\\right).\n",
    "$$\n",
    "\n",
    "That's the Metropolis rule.\n",
    "\n",
    "---\n",
    "\n",
    "### 14.2.2 Practical diagnostics: burn-in and autocorrelation\n",
    "\n",
    "MCMC is powerful, but you should build two habits:\n",
    "\n",
    "1) **Look at a trace plot** of $\\Sigma$ (or some statistic).  \n",
    "   - early iterations can reflect initialization (\"burn-in\")\n",
    "   - later iterations should look like noisy fluctuations around a stable region\n",
    "\n",
    "2) **Expect autocorrelation.**  \n",
    "   Successive samples are not independent. If you record every iteration, you'll get many nearly-duplicate samples.\n",
    "\n",
    "A simple diagnostic is the lag-$\\ell$ autocorrelation:\n",
    "\n",
    "$$\n",
    "\\rho(\\ell) = \\frac{\\mathrm{Cov}(\\Sigma_t,\\Sigma_{t+\\ell})}{\\mathrm{Var}(\\Sigma_t)}.\n",
    "$$\n",
    "\n",
    "When $\\rho(\\ell)$ decays slowly, you need either:\n",
    "- more sweeps, or\n",
    "- thinner samples (record every $k$ sweeps), or\n",
    "- better proposals (graph-tool handles many of these details for you, but you still need to *think* statistically).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deee641",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx_to_gt(nx.karate_club_graph())\n",
    "\n",
    "# Fit a degree-corrected SBM\n",
    "state = gt.minimize_blockmodel_dl(g, state_args=dict(deg_corr=True))\n",
    "print(\"Initial entropy:\", round(state.entropy(), 2), \"B =\", state.get_nonempty_B())\n",
    "\n",
    "# We'll record both partitions and description lengths along the chain\n",
    "partitions = []\n",
    "entropies = []\n",
    "\n",
    "def record_state(s):\n",
    "    partitions.append(s.get_blocks().a.copy())\n",
    "    entropies.append(s.entropy())\n",
    "\n",
    "# Equilibrate + collect\n",
    "gt.mcmc_equilibrate(\n",
    "    state,\n",
    "    wait=20,               # stop when no new record DL found for this many sweeps (heuristic)\n",
    "    force_niter=50,         # minimum sweeps before we consider stopping\n",
    "    mcmc_args=dict(niter=5),# how many sweeps per callback\n",
    "    callback=record_state,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "entropies = np.asarray(entropies)\n",
    "print(\"Collected samples:\", len(partitions))\n",
    "print(\"Entropy range:\", float(entropies.min()), \"to\", float(entropies.max()))\n",
    "\n",
    "# Trace plot of Σ (description length)\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(entropies, marker=\"o\", linewidth=1)\n",
    "plt.xlabel(\"Recorded sample index\")\n",
    "plt.ylabel(\"Σ (nats)\")\n",
    "plt.title(\"MCMC trace of description length\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# A tiny autocorrelation helper (for intuition, not for publication-grade ESS estimates)\n",
    "def autocorr(x, lag):\n",
    "    x = np.asarray(x)\n",
    "    x0 = x[:-lag]\n",
    "    x1 = x[lag:]\n",
    "    return np.corrcoef(x0, x1)[0, 1]\n",
    "\n",
    "for lag in [1, 2, 5, 10]:\n",
    "    if len(entropies) > lag + 2:\n",
    "        print(f\"autocorr(lag={lag:>2}) ≈ {autocorr(entropies, lag):.3f}\")\n",
    "\n",
    "# Align labels + compute vertex marginals\n",
    "pmode = gt.PartitionModeState(partitions, relabel=True, converge=True)\n",
    "pv = pmode.get_marginal(g)  # VertexPropertyMap with vector<int> counts\n",
    "\n",
    "# Draw: node pie charts show uncertainty (multi-membership probability mass)\n",
    "pos = gt.sfdp_layout(g)\n",
    "state.draw(\n",
    "    pos=pos,\n",
    "    vertex_shape=\"pie\",\n",
    "    vertex_pie_fractions=pv,\n",
    "    output_size=(750, 700)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b75173",
   "metadata": {},
   "source": [
    "### 14.5 Quantifying uncertainty: consensus partition, entropy, and co-assignment\n",
    "\n",
    "Visualizations are great, but you also want **numbers**.\n",
    "\n",
    "Three common summaries:\n",
    "\n",
    "1. **Consensus (\"max marginal\") partition**  \n",
    "   Assign each node to its most probable block:\n",
    "\n",
    "   $$\\hat b_i = \\arg\\max_r \\; P(b_i=r \\mid G).$$\n",
    "\n",
    "2. **Mean-field entropy of node marginals**  \n",
    "   If $p_{ir} = P(b_i=r\\mid G)$, then one natural uncertainty measure is:\n",
    "\n",
    "   $$H_{\\mathrm{MF}} = -\\sum_i \\sum_r p_{ir}\\log p_{ir}.$$\n",
    "\n",
    "   This is near 0 when assignments are nearly deterministic, and larger when nodes are ambiguous.\n",
    "\n",
    "3. **Co-assignment matrix**  \n",
    "   Define:\n",
    "\n",
    "   $$C_{ij} = P(b_i=b_j\\mid G).$$\n",
    "\n",
    "   This is a very interpretable object: it tells you which pairs of nodes are \"almost always\" together vs \"often split\".\n",
    "\n",
    "We'll compute all three below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a7c8e2-60be-4627-9d92-27a7b61e92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MCMC sampling settings ---\n",
    "burnin = 2000          # sweeps to forget initialization\n",
    "thin = 20              # keep one sample every 'thin' sweeps\n",
    "n_samples = 200        # number of saved samples\n",
    "\n",
    "# --- Run burn-in ---\n",
    "for _ in range(burnin):\n",
    "    state.mcmc_sweep(beta=1.0)\n",
    "\n",
    "# --- Collect samples of block assignments ---\n",
    "samples = []\n",
    "Bs = []\n",
    "\n",
    "for t in range(n_samples):\n",
    "    for _ in range(thin):\n",
    "        state.mcmc_sweep(beta=1.0)\n",
    "\n",
    "    b = state.get_blocks().a.copy()   # length N\n",
    "    samples.append(b)\n",
    "    Bs.append(len(np.unique(b)))\n",
    "\n",
    "samples = np.asarray(samples)  # shape (n_samples, N)\n",
    "Bs = np.asarray(Bs)\n",
    "\n",
    "print(\"Sampled B (min/median/max):\", Bs.min(), int(np.median(Bs)), Bs.max())\n",
    "print(\"Fraction with B=1:\", np.mean(Bs == 1))\n",
    "\n",
    "\n",
    "# Sanity checks (helps if you run cells out of order)\n",
    "N = samples.shape[1]\n",
    "print(\"Sample matrix shape:\", samples.shape, \"| g.num_vertices() =\", g.num_vertices())\n",
    "assert N == g.num_vertices(), \"Mismatch: samples were generated for a different graph than 'g'. Re-run the sampling cell.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf327db-5239-49a8-8d1f-35f3a3bab290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-assignment (co-clustering) matrix from raw MCMC samples.\n",
    "#\n",
    "# Definition:\n",
    "#   C[i,j] = fraction of posterior samples where i and j are assigned to the same block.\n",
    "#\n",
    "# This object is *label-invariant*: it does not care what the numeric block labels are,\n",
    "# only whether two nodes match within each sample.\n",
    "\n",
    "samples_arr = np.asarray(samples)\n",
    "N = samples_arr.shape[1]\n",
    "\n",
    "print(\"Using samples array with shape:\", samples_arr.shape)\n",
    "assert N == g.num_vertices(), \"Mismatch: samples were generated for a different graph than 'g'. Re-run the sampling cell.\"\n",
    "\n",
    "C = np.zeros((N, N), dtype=float)\n",
    "for b in samples_arr:\n",
    "    C += (b[:, None] == b[None, :]).astype(float)\n",
    "\n",
    "C /= len(samples_arr)\n",
    "print(\"Co-assignment matrix computed from\", len(samples_arr), \"samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad808cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Consensus partition (max marginal) via PartitionModeState\n",
    "#\n",
    "# pmode is built from the earlier collection of partitions (from gt.mcmc_equilibrate).\n",
    "# This gives a *label-aligned* view of the posterior and is convenient for marginals.\n",
    "\n",
    "b_max = pmode.get_max(g)  # int-valued VertexPropertyMap\n",
    "b_max_arr = np.asarray(b_max.a)\n",
    "print(\"Consensus B (nonempty):\", len(np.unique(b_max_arr)))\n",
    "\n",
    "# 2) Mean-field entropy of node marginals (graph-tool convenience)\n",
    "#\n",
    "# Intuition:\n",
    "# - If each node's marginal is concentrated on one block, entropy is low (confident assignments).\n",
    "# - If many nodes have spread-out marginals, entropy is higher (uncertainty / boundary nodes).\n",
    "\n",
    "H_mf = gt.mf_entropy(g, pv)\n",
    "print(\"Mean-field entropy of node marginals:\", round(H_mf, 3), \"(nats)\")\n",
    "\n",
    "# 3) Visualize the co-assignment matrix (computed from raw samples)\n",
    "#\n",
    "# We sort nodes by the consensus labels so the block structure is easier to see.\n",
    "\n",
    "# If C isn't in memory (e.g., you ran cells out of order), recompute it from `samples`.\n",
    "try:\n",
    "    C\n",
    "except NameError:\n",
    "    samples_arr = np.asarray(samples)\n",
    "    N = samples_arr.shape[1]\n",
    "    assert N == g.num_vertices(), \"Mismatch: samples were generated for a different graph than 'g'. Re-run the sampling cell.\"\n",
    "    C = np.zeros((N, N), dtype=float)\n",
    "    for b in samples_arr:\n",
    "        C += (b[:, None] == b[None, :]).astype(float)\n",
    "    C /= len(samples_arr)\n",
    "\n",
    "order = np.argsort(b_max_arr)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5), dpi=100)\n",
    "im = ax.imshow(C[order][:, order], aspect=\"auto\")\n",
    "fig.colorbar(im, ax=ax, label=\"P(same block)\")\n",
    "ax.set_title(\"Co-assignment matrix (sorted by consensus blocks)\")\n",
    "ax.set_xlabel(\"nodes (sorted)\")\n",
    "ax.set_ylabel(\"nodes (sorted)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Draw consensus partition (hard colors, no pie charts)\n",
    "pos = gt.sfdp_layout(g)\n",
    "state.draw(pos=pos, vertex_fill_color=b_max, output_size=(750, 700))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd81b16",
   "metadata": {},
   "source": [
    "### 14.6 Reading the pie charts\n",
    "\n",
    "- A node with a **single dominant color** is confidently assigned to one block.\n",
    "- A node with a **mixed pie** is uncertain (it plausibly belongs to multiple groups).\n",
    "- Boundary / bridge nodes often show higher uncertainty.\n",
    "\n",
    "> **Checkpoint:** Identify the \"bridge\" nodes in Karate Club and see if their pies look mixed.\n",
    "\n",
    "---\n",
    "\n",
    "### 14.7 Alternative: collecting marginals directly from `state`\n",
    "\n",
    "`state.collect_vertex_marginals()` accumulates membership counts, but you must handle label switching carefully.\n",
    "\n",
    "`PartitionModeState` is usually the easiest way for beginners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318818a6",
   "metadata": {},
   "source": [
    "## 15. Social science case study: PolBlogs polarization + posterior predictive checks\n",
    "\n",
    "This is a classic dataset in computational social science:\n",
    "- blogs as nodes\n",
    "- hyperlinks as edges\n",
    "- known political orientation (often: liberal vs conservative)\n",
    "- strong polarization (assortative mixing by ideology)\n",
    "\n",
    "We'll use it to practice:\n",
    "1. cleaning and inspecting a real graph-tool dataset\n",
    "2. choosing a model (degree-corrected vs not) using description length\n",
    "3. interpreting blocks as social structure\n",
    "4. checking whether the model captures key features (posterior predictive checks)\n",
    "\n",
    "> If you cannot load `polblogs` (e.g., offline environment), skip to the \"optional\" synthetic examples below.\n",
    "\n",
    "---\n",
    "\n",
    "### 15.1 Load and prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4018cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try built-in collection first (often offline-friendly), then netzschleuder.\n",
    "g0 = gt.collection.data[\"polblogs\"]\n",
    "source = \"gt.collection.data['polblogs']\"\n",
    "# except Exception:\n",
    "#     g0 = gt.collection.ns[\"polblogs\"]\n",
    "#     source = \"gt.collection.ns['polblogs']\"\n",
    "\n",
    "# Work on a *copy* we can safely edit (remove edges, prune components, etc.)\n",
    "g = gt.Graph(g0)\n",
    "g.set_directed(False)\n",
    "\n",
    "# Clean up multigraph artifacts (common when data are merged from multiple sources)\n",
    "gt.remove_parallel_edges(g)\n",
    "gt.remove_self_loops(g)\n",
    "\n",
    "# Largest connected component (return a pruned Graph, not just a view)\n",
    "g = gt.extract_largest_component(g, directed=False, prune=True)\n",
    "\n",
    "print(\"Loaded:\", source)\n",
    "print(\"V =\", g.num_vertices(), \"E =\", g.num_edges())\n",
    "print(\"Vertex properties:\", list(g.vp.keys()))\n",
    "print(\"Edge properties:\", list(g.ep.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7189a6",
   "metadata": {},
   "source": [
    "### 15.2 Fit nested SBMs (degree-corrected vs not) and compare description lengths\n",
    "\n",
    "We'll fit two models:\n",
    "- **non-degree-corrected** nested SBM\n",
    "- **degree-corrected** nested SBM\n",
    "\n",
    "and choose the one with smaller description length.\n",
    "\n",
    "> This is model selection: \"which explanation compresses the network best?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models (can take some time depending on machine)\n",
    "state_ndc = gt.minimize_nested_blockmodel_dl(g, state_args=dict(deg_corr=False))\n",
    "state_dc  = gt.minimize_nested_blockmodel_dl(g, state_args=dict(deg_corr=True))\n",
    "\n",
    "dl_ndc = state_ndc.entropy()\n",
    "dl_dc  = state_dc.entropy()\n",
    "\n",
    "print(\"Nested SBM (non-degree-corrected) DL:\", round(dl_ndc, 2))\n",
    "print(\"Nested SBM (degree-corrected)     DL:\", round(dl_dc,  2))\n",
    "\n",
    "best = state_dc if dl_dc < dl_ndc else state_ndc\n",
    "print(\"\\nSelected model:\", \"degree-corrected\" if best is state_dc else \"non-degree-corrected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6dc4ec",
   "metadata": {},
   "source": [
    "### 15.3 Visualize the inferred macro-structure\n",
    "\n",
    "Nested SBMs contain multiple levels.  \n",
    "For social science interpretation, it's often useful to look at a **coarse** level (few groups).\n",
    "\n",
    "We'll:\n",
    "1. inspect the number of groups at each level\n",
    "2. select the *coarsest* level with, say, 2-10 groups\n",
    "3. project that partition back onto the original graph\n",
    "4. draw it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc3d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best model from previous cell\n",
    "try:\n",
    "    best\n",
    "except NameError:\n",
    "    best = gt.minimize_nested_blockmodel_dl(g, state_args=dict(deg_corr=True))\n",
    "\n",
    "# Determine number of groups at each level\n",
    "bs = best.get_bs()\n",
    "Bs = [len(np.unique(b)) for b in bs]\n",
    "print(\"B by level (0 = finest):\", Bs)\n",
    "\n",
    "# Pick a coarse level with 2..10 groups (fallback: level 0)\n",
    "candidate_levels = [i for i, B in enumerate(Bs) if 2 <= B <= 10]\n",
    "level = candidate_levels[-1] if candidate_levels else 0\n",
    "print(\"Chosen level:\", level, \"with B =\", Bs[level])\n",
    "\n",
    "# Project that level to a flat BlockState on the original graph\n",
    "state_coarse = best.project_level(level)\n",
    "\n",
    "# Choose positions: dataset often provides g.vp['pos']\n",
    "pos = g.vp[\"pos\"] if \"pos\" in g.vp else gt.sfdp_layout(g)\n",
    "\n",
    "state_coarse.draw(pos=pos, output_size=(900, 750))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8296fa6",
   "metadata": {},
   "source": [
    "### 15.4 Quantify polarization: assortativity by ideology and by inferred blocks\n",
    "\n",
    "**Assortativity** is a normalized \"like connects to like\" score.\n",
    "\n",
    "For a categorical label $x \\in \\{1,\\dots,K\\}$, define a mixing matrix:\n",
    "\n",
    "- $e_{ij}$: fraction of edges from type $i$ to type $j$\n",
    "- $a_i = \\sum_j e_{ij}$, $b_j = \\sum_i e_{ij}$\n",
    "\n",
    "Then the assortativity coefficient (Newman) is:\n",
    "\n",
    "$$r = \\frac{\\sum_i e_{ii} - \\sum_i a_i b_i}{1-\\sum_i a_i b_i}.$$\n",
    "\n",
    "Interpretation:\n",
    "- $r \\approx 1$: almost all edges are within the same type (strong homophily / polarization)\n",
    "- $r \\approx 0$: connections are close to random mixing (given type frequencies)\n",
    "- $r < 0$: disassortative mixing (edges tend to go across types)\n",
    "\n",
    "In the PolBlogs dataset, we can compute assortativity for:\n",
    "- **inferred blocks** (model-implied polarization / segmentation)\n",
    "- **known ideology label** (if provided as a vertex property)\n",
    "\n",
    "We'll use `gt.assortativity()`, which returns $r$ and an estimated variance (via jackknife)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec027d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred block labels\n",
    "b = state_coarse.get_blocks()\n",
    "\n",
    "r_blocks, var_blocks = gt.assortativity(g, b)\n",
    "print(\"Assortativity by inferred blocks:\", round(r_blocks, 3), \"+/-\", round(np.sqrt(var_blocks), 3))\n",
    "\n",
    "# Try to detect a known ideology property map automatically\n",
    "label_candidates = [\"value\", \"label\", \"party\", \"ideology\", \"pol\", \"color\"]\n",
    "known_key = next((k for k in label_candidates if k in g.vp), None)\n",
    "\n",
    "if known_key is None:\n",
    "    print(\"No obvious ideology label found in g.vp. (That's okay.)\")\n",
    "else:\n",
    "    y = g.vp[known_key]\n",
    "    r_y, var_y = gt.assortativity(g, y)\n",
    "    print(f\"Assortativity by known label '{known_key}':\", round(r_y, 3), \"+/-\", round(np.sqrt(var_y), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f97da",
   "metadata": {},
   "source": [
    "### 15.5 Posterior predictive checks: does the fitted model reproduce key statistics?\n",
    "\n",
    "A foundational Bayesian habit:\n",
    "\n",
    "> Fit a generative model → **simulate replicated data** from the fitted model → compare to what you observed.\n",
    "\n",
    "For a network model with latent structure $(b,\\theta)$, the **posterior predictive** distribution is:\n",
    "\n",
    "$$P(G_{\\mathrm{rep}} \\mid G)\n",
    "= \\int P(G_{\\mathrm{rep}} \\mid b,\\theta)\\;P(b,\\theta \\mid G)\\, db\\, d\\theta.$$\n",
    "\n",
    "In practice, we approximate this by:\n",
    "1. fitting a state (which approximates the posterior)\n",
    "2. sampling replicated graphs $G_{\\mathrm{rep}}$ from the fitted state\n",
    "3. computing statistics $T(G_{\\mathrm{rep}})$ and comparing to $T(G)$\n",
    "\n",
    "If the observed statistic is consistently extreme under the replicated distribution, the model is missing something.\n",
    "\n",
    "Below we check several statistics:\n",
    "- global clustering coefficient\n",
    "- degree assortativity (by degree)\n",
    "- approximate average shortest-path length (via sampled pairs)\n",
    "\n",
    "You can add more: motif counts, rich-club coefficients, reciprocity (directed graphs), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the finest level (level 0) for sampling replicated graphs\n",
    "base_state = best.get_levels()[0]  # BlockState on original graph\n",
    "\n",
    "# For statistics, we'll treat the graph as undirected\n",
    "g_obs = gt.GraphView(g, directed=False)\n",
    "\n",
    "def compute_stats(g_in, dist_samples=500):\n",
    "    # Return a small vector of network summary statistics.\n",
    "    # (We keep this tiny, but you can extend it easily.)\n",
    "    # 1) global clustering\n",
    "    c = gt.global_clustering(g_in)[0]\n",
    "\n",
    "    # 2) degree assortativity (scalar assortativity of total degree)\n",
    "    r_deg = gt.scalar_assortativity(g_in, \"total\")[0]\n",
    "\n",
    "    # 3) approximate average shortest-path length via distance histogram\n",
    "    counts, bins = gt.distance_histogram(g_in, bins=[0, 1], samples=dist_samples)\n",
    "    mids = (bins[:-1] + bins[1:]) / 2.0\n",
    "\n",
    "    # drop distance ~ 0 bin (self-pairs)\n",
    "    mask = mids > 0\n",
    "    apl = float((mids[mask] * counts[mask]).sum() / counts[mask].sum())\n",
    "\n",
    "    return c, r_deg, apl\n",
    "\n",
    "# Observed statistics\n",
    "c_obs, rdeg_obs, apl_obs = compute_stats(g_obs, dist_samples=1000)\n",
    "print(\"Observed stats:\")\n",
    "print(\"  global clustering         :\", round(c_obs, 4))\n",
    "print(\"  degree assortativity      :\", round(rdeg_obs, 4))\n",
    "print(\"  avg shortest-path length  :\", round(apl_obs, 3))\n",
    "\n",
    "# Sample from the fitted model\n",
    "rng = np.random.default_rng(1)\n",
    "n_samp = 25\n",
    "stats = []\n",
    "\n",
    "for _ in range(n_samp):\n",
    "    gs = base_state.sample_graph()  # replicate from fitted model\n",
    "    gt.remove_parallel_edges(gs)\n",
    "    gt.remove_self_loops(gs)\n",
    "\n",
    "    gs = gt.GraphView(gs, directed=False)\n",
    "    stats.append(compute_stats(gs, dist_samples=500))\n",
    "\n",
    "stats = np.array(stats)\n",
    "c_samp    = stats[:, 0]\n",
    "rdeg_samp = stats[:, 1]\n",
    "apl_samp  = stats[:, 2]\n",
    "\n",
    "def mc_pvalue(null_samples, obs):\n",
    "    return (1 + np.sum(null_samples >= obs)) / (len(null_samples) + 1)\n",
    "\n",
    "print(\"\\nPosterior predictive p-values (one-sided: replicate >= observed):\")\n",
    "print(\"  clustering p-value    :\", round(mc_pvalue(c_samp, c_obs), 4))\n",
    "print(\"  assortativity p-value :\", round(mc_pvalue(rdeg_samp, rdeg_obs), 4))\n",
    "print(\"  path length p-value   :\", round(mc_pvalue(apl_samp, apl_obs), 4))\n",
    "\n",
    "# Plot PPC distributions\n",
    "def ppc_hist(samples, obs, title, xlabel):\n",
    "    plt.figure(figsize=(6.5, 3.2))\n",
    "    plt.hist(samples, bins=12, density=True)\n",
    "    plt.axvline(obs, linewidth=3, label=\"observed\", color='k')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"density (approx)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "ppc_hist(c_samp,    c_obs,    \"Posterior predictive check: clustering\", \"global clustering\")\n",
    "ppc_hist(rdeg_samp, rdeg_obs, \"Posterior predictive check: degree assortativity\", \"degree assortativity\")\n",
    "ppc_hist(apl_samp,  apl_obs,  \"Posterior predictive check: mean path length\", \"avg shortest-path length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee327f",
   "metadata": {},
   "source": [
    "### 15.6 Interpretation prompts (for discussion)\n",
    "\n",
    "1. Does the inferred *coarse* partition look like \"two big camps\"?\n",
    "2. Are there smaller subcommunities within camps?\n",
    "3. Which nodes appear as bridges between groups?\n",
    "4. In the posterior predictive check:\n",
    "   - Is the observed clustering typical under the model?\n",
    "   - If not, what kinds of mechanisms might increase clustering beyond SBM?\n",
    "     (e.g., triadic closure, geometric constraints, social circles)\n",
    "\n",
    "---\n",
    "\n",
    "### 15.7 Exercises + further resources\n",
    "\n",
    "#### Exercises (suggested)\n",
    "1. **NetworkX → graph-tool translation drill**  \n",
    "   Take a NetworkX graph you built in a previous assignment and:\n",
    "   - convert to graph-tool\n",
    "   - compute betweenness and clustering in both libraries\n",
    "   - verify results agree (approximately)\n",
    "\n",
    "2. **Model comparison**  \n",
    "   On PolBlogs (or another dataset):\n",
    "   - compare `deg_corr=True` vs `deg_corr=False`\n",
    "   - record description lengths and inferred B per level\n",
    "   - write a short interpretation: *why do you think one wins?*\n",
    "\n",
    "3. **Uncertainty**  \n",
    "   For Karate Club:\n",
    "   - compute marginals with MCMC\n",
    "   - identify the most uncertain nodes\n",
    "   - relate uncertainty to graph position (bridges, cut vertices, etc.)\n",
    "\n",
    "4. **Posterior predictive checks**  \n",
    "   Pick a statistic (e.g., average path length, assortativity by degree) and test whether the fitted SBM reproduces it.\n",
    "\n",
    "#### Further reading / references\n",
    "- graph-tool cookbook: **Inferring modular network structure** (official docs)\n",
    "- Tiago Peixoto's papers on Bayesian/MDL SBM inference and nested SBMs\n",
    "- A reminder: generative models are *useful stories*, not \"truth machines\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fce80e",
   "metadata": {},
   "source": [
    "### Checks for understanding (after §15)\n",
    "\n",
    "1. In the PolBlogs example, what features of the graph are you implicitly asking the SBM to explain?\n",
    "2. Posterior predictive checks ask: \"If the model were true, would data like this be typical?\"  \n",
    "   What did you choose to check here, and what would be a good *additional* check for this dataset?\n",
    "3. If you have node metadata (e.g., \"liberal\" / \"conservative\"), where should it enter the analysis: as **validation**, as **priors**, or as **covariates**? What are the risks of each?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dab713",
   "metadata": {},
   "source": [
    "## 16. Community detection gallery: fit SBMs on several networks and compare the pictures\n",
    "\n",
    "PolBlogs is a great teaching dataset because it has a clear political interpretation.\n",
    "\n",
    "But one danger in learning any method from a single dataset is that you start thinking the method *always* produces the same kind of output.\n",
    "\n",
    "So here we do something simple:\n",
    "\n",
    "- load a few networks with very different structures\n",
    "- fit the same **degree-corrected nested SBM** workflow\n",
    "- draw the resulting coarse partition\n",
    "\n",
    "The goal is not to argue that SBMs are \"right\" for every dataset.  \n",
    "The goal is to build **visual intuition** for what these partitions look like across domains.\n",
    "\n",
    "> If a dataset fails to download on a given machine (network restrictions), skip it and run the ones that work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def load_dataset(name):\n",
    "    \"\"\"Try gt.collection.data first, then gt.collection.ns. Fallback to a few NetworkX built-ins.\"\"\"\n",
    "    # graph-tool collections (may download on first use)\n",
    "    try:\n",
    "        g0 = gt.collection.data[name]\n",
    "        return g0, f\"gt.collection.data['{name}']\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            g0 = gt.collection.ns[name]\n",
    "            return g0, f\"gt.collection.ns['{name}']\"\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # A few offline fallbacks via NetworkX\n",
    "    if name == \"karate\":\n",
    "        return nx_to_gt(nx.karate_club_graph()), \"NetworkX: karate_club_graph() (fallback)\"\n",
    "    if name == \"karate_nx\":\n",
    "        return nx_to_gt(nx.karate_club_graph()), \"NetworkX: karate_club_graph()\"\n",
    "    if name == \"lesmis_nx\":\n",
    "        return nx_to_gt(nx.les_miserables_graph()), \"NetworkX: les_miserables_graph()\"\n",
    "    if name == \"florentine_nx\":\n",
    "        return nx_to_gt(nx.florentine_families_graph()), \"NetworkX: florentine_families_graph()\"\n",
    "\n",
    "    raise KeyError(f\"Could not load dataset '{name}'.\")\n",
    "\n",
    "def clean_undirected_simple(g_in):\n",
    "    \"\"\"Return a cleaned, simple, undirected graph.\n",
    "\n",
    "    Steps:\n",
    "    - copy the input graph (so edits are safe),\n",
    "    - make it undirected,\n",
    "    - remove parallel edges and self-loops,\n",
    "    - keep only the largest connected component.\n",
    "\n",
    "    This keeps the gallery focused on community structure instead of artifacts.\n",
    "    \"\"\"\n",
    "    g = gt.Graph(g_in)\n",
    "    g.set_directed(False)\n",
    "\n",
    "    gt.remove_parallel_edges(g)\n",
    "    gt.remove_self_loops(g)\n",
    "\n",
    "    g = gt.extract_largest_component(g, directed=False, prune=True)\n",
    "    return g\n",
    "\n",
    "def fit_nested_sbm_and_choose_level(g, B_min=2, B_max=10, deg_corr=True):\n",
    "    state = gt.minimize_nested_blockmodel_dl(g, state_args=dict(deg_corr=deg_corr))\n",
    "\n",
    "    # Determine B at each level\n",
    "    bs = state.get_bs()\n",
    "    Bs = [len(np.unique(np.asarray(b))) for b in bs]\n",
    "\n",
    "    # Choose a \"teaching level\": coarse but not trivial\n",
    "    candidate_levels = [i for i, B in enumerate(Bs) if B_min <= B <= B_max]\n",
    "    level = candidate_levels[-1] if candidate_levels else 0\n",
    "\n",
    "    return state, level, Bs\n",
    "\n",
    "def draw_state(state, g, title, level=0):\n",
    "    st = state.project_level(level)\n",
    "    pos = g.vp[\"pos\"] if \"pos\" in g.vp else gt.sfdp_layout(g)\n",
    "    print(title)\n",
    "    print(\"  vertices:\", g.num_vertices(), \"edges:\", g.num_edges())\n",
    "    print(\"  chosen level:\", level)\n",
    "    st.draw(pos=pos, output_size=(500, 500))\n",
    "\n",
    "# Pick a small gallery. Add/remove items as you like.\n",
    "gallery = [\n",
    "    (\"karate\", \"Karate Club (classic social network)\"), \n",
    "    (\"dolphins\", \"Dolphins (animal social network)\"), \n",
    "    (\"football\", \"College football (games network)\"), \n",
    "    (\"polbooks\", \"Political books (co-purchase network)\"),\n",
    "    (\"celegansneural\", \"Worm brain\"),\n",
    "    (\"power\", \"Power grid\"),\n",
    "    (\"serengeti-foodweb\",'Food web'),\n",
    "    \n",
    "    # Offline fallbacks (no download required if NetworkX has them)\n",
    "    (\"lesmis_nx\", \"Les Misérables (character co-appearance)\"), \n",
    "    (\"florentine_nx\", \"Florentine families (marriage/business ties)\"), \n",
    "]\n",
    "\n",
    "for name, label in gallery:\n",
    "    try:\n",
    "        g0, src = load_dataset(name)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {name}: {e}\")\n",
    "        print(\"---\")\n",
    "        continue\n",
    "\n",
    "    g = clean_undirected_simple(g0)\n",
    "\n",
    "    state, level, Bs = fit_nested_sbm_and_choose_level(g, B_min=2, B_max=12, deg_corr=True)\n",
    "\n",
    "    title = f\"{label}  |  {src}  |  B by level = {Bs}\"\n",
    "    draw_state(state, g, title=title, level=level)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0e590",
   "metadata": {},
   "source": [
    "### Wrap-up: what to carry forward\n",
    "\n",
    "A good mental checklist when you run community detection \"for real\":\n",
    "\n",
    "1. **Start with a null model.** Ask what structure you would see \"by chance,\" given what must be preserved (size, degrees, etc.).\n",
    "2. **Be explicit about assumptions.** Modularity, Infomap, and SBMs answer different questions because they assume different generating stories.\n",
    "3. **Model selection is part of the problem.** If you must choose the number of communities by hand, you are doing model selection \"off to the side.\"\n",
    "4. **Uncertainty matters.** A single partition is rarely the full story; look at marginals, stability, and posterior predictive behavior.\n",
    "\n",
    "For projects, the most valuable upgrade you can make is to connect inference output back to a **substantive question**:\n",
    "- What is a \"community\" in this domain?\n",
    "- What mechanism might generate this mesoscopic structure?\n",
    "- What alternative explanations (degree sequence, bipartite structure, sampling artifacts) must be ruled out?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aecb85",
   "metadata": {},
   "source": [
    "## 17. References and further reading\n",
    "\n",
    "This notebook is opinionated: it pushes you toward **inference-based** community detection, because it makes model assumptions explicit and gives you uncertainty.\n",
    "\n",
    "That said, graduate-level network science is partly about knowing the *tool landscape*. Here are sources worth keeping close.\n",
    "\n",
    "### `graph-tool` (documentation + examples)\n",
    "\n",
    "- The official `graph-tool` manual and \"inference\" documentation (SBMs, nested SBMs, overlaps, layers, link prediction).\n",
    "- Netzschleuder: the dataset repository used by `graph-tool` (via `gt.collection.ns[...]`).\n",
    "\n",
    "### Community detection: core papers\n",
    "\n",
    "- Girvan, M. & Newman, M. E. J. (2002). *Community structure in social and biological networks*. **PNAS**, 99(12), 7821-7826. DOI: 10.1073/pnas.122653799\n",
    "- Newman, M. E. J. & Girvan, M. (2004). *Finding and evaluating community structure in networks*. **Physical Review E**, 69, 026113. DOI: 10.1103/PhysRevE.69.026113\n",
    "- Newman, M. E. J. (2006). *Modularity and community structure in networks*. **PNAS**, 103(23), 8577-8582. DOI: 10.1073/pnas.0601602103\n",
    "- Blondel, V. D., Guillaume, J.-L., Lambiotte, R., & Lefebvre, E. (2008). *Fast unfolding of communities in large networks* (Louvain). **J. Stat. Mech.**, P10008. DOI: 10.1088/1742-5468/2008/10/P10008\n",
    "- Traag, V. A., Waltman, L., & van Eck, N. J. (2019). *From Louvain to Leiden: guaranteeing well-connected communities*. **Scientific Reports**, 9, 5233. DOI: 10.1038/s41598-019-41695-z\n",
    "- Rosvall, M. & Bergstrom, C. T. (2008). *Maps of random walks on complex networks reveal community structure* (Infomap / map equation). **PNAS**, 105(4), 1118-1123. DOI: 10.1073/pnas.0706851105\n",
    "\n",
    "### Reviews (excellent \"big picture\" overviews)\n",
    "\n",
    "- Fortunato, S. (2010). *Community detection in graphs*. **Physics Reports**, 486(3-5), 75-174. DOI: 10.1016/j.physrep.2009.11.002\n",
    "- Abbe, E. (2018). *Community Detection and Stochastic Block Models: Recent Developments*. **JMLR**, 18(177), 1-86. (Often circulated as a free PDF; search the title.)\n",
    "\n",
    "### \"Modularity is tricky\" (read these before you publish with $Q$)\n",
    "\n",
    "- Fortunato, S. & Barthélemy, M. (2007). *Resolution limit in community detection*. **PNAS**, 104(1), 36-41. DOI: 10.1073/pnas.0605965104\n",
    "- Good, B. H., de Montjoye, Y.-A., & Clauset, A. (2010). *The performance of modularity maximization in practical contexts*. **Physical Review E**, 81, 046106. DOI: 10.1103/PhysRevE.81.046106\n",
    "\n",
    "### Stochastic block models and inference\n",
    "\n",
    "- Holland, P. W., Laskey, K. B., & Leinhardt, S. (1983). *Stochastic blockmodels: First steps*. **Social Networks**, 5(2), 109-137. DOI: 10.1016/0378-8733(83)90021-7\n",
    "- Karrer, B. & Newman, M. E. J. (2011). *Stochastic blockmodels and community structure in networks* (degree correction). **Physical Review E**, 83, 016107. DOI: 10.1103/PhysRevE.83.016107\n",
    "- Peixoto, T. P. (2014). *Hierarchical block structures and high-resolution model selection in large networks*. **Physical Review X**, 4, 011047. DOI: 10.1103/PhysRevX.4.011047\n",
    "- Peixoto, T. P. (2014). *Efficient Monte Carlo and greedy heuristic for the inference of stochastic block models*. **Physical Review E**, 89, 012804. DOI: 10.1103/PhysRevE.89.012804\n",
    "- Peixoto, T. P. (2017). *Nonparametric Bayesian inference of the microcanonical stochastic block model*. **Physical Review E**, 95, 012317. DOI: 10.1103/PhysRevE.95.012317\n",
    "- Funke, T. & Becker, T. (2019). *Stochastic block models: A comparison of variants and inference methods*. **PLOS ONE**, 14(4): e0215296. DOI: 10.1371/journal.pone.0215296\n",
    "\n",
    "### Bayesian inference and MDL (background that transfers everywhere)\n",
    "\n",
    "- Kass, R. E. & Raftery, A. E. (1995). *Bayes factors*. **Journal of the American Statistical Association**, 90(430), 773-795. DOI: 10.1080/01621459.1995.10476572\n",
    "- Rissanen, J. (1978). *Modeling by shortest data description*. **Automatica**, 14(5), 465-471. DOI: 10.1016/0005-1098(78)90005-5\n",
    "- MacKay, D. J. C. (2003). *Information Theory, Inference, and Learning Algorithms*. Cambridge University Press. ISBN-13: 978-0521642989\n",
    "- Gelman, A., et al. (various editions). *Bayesian Data Analysis* (excellent for building \"posterior intuition\")\n",
    "\n",
    "### Network science background texts (great to keep on your desk)\n",
    "\n",
    "- Newman, M. E. J. (2010). *Networks: An Introduction*. Oxford University Press. ISBN-13: 978-0199206650\n",
    "- Kolaczyk, E. D. & Csárdi, G. (2014). *Statistical Analysis of Network Data with R*. Springer (Use R!). DOI: 10.1007/978-1-4939-0983-4\n",
    "\n",
    "---\n",
    "\n",
    "If you want one \"path\" through these:\n",
    "\n",
    "1. Newman & Girvan → modularity and early community detection\n",
    "2. Fortunato & Barthélemy + Good et al. → why modularity can mislead\n",
    "3. Karrer & Newman → degree correction\n",
    "4. Peixoto (2014 PRX) + Peixoto (2017 PRE) → nested SBMs + nonparametric Bayesian/MDL inference in practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d1160",
   "metadata": {},
   "source": [
    "# Appendix A: NetworkX ↔ `graph-tool` cheat sheet\n",
    "\n",
    "This appendix is meant to be something you can **keep open while coding**.\n",
    "\n",
    "> In `graph-tool`, the \"secret\" is: **structure in the Graph, data in property maps**.\n",
    "\n",
    "---\n",
    "\n",
    "## A.1 Create a graph\n",
    "\n",
    "**NetworkX**\n",
    "```python\n",
    "G = nx.Graph()\n",
    "G = nx.DiGraph()\n",
    "```\n",
    "\n",
    "**graph-tool**\n",
    "```python\n",
    "g = gt.Graph(directed=False)\n",
    "g = gt.Graph(directed=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## A.2 Add edges\n",
    "\n",
    "**NetworkX**\n",
    "```python\n",
    "G.add_edge(\"a\", \"b\")\n",
    "G.add_edges_from([(\"a\",\"b\"), (\"b\",\"c\")])\n",
    "```\n",
    "\n",
    "**graph-tool**\n",
    "```python\n",
    "g.add_edge(g.vertex(0), g.vertex(1))\n",
    "g.add_edge_list([(0,1), (1,2)])\n",
    "```\n",
    "\n",
    "If your node IDs are strings:\n",
    "```python\n",
    "v_id = g.add_edge_list(edge_list, hashed=True, hash_type=\"string\")\n",
    "g.vp[\"id\"] = v_id\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## A.3 Node/edge attributes\n",
    "\n",
    "**NetworkX**\n",
    "```python\n",
    "G.nodes[\"a\"][\"age\"] = 20\n",
    "G.edges[\"a\",\"b\"][\"w\"] = 0.7\n",
    "```\n",
    "\n",
    "**graph-tool**\n",
    "```python\n",
    "age = g.new_vp(\"int\")\n",
    "w   = g.new_ep(\"double\")\n",
    "\n",
    "age[v] = 20\n",
    "w[e]   = 0.7\n",
    "\n",
    "g.vp[\"age\"] = age\n",
    "g.ep[\"w\"]   = w\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## A.4 Subgraphs\n",
    "\n",
    "**NetworkX**\n",
    "```python\n",
    "H = G.subgraph(nodes_to_keep).copy()\n",
    "```\n",
    "\n",
    "**graph-tool**\n",
    "```python\n",
    "vfilt = g.new_vp(\"bool\")\n",
    "vfilt.a = ...  # boolean mask of length N\n",
    "gv = gt.GraphView(g, vfilt=vfilt)\n",
    "H = gt.Graph(gv, prune=True)  # optional: make a real copy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## A.5 Degrees\n",
    "\n",
    "**NetworkX**\n",
    "```python\n",
    "deg = dict(G.degree())\n",
    "```\n",
    "\n",
    "**graph-tool**\n",
    "```python\n",
    "deg = g.degree_property_map(\"total\").a  # numpy array\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## A.6 Clustering\n",
    "\n",
    "**NetworkX**\n",
    "```python\n",
    "nx.average_clustering(G)\n",
    "```\n",
    "\n",
    "**graph-tool**\n",
    "```python\n",
    "gt.global_clustering(g)  # returns (value, std)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## A.7 Shortest paths\n",
    "\n",
    "**NetworkX**\n",
    "```python\n",
    "nx.shortest_path_length(G, source, target)\n",
    "```\n",
    "\n",
    "**graph-tool**\n",
    "```python\n",
    "gt.shortest_distance(g, source=v, target=u)\n",
    "```\n",
    "\n",
    "Weighted:\n",
    "```python\n",
    "gt.shortest_distance(g, source=v, weights=g.ep.weight)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## A.8 Community detection (conceptual translation)\n",
    "\n",
    "- NetworkX workflows often end at: **partition that maximizes modularity**\n",
    "- graph-tool workflows often start at: **generative model** → infer partition + uncertainty\n",
    "\n",
    "**NetworkX (Louvain)**\n",
    "```python\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "parts = louvain_communities(G, seed=0)\n",
    "```\n",
    "\n",
    "**graph-tool (SBM inference)**\n",
    "```python\n",
    "state = gt.minimize_nested_blockmodel_dl(g, state_args=dict(deg_corr=True))\n",
    "state.draw()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## A.9 Saving/loading\n",
    "\n",
    "**NetworkX**\n",
    "```python\n",
    "nx.write_graphml(G, \"g.graphml\")\n",
    "G = nx.read_graphml(\"g.graphml\")\n",
    "```\n",
    "\n",
    "**graph-tool**\n",
    "```python\n",
    "g.save(\"g.gt\")\n",
    "g = gt.load_graph(\"g.gt\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d319bca",
   "metadata": {},
   "source": [
    "# Appendix B: `graph-tool` datasets and Netzschleuder\n",
    "\n",
    "`graph-tool` ships with a small built-in collection (`gt.collection.data`) and\n",
    "can also download many more graphs from **Netzschleuder** (`gt.collection.ns`).\n",
    "\n",
    "Why this matters for learning:\n",
    "- You can grab real graphs in 1 line\n",
    "- You can reproduce experiments easily\n",
    "- You can focus on analysis, not data wrangling\n",
    "\n",
    "<p>\n",
    "<img src=\"images/netzschleuder.png\" width=\"650\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## B.1 Built-in datasets (`gt.collection.data`)\n",
    "\n",
    "These typically work offline once installed.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "g = gt.collection.data[\"polbooks\"]\n",
    "```\n",
    "\n",
    "## B.2 Netzschleuder datasets (`gt.collection.ns`)\n",
    "\n",
    "Netzschleuder access requires internet the first time; graphs are cached locally afterward.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "g = gt.collection.ns[\"ego_social/facebook_combined\"]  # example dataset\n",
    "```\n",
    "\n",
    "<p>\n",
    "<img src=\"images/facebook-sbm.png\" width=\"650\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## B.3 Hands-on: list dataset keys and load an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Some built-in datasets:\", list(gt.collection.data.keys())[:20])\n",
    "\n",
    "# Netzschleuder categories can be large; this may require internet.\n",
    "try:\n",
    "    # This prints a dictionary-like structure with available collections.\n",
    "    print(\"\\nNetzschleuder example keys (may take a moment):\")\n",
    "    print(list(gt.collection.ns.keys())[:10])\n",
    "except Exception as e:\n",
    "    print(\"Could not access netzschleuder in this environment:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Facebook combined ego network (requires internet for first download)\n",
    "try:\n",
    "    g_fb = gt.collection.ns[\"ego_social/facebook_combined\"]\n",
    "    print(\"Loaded facebook_combined:\", g_fb.num_vertices(), \"V,\", g_fb.num_edges(), \"E\")\n",
    "    print(\"Vertex properties:\", list(g_fb.vp.keys())[:10])\n",
    "\n",
    "    # positions are sometimes stored as an internal property map\n",
    "    pos = g_fb.vp[\"_pos\"] if \"_pos\" in g_fb.vp else gt.sfdp_layout(g_fb)\n",
    "    gt.graph_draw(g_fb, pos=pos, output_size=(800, 800))\n",
    "\n",
    "    # quick SBM inference (warning: can be computationally heavy)\n",
    "    st = gt.minimize_blockmodel_dl(g_fb, state_args=dict(deg_corr=True))\n",
    "    st.draw(pos=pos, output_size=(800, 800))\n",
    "except Exception as e:\n",
    "    print(\"Could not load ego_social/facebook_combined (maybe offline):\", repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be30d5",
   "metadata": {},
   "source": [
    "# Appendix C: Weighted graphs in `graph-tool` (distances and SBM inference)\n",
    "\n",
    "Edge \"weights\" can mean different things:\n",
    "- **cost** or **distance** (shortest paths)\n",
    "- **tie strength** (stronger relationship)\n",
    "- **frequency** (number of interactions)\n",
    "- **capacity** (flow)\n",
    "\n",
    "In `graph-tool`, weights are usually stored in an **edge property map** (e.g., `double`).\n",
    "\n",
    "---\n",
    "\n",
    "## C.1 Weighted shortest paths\n",
    "\n",
    "If you have `g.ep[\"weight\"]`, you can do:\n",
    "\n",
    "```python\n",
    "dist = gt.shortest_distance(g, source=v, weights=g.ep.weight)\n",
    "```\n",
    "\n",
    "This treats weights as *edge lengths/costs*.  \n",
    "If your weights are \"strength\", you might want to invert them (cost = 1/strength).\n",
    "\n",
    "---\n",
    "\n",
    "## C.2 Weighted SBMs (edge covariates)\n",
    "\n",
    "SBM inference can incorporate weights as **edge covariates** using:\n",
    "- `recs=[...]` (list of edge property maps)\n",
    "- `rec_types=[...]` (list of distributional assumptions)\n",
    "\n",
    "Common `rec_types` include:\n",
    "- `\"real-normal\"` (continuous real weights)\n",
    "- `\"real-exponential\"`\n",
    "- `\"discrete-poisson\"` (counts)\n",
    "- `\"discrete-binomial\"`, `\"discrete-geometric\"`\n",
    "\n",
    "This is *not* the same as \"weighted adjacency\" — it models:\n",
    "1) whether an edge exists\n",
    "2) the weight of the edge, given it exists\n",
    "\n",
    "---\n",
    "\n",
    "## C.3 Hands-on: weighted distances and a weighted SBM on a tiny example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the earlier pandas edge list\n",
    "df_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eec77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetworkX: weighted shortest path\n",
    "G_w = nx.from_pandas_edgelist(df_edges, \"src\", \"dst\", edge_attr=\"weight\", create_using=nx.Graph)\n",
    "\n",
    "print(\"NetworkX weighted distance alice→dana:\",\n",
    "      nx.shortest_path_length(G_w, \"alice\", \"dana\", weight=\"weight\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a9db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the same weighted graph in graph-tool\n",
    "edge_list = list(df_edges.itertuples(index=False, name=None))\n",
    "g = gt.Graph(directed=False)\n",
    "w = g.new_ep(\"double\")\n",
    "v_id = g.add_edge_list(edge_list, hashed=True, hash_type=\"string\", eprops=[w])\n",
    "\n",
    "g.vp[\"id\"] = v_id\n",
    "g.ep[\"weight\"] = w\n",
    "\n",
    "# Find vertex handles by ID\n",
    "v_by_id = {g.vp.id[v]: v for v in g.vertices()}\n",
    "\n",
    "v_src = v_by_id[\"alice\"]\n",
    "v_dst = v_by_id[\"dana\"]\n",
    "\n",
    "dist = gt.shortest_distance(g, source=v_src, weights=g.ep.weight)\n",
    "print(\"graph-tool weighted distance alice→dana:\", dist[v_dst])\n",
    "\n",
    "# Weighted SBM inference (toy example)\n",
    "state_w = gt.minimize_blockmodel_dl(\n",
    "    g,\n",
    "    state_args=dict(\n",
    "        deg_corr=True,\n",
    "        recs=[g.ep.weight],\n",
    "        rec_types=[\"real-normal\"]\n",
    "    )\n",
    ")\n",
    "print(\"Weighted SBM DL:\", round(state_w.entropy(), 2), \"B =\", state_w.get_nonempty_B())\n",
    "state_w.draw(output_size=(650, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61492d",
   "metadata": {},
   "source": [
    "# Appendix D: Practical tips, performance, and common gotchas\n",
    "\n",
    "## D.1 OpenMP threads (parallel algorithms)\n",
    "\n",
    "Many `graph-tool` algorithms use OpenMP parallelism.\n",
    "\n",
    "You can inspect and set threads:\n",
    "\n",
    "```python\n",
    "import graph_tool.all as gt\n",
    "import graph_tool\n",
    "\n",
    "print(graph_tool.openmp_enabled())\n",
    "print(graph_tool.openmp_get_num_threads())\n",
    "graph_tool.openmp_set_num_threads(8)\n",
    "```\n",
    "\n",
    "You can also use a context manager:\n",
    "```python\n",
    "with graph_tool.openmp_context(nthreads=8):\n",
    "    # run expensive algorithms here\n",
    "    ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## D.2 Gotcha: multigraph by default\n",
    "\n",
    "Parallel edges and self-loops are allowed unless you remove them:\n",
    "```python\n",
    "gt.remove_parallel_edges(g)\n",
    "gt.remove_self_loops(g)\n",
    "```\n",
    "\n",
    "This matters for:\n",
    "- clustering coefficient\n",
    "- simple-graph assumptions\n",
    "- some visualizations\n",
    "\n",
    "---\n",
    "\n",
    "## D.3 Gotcha: property map types\n",
    "\n",
    "If you create a property map with the wrong type, you'll get confusing errors.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "p = g.new_vp(\"int\")\n",
    "p[v] = \"alice\"   # error / nonsense\n",
    "```\n",
    "\n",
    "When in doubt:\n",
    "- store labels as `\"string\"`\n",
    "- store numeric quantities as `\"double\"`\n",
    "- store masks as `\"bool\"`\n",
    "\n",
    "---\n",
    "\n",
    "## D.4 Gotcha: label switching in MCMC\n",
    "\n",
    "Block labels are arbitrary. Partition A and partition B might be the *same* partition up to relabeling.\n",
    "\n",
    "This is why we used:\n",
    "- `PartitionModeState` for marginals\n",
    "\n",
    "---\n",
    "\n",
    "## D.5 When you get stuck\n",
    "\n",
    "1. Print what you have:\n",
    "```python\n",
    "print(g)\n",
    "print(list(g.vp.keys()))\n",
    "print(list(g.ep.keys()))\n",
    "```\n",
    "\n",
    "2. Start with smaller graphs (Karate, PolBooks), then scale up.\n",
    "\n",
    "3. Use the cookbook demos in the official docs:\n",
    "- *Inferring modular network structure*\n",
    "- *Weighted network inference*\n",
    "- *Network reconstruction*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_tool\n",
    "print(\"OpenMP enabled:\", graph_tool.openmp_enabled())\n",
    "print(\"Current threads:\", graph_tool.openmp_get_num_threads())\n",
    "# Example: set threads (adjust to your machine)\n",
    "graph_tool.openmp_set_num_threads(4)\n",
    "print(\"Threads after set:\", graph_tool.openmp_get_num_threads())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f88067d",
   "metadata": {},
   "source": [
    "\n",
    "# Appendix E: Modularity, `ModularityState`, and why inference is better\n",
    "\n",
    "In **Class 4**, you likely saw modularity and its limitations.  \n",
    "Here we show how to compute modularity in `graph-tool`, and connect it to statistical inference.\n",
    "\n",
    "---\n",
    "\n",
    "## E.1 Newman's generalized modularity (equation)\n",
    "\n",
    "Given a partition $b$, Newman's generalized modularity is:\n",
    "\n",
    "$$Q = \\frac{1}{2m}\\sum_{ij}\\left(A_{ij} - \\gamma\\frac{k_i k_j}{2m}\\right)\\delta(b_i,b_j),$$\n",
    "\n",
    "where:\n",
    "- $m$ is the number of edges\n",
    "- $k_i$ is the degree of node $i$\n",
    "- $\\gamma$ is a **resolution parameter**\n",
    "- $\\delta(b_i,b_j)=1$ if $i$ and $j$ are in the same community\n",
    "\n",
    "Interpretation:\n",
    "- compares within-community edge density to a configuration-model null baseline\n",
    "- $\\gamma$ tunes the \"preferred\" community size (resolution limit issues are real)\n",
    "\n",
    "---\n",
    "\n",
    "## E.2 `graph-tool` has modularity maximization — but it's explicitly labeled dangerous\n",
    "\n",
    "`graph-tool` can do modularity optimization via `ModularityState`.\n",
    "\n",
    "But its own documentation warns that modularity lacks statistical regularization and tends to overfit.  \n",
    "We include it here so you can **compare** it to SBM inference and understand the difference.\n",
    "\n",
    "---\n",
    "\n",
    "## E.3 Hands-on: maximize modularity on an ER graph (overfitting demo)\n",
    "\n",
    "We generate an Erdős-Rényi graph and maximize modularity.\n",
    "Even though the graph has no planted communities, modularity typically finds a partition with non-trivial $Q$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af88bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate an Erdos-Renyi graph in NetworkX for convenience, then convert.\n",
    "# N = 250\n",
    "# p = 0.03\n",
    "# G_er = nx.erdos_renyi_graph(N, p, seed=1)\n",
    "# g_er = nx_to_gt(G_er)\n",
    "\n",
    "# # Modularity maximization via ModularityState (graph-tool warns about overfitting)\n",
    "# mod_state = gt.minimize_blockmodel_dl(g_er, state=gt.ModularityState)\n",
    "# b_mod = mod_state.get_blocks()\n",
    "\n",
    "# Q = gt.modularity(g_er, b_mod, gamma=1.0)\n",
    "# print(\"ER graph (N={}, p={}):\".format(N, p))\n",
    "# print(\"  inferred B (modularity) :\", mod_state.get_nonempty_B())\n",
    "# print(\"  modularity Q            :\", round(Q, 4))\n",
    "\n",
    "# # Compare to SBM inference (degree-corrected)\n",
    "# sbm_state = gt.minimize_blockmodel_dl(g_er, state_args=dict(deg_corr=True))\n",
    "# print(\"\\nSBM inference on the same ER graph:\")\n",
    "# print(\"  inferred B (SBM)        :\", sbm_state.get_nonempty_B())\n",
    "# print(\"  description length Σ    :\", round(sbm_state.entropy(), 3), \"(nats)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c8705",
   "metadata": {},
   "source": [
    "\n",
    "# Appendix F: Overlapping communities with `OverlapBlockState`\n",
    "\n",
    "Many real networks have **overlapping** community structure:\n",
    "\n",
    "- people belong to multiple social circles\n",
    "- papers span multiple topics\n",
    "- genes participate in multiple pathways\n",
    "\n",
    "A standard SBM assumes a *single* block label per node.  \n",
    "`graph-tool` supports overlapping SBMs via `OverlapBlockState`.\n",
    "\n",
    "---\n",
    "\n",
    "## F.1 Conceptual sketch\n",
    "\n",
    "Instead of a single block label $b_i$, a node can have multiple memberships.  \n",
    "One way to think about this is: **each half-edge** of a node may \"belong\" to a different block, so a node can connect to multiple groups in different interaction contexts.\n",
    "\n",
    "Overlapping models are powerful — and more complex — so treat this as an optional extension if you want to go deeper.\n",
    "\n",
    "---\n",
    "\n",
    "## F.2 Minimal workflow in `graph-tool`\n",
    "\n",
    "1. Fit a standard (non-overlapping) SBM\n",
    "2. Fit an overlapping SBM\n",
    "3. Compare description lengths\n",
    "4. If the overlap model is favored, interpret carefully (uncertainty is even more important)\n",
    "\n",
    "Below we show the minimal code on Karate Club.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df5ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx_to_gt(nx.karate_club_graph())\n",
    "\n",
    "base = gt.minimize_blockmodel_dl(g, state_args=dict(deg_corr=True))\n",
    "ov   = gt.minimize_blockmodel_dl(g, state=gt.OverlapBlockState,\n",
    "                                 state_args=dict(deg_corr=True))\n",
    "\n",
    "print(\"Non-overlap Σ:\", round(base.entropy(), 3), \"nats | B:\", base.get_nonempty_B())\n",
    "print(\"Overlap    Σ:\", round(ov.entropy(),   3), \"nats | B:\", ov.get_nonempty_B())\n",
    "print(\"ΔΣ (overlap - base):\", round(ov.entropy() - base.entropy(), 3))\n",
    "\n",
    "pos = gt.sfdp_layout(g)\n",
    "base.draw(pos=pos, output_size=(650, 600))\n",
    "ov.draw(pos=pos, output_size=(650, 600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8744005",
   "metadata": {},
   "source": [
    "\n",
    "# Appendix G: Multilayer (multiplex) networks with `LayeredBlockState`\n",
    "\n",
    "Many real-world network science problems involve **multiple edge types**:\n",
    "\n",
    "- friendship vs advice ties\n",
    "- retweets vs replies\n",
    "- co-authorship vs citation\n",
    "- positive vs negative interactions\n",
    "\n",
    "A naive approach aggregates everything into one graph, which can erase structure.  \n",
    "`graph-tool` supports **layered SBMs** where each edge belongs to a *layer*.\n",
    "\n",
    "---\n",
    "\n",
    "## G.1 Representing layers\n",
    "\n",
    "We store a layer label on each edge:\n",
    "\n",
    "- `ec[e] = 0` for layer 0\n",
    "- `ec[e] = 1` for layer 1\n",
    "- ...\n",
    "\n",
    "The graph can be a multigraph so the same node pair can have multiple edges across layers.\n",
    "\n",
    "---\n",
    "\n",
    "## G.2 Synthetic example: two layers, same nodes, different mixing\n",
    "\n",
    "We build a toy multiplex graph with two layers and then fit a layered SBM:\n",
    "\n",
    "```python\n",
    "state = gt.minimize_blockmodel_dl(g,\n",
    "                                 state=gt.LayeredBlockState,\n",
    "                                 state_args=dict(ec=ec, layers=True, deg_corr=True))\n",
    "```\n",
    "\n",
    "We compare it to fitting a single SBM on the aggregated graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(5)\n",
    "\n",
    "N = 80\n",
    "B = 4\n",
    "sizes = [20, 20, 20, 20]\n",
    "b_true = np.concatenate([np.full(s, r) for r, s in enumerate(sizes)])\n",
    "\n",
    "# Two layers with different mixing patterns\n",
    "p0 = 0.12 * np.eye(B) + 0.01 * (np.ones((B, B)) - np.eye(B))  # assortative\n",
    "p1 = 0.02 * np.eye(B) + 0.08 * (np.ones((B, B)) - np.eye(B))  # disassortative-ish\n",
    "\n",
    "g = gt.Graph(directed=False)\n",
    "g.add_vertex(N)\n",
    "\n",
    "ec = g.new_ep(\"int\")  # layer label per edge\n",
    "\n",
    "# Add edges for each layer (multigraph allowed)\n",
    "for i in range(N):\n",
    "    for j in range(i + 1, N):\n",
    "        bi, bj = b_true[i], b_true[j]\n",
    "\n",
    "        if rng.random() < p0[bi, bj]:\n",
    "            e = g.add_edge(i, j)\n",
    "            ec[e] = 0\n",
    "\n",
    "        if rng.random() < p1[bi, bj]:\n",
    "            e = g.add_edge(i, j)\n",
    "            ec[e] = 1\n",
    "\n",
    "print(\"Total edges:\", g.num_edges())\n",
    "\n",
    "# Fit layered SBM\n",
    "layered = gt.minimize_blockmodel_dl(\n",
    "    g,\n",
    "    state=gt.LayeredBlockState,\n",
    "    state_args=dict(ec=ec, layers=True, deg_corr=True)\n",
    ")\n",
    "\n",
    "# Fit a single SBM on the aggregated graph (ignoring layers)\n",
    "aggregated = gt.minimize_blockmodel_dl(g, state_args=dict(deg_corr=True))\n",
    "\n",
    "print(\"Layered Σ   :\", round(layered.entropy(), 3), \"nats | B:\", layered.get_nonempty_B())\n",
    "print(\"Aggregated Σ:\", round(aggregated.entropy(), 3), \"nats | B:\", aggregated.get_nonempty_B())\n",
    "\n",
    "# Visualize each layer as a separate GraphView\n",
    "ef0 = g.new_ep(\"bool\")\n",
    "ef1 = g.new_ep(\"bool\")\n",
    "for e in g.edges():\n",
    "    ef0[e] = (ec[e] == 0)\n",
    "    ef1[e] = (ec[e] == 1)\n",
    "\n",
    "g0 = gt.GraphView(g, efilt=ef0)\n",
    "g1 = gt.GraphView(g, efilt=ef1)\n",
    "\n",
    "pos = gt.sfdp_layout(gt.GraphView(g, directed=False))\n",
    "gt.graph_draw(g0, pos=pos, vertex_size=5, output_size=(600, 500))\n",
    "gt.graph_draw(g1, pos=pos, vertex_size=5, output_size=(600, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc177ff",
   "metadata": {},
   "source": [
    "\n",
    "# Appendix H: Link prediction with SBMs (and a path to uncertain networks)\n",
    "\n",
    "A very common network science task:\n",
    "\n",
    "> Given a partially observed network, can we predict missing links?\n",
    "\n",
    "SBMs are generative models, so they naturally support link prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## H.1 A scoring idea\n",
    "\n",
    "Let $G$ be your observed graph, and consider adding a candidate edge $(u,v)$.\n",
    "\n",
    "A simple score is a log posterior predictive ratio:\n",
    "\n",
    "$$\\mathrm{score}(u,v) \\propto \\log\\frac{P(G + (u,v)\\mid \\text{model})}{P(G\\mid \\text{model})}.$$\n",
    "\n",
    "In `graph-tool`, `BlockState.get_edges_prob(missing=[(u,v)])` provides a log-probability quantity you can use as a **ranking score**.\n",
    "\n",
    "---\n",
    "\n",
    "## H.2 Holdout evaluation (AUC)\n",
    "\n",
    "Workflow:\n",
    "\n",
    "1. Remove a set of true edges (holdout set)\n",
    "2. Fit the SBM on the remaining edges\n",
    "3. Score:\n",
    "   - held-out true edges (\"positives\")\n",
    "   - randomly chosen non-edges (\"negatives\")\n",
    "4. Compute an AUC-like statistic:\n",
    "   - probability a random positive gets a higher score than a random negative\n",
    "\n",
    "This is not the only evaluation method, but it's a good start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2)\n",
    "\n",
    "# Use Karate Club for a lightweight demo\n",
    "g_full = nx_to_gt(nx.karate_club_graph())\n",
    "g_full = gt.GraphView(g_full, directed=False)\n",
    "\n",
    "# Hold out a subset of true edges\n",
    "edges = [(int(e.source()), int(e.target())) for e in g_full.edges()]\n",
    "rng.shuffle(edges)\n",
    "n_test = 15\n",
    "test_edges = edges[:n_test]\n",
    "\n",
    "# Build training graph = full graph minus test edges\n",
    "g_train = gt.Graph(g_full)\n",
    "for (u, v) in test_edges:\n",
    "    e = g_train.edge(u, v)\n",
    "    if e is not None:\n",
    "        g_train.remove_edge(e)\n",
    "\n",
    "print(\"Train edges:\", g_train.num_edges(), \"| Test edges:\", len(test_edges))\n",
    "\n",
    "# Fit SBM on training graph\n",
    "state = gt.minimize_blockmodel_dl(g_train, state_args=dict(deg_corr=True))\n",
    "\n",
    "# Score function using the fitted state\n",
    "def score_edge(u, v):\n",
    "    # get_edges_prob expects a list of (source,target) tuples or Edge objects\n",
    "    return float(state.get_edges_prob([(g_train.vertex(u), g_train.vertex(v))]))\n",
    "\n",
    "# Scores for held-out true edges (positives)\n",
    "pos_scores = np.array([score_edge(u, v) for (u, v) in test_edges])\n",
    "\n",
    "# Sample random non-edges (negatives)\n",
    "neg_edges = []\n",
    "N = g_train.num_vertices()\n",
    "while len(neg_edges) < len(test_edges):\n",
    "    u = int(rng.integers(0, N))\n",
    "    v = int(rng.integers(0, N))\n",
    "    if u == v:\n",
    "        continue\n",
    "    if g_train.edge(u, v) is None:\n",
    "        neg_edges.append((u, v))\n",
    "\n",
    "neg_scores = np.array([score_edge(u, v) for (u, v) in neg_edges])\n",
    "\n",
    "# AUC-like statistic: P(score(pos) > score(neg))\n",
    "# (ties count as 0.5)\n",
    "wins = 0.0\n",
    "for s in pos_scores:\n",
    "    wins += np.sum(s > neg_scores) + 0.5 * np.sum(s == neg_scores)\n",
    "auc = wins / (len(pos_scores) * len(neg_scores))\n",
    "\n",
    "print(\"Mean positive score:\", round(pos_scores.mean(), 3))\n",
    "print(\"Mean negative score:\", round(neg_scores.mean(), 3))\n",
    "print(\"AUC (higher is better):\", round(auc, 3))\n",
    "\n",
    "# Visualize score distributions\n",
    "plt.figure(figsize=(6.5, 3.2))\n",
    "plt.hist(pos_scores, bins=10, alpha=0.7, label=\"held-out true edges\")\n",
    "plt.hist(neg_scores, bins=10, alpha=0.7, label=\"random non-edges\")\n",
    "plt.xlabel(\"SBM link-prediction score (log-prob quantity)\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"SBM-based link prediction (Karate Club demo)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe20e3d",
   "metadata": {},
   "source": [
    "\n",
    "# Appendix I: Network dynamics in `graph-tool` — SIS epidemics + spectral thresholds\n",
    "\n",
    "`graph-tool` includes a `dynamics` module for simulating standard processes on networks.\n",
    "This is extremely useful in applied network science.\n",
    "\n",
    "We'll focus on the SIS (susceptible-infectious-susceptible) model.\n",
    "\n",
    "---\n",
    "\n",
    "## I.1 SIS model (equations)\n",
    "\n",
    "Each node is either:\n",
    "- susceptible: $s_i(t)=0$\n",
    "- infectious: $s_i(t)=1$\n",
    "\n",
    "A node $i$ becomes infected based on its infectious neighbors.\n",
    "In `graph-tool`'s SIS model, when node $i$ is updated at time $t$:\n",
    "\n",
    "- If $s_i(t)=0$, it becomes infected with probability\n",
    "\n",
    "$$(1-r_i)\\left[1-\\prod_j(1-\\beta_{ij})^{A_{ij}\\delta_{s_j(t),1}}\\right] + r_i$$\n",
    "\n",
    "- If $s_i(t)=1$, it recovers with probability $\\gamma_i$.\n",
    "\n",
    "Here:\n",
    "- $\\beta_{ij}$ is the transmission probability along edge $i\\leftarrow j$\n",
    "- $\\gamma_i$ is the recovery probability\n",
    "- $r_i$ is spontaneous infection probability (we'll set $r=0$)\n",
    "\n",
    "---\n",
    "\n",
    "## I.2 Spectral intuition: epidemic threshold\n",
    "\n",
    "A common approximation (quenched mean-field) says an epidemic can persist if:\n",
    "\n",
    "$$\\frac{\\beta}{\\gamma} \\gtrsim \\frac{1}{\\lambda_{\\max}},$$\n",
    "\n",
    "where $\\lambda_{\\max}$ is the largest eigenvalue of the adjacency matrix.\n",
    "\n",
    "We'll compute $\\lambda_{\\max}$ and simulate SIS above and below this threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda1c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.dynamics import SISState\n",
    "\n",
    "g = nx_to_gt(nx.karate_club_graph())\n",
    "g = gt.GraphView(g, directed=False)\n",
    "\n",
    "# Largest eigenvalue of adjacency matrix (sparse)\n",
    "A = gt.adjacency(g)\n",
    "lam_max = float(np.real(spla.eigs(A, k=1, which=\"LR\", return_eigenvectors=False)[0]))\n",
    "print(\"lambda_max (adjacency):\", round(lam_max, 3))\n",
    "\n",
    "gamma = 0.2\n",
    "beta_c = gamma / lam_max\n",
    "print(\"Approx threshold beta_c ≈ gamma / lambda_max =\", round(beta_c, 4))\n",
    "\n",
    "# Simulate below and above threshold\n",
    "betas = [0.6 * beta_c, 2.0 * beta_c]\n",
    "T = 200\n",
    "\n",
    "plt.figure(figsize=(7, 3.8))\n",
    "for beta in betas:\n",
    "    state = SISState(g, beta=float(beta), gamma=float(gamma), r=0.0)\n",
    "    X = []\n",
    "    for _ in range(T):\n",
    "        state.iterate_sync()  # one synchronous sweep\n",
    "        # number infectious nodes\n",
    "        X.append(state.get_state().fa.sum())\n",
    "    plt.plot(X, label=f\"beta={beta:.4f}\")\n",
    "\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"# infectious nodes\")\n",
    "plt.title(\"SIS dynamics on Karate Club (below vs above threshold)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e410b",
   "metadata": {},
   "source": [
    "\n",
    "# Appendix J: Graph algorithms every network scientist should know (with `graph-tool`)\n",
    "\n",
    "Beyond inference, `graph-tool` is an extremely fast graph-algorithms library.\n",
    "Here are a few \"grad school staples\" with short examples.\n",
    "\n",
    "---\n",
    "\n",
    "## J.1 k-core decomposition\n",
    "\n",
    "The k-core is the maximal subgraph where every node has degree ≥ k (within that subgraph).\n",
    "The **core number** of a node is the largest k for which it belongs to the k-core.\n",
    "\n",
    "Use cases:\n",
    "- identify \"network core\" vs periphery\n",
    "- robustness / percolation intuition\n",
    "- preprocessing for visualization and inference\n",
    "\n",
    "---\n",
    "\n",
    "## J.2 Minimum spanning tree (MST)\n",
    "\n",
    "For weighted connected graphs, the MST is a cycle-free subset of edges connecting all vertices with minimum total weight.\n",
    "\n",
    "Use cases:\n",
    "- extracting a \"backbone\" structure\n",
    "- simplifying dense weighted networks\n",
    "\n",
    "---\n",
    "\n",
    "## J.3 Maximum cardinality matching\n",
    "\n",
    "A matching is a set of edges with no shared vertices.\n",
    "Maximum matching finds the largest such set.\n",
    "\n",
    "Use cases:\n",
    "- assignment problems\n",
    "- bipartite matching\n",
    "- structural controllability intuition\n",
    "\n",
    "---\n",
    "\n",
    "## J.4 Quick connectivity checks\n",
    "\n",
    "- largest connected component\n",
    "- bipartite test\n",
    "- DAG test (directed acyclicity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = nx_to_gt(nx.karate_club_graph())\n",
    "# g = gt.GraphView(g, directed=False)\n",
    "\n",
    "# # --- k-core decomposition ---\n",
    "# core = gt.kcore_decomposition(g)\n",
    "# print(\"Core numbers (min, max):\", int(core.a.min()), int(core.a.max()))\n",
    "\n",
    "# plt.figure(figsize=(6.2, 3.2))\n",
    "# plt.hist(core.a, bins=np.arange(core.a.max() + 2) - 0.5, density=False)\n",
    "# plt.xlabel(\"core number\")\n",
    "# plt.ylabel(\"count\")\n",
    "# plt.title(\"k-core decomposition (Karate Club)\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Extract the 3-core (as an example)\n",
    "# k = 3\n",
    "# vfilt = g.new_vp(\"bool\")\n",
    "# vfilt.a = core.a >= k\n",
    "# g_kcore = gt.GraphView(g, vfilt=vfilt)\n",
    "# print(f\"{k}-core vertices:\", g_kcore.num_vertices(), \"| edges:\", g_kcore.num_edges())\n",
    "\n",
    "# # --- Minimum spanning tree (needs weights) ---\n",
    "# w = g.new_ep(\"double\")\n",
    "# rng = np.random.default_rng(0)\n",
    "# for e in g.edges():\n",
    "#     w[e] = float(rng.random())\n",
    "\n",
    "# tree = gt.min_spanning_tree(g, weights=w)  # boolean EdgePropertyMap\n",
    "# g_mst = gt.GraphView(g, efilt=tree)\n",
    "# print(\"MST edges:\", g_mst.num_edges())\n",
    "\n",
    "# # --- Maximum cardinality matching ---\n",
    "# matching = gt.max_cardinality_matching(g)\n",
    "# g_match = gt.GraphView(g, efilt=matching)\n",
    "# print(\"Matching size (#edges):\", g_match.num_edges())\n",
    "\n",
    "# # --- Quick checks ---\n",
    "# print(\"Is bipartite?\", gt.is_bipartite(g))\n",
    "# print(\"Largest component size:\", gt.label_largest_component(g).a.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df445e41",
   "metadata": {},
   "source": [
    "# Appendix K: Centrality (PageRank, betweenness, closeness, eigenvector) in graph-tool\n",
    "\n",
    "Community structure is one \"macro\" lens. Centrality is a complementary \"micro\" lens: it asks **which vertices are structurally important**, and *why*.\n",
    "\n",
    "`graph-tool` implements many standard centrality measures efficiently in C++.\n",
    "\n",
    "---\n",
    "\n",
    "## K.1 The math (definitions you should recognize)\n",
    "\n",
    "### Degree\n",
    "For an undirected graph, degree is:\n",
    "\n",
    "$$\n",
    "k_i = \\sum_j A_{ij}.\n",
    "$$\n",
    "\n",
    "It's fast and often surprisingly informative, but it's local.\n",
    "\n",
    "### Betweenness\n",
    "Betweenness counts how often a node lies on shortest paths:\n",
    "\n",
    "$$\n",
    "C_B(v) = \\sum_{s\\ne v\\ne t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}},\n",
    "$$\n",
    "\n",
    "where $\\sigma_{st}$ is the number of shortest paths from $s$ to $t$, and $\\sigma_{st}(v)$ is the number of those paths that pass through $v$.\n",
    "\n",
    "Interpretation: high betweenness nodes can act as \"bridges\" / bottlenecks.\n",
    "\n",
    "### Closeness\n",
    "Closeness centrality (connected graphs):\n",
    "\n",
    "$$\n",
    "C_C(v) = \\frac{N-1}{\\sum_{u\\ne v} d(v,u)}.\n",
    "$$\n",
    "\n",
    "For disconnected graphs, a more stable variant is **harmonic closeness**:\n",
    "\n",
    "$$\n",
    "C_H(v) = \\sum_{u\\ne v} \\frac{1}{d(v,u)},\n",
    "$$\n",
    "\n",
    "with the convention $1/\\infty = 0$.\n",
    "\n",
    "### Eigenvector centrality\n",
    "Eigenvector centrality solves:\n",
    "\n",
    "$$\n",
    "A\\,\\mathbf{x} = \\lambda\\,\\mathbf{x}.\n",
    "$$\n",
    "\n",
    "Nodes are important if they connect to other important nodes.\n",
    "\n",
    "### PageRank\n",
    "PageRank is a damped random-walk centrality:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\alpha\\,P^T\\mathbf{x} + (1-\\alpha)\\,\\frac{1}{N}\\mathbf{1},\n",
    "$$\n",
    "\n",
    "where $P$ is a transition matrix and $\\alpha$ is the damping factor (often 0.85).\n",
    "\n",
    "---\n",
    "\n",
    "## K.2 `graph-tool` API shape: property maps\n",
    "\n",
    "Centralities are returned as **VertexPropertyMaps**.\n",
    "\n",
    "- The property map itself behaves like a mapping from vertices to values.\n",
    "- The underlying NumPy array is usually available as `.a`.\n",
    "\n",
    "Below we compute centralities on Karate in both `graph-tool` and `networkx` and compare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = nx_to_gt(nx.karate_club_graph())\n",
    "# G_nx = gt_to_nx(g)\n",
    "\n",
    "# # --- graph-tool centralities ---\n",
    "# pr_gt = gt.pagerank(g, damping=0.85)                 # VertexPropertyMap\n",
    "# vb_gt, eb_gt = gt.betweenness(g, norm=True)          # (vertex vp, edge ep)\n",
    "# cl_gt = gt.closeness(g, norm=True)                   # VertexPropertyMap\n",
    "# ch_gt = gt.closeness(g, harmonic=True, norm=True)    # harmonic closeness\n",
    "# eigval, ev_gt = gt.eigenvector(g)                    # (largest eigenvalue, VertexPropertyMap)\n",
    "\n",
    "# # --- networkx centralities ---\n",
    "# pr_nx = nx.pagerank(G_nx, alpha=0.85)\n",
    "# vb_nx = nx.betweenness_centrality(G_nx, normalized=True)\n",
    "# cl_nx = nx.closeness_centrality(G_nx)\n",
    "\n",
    "# # align to a consistent node order\n",
    "# nodes = list(G_nx.nodes())\n",
    "# pr_nx_vec = np.array([pr_nx[v] for v in nodes])\n",
    "# vb_nx_vec = np.array([vb_nx[v] for v in nodes])\n",
    "# cl_nx_vec = np.array([cl_nx[v] for v in nodes])\n",
    "\n",
    "# # graph-tool vectors\n",
    "# pr_gt_vec = pr_gt.a\n",
    "# vb_gt_vec = vb_gt.a\n",
    "# cl_gt_vec = cl_gt.a\n",
    "\n",
    "# def spearman(x, y):\n",
    "#     # tiny Spearman rank correlation (no scipy.stats dependency)\n",
    "#     rx = pd.Series(x).rank().to_numpy()\n",
    "#     ry = pd.Series(y).rank().to_numpy()\n",
    "#     return np.corrcoef(rx, ry)[0, 1]\n",
    "\n",
    "# print(\"Spearman corr (PageRank):   \", round(spearman(pr_gt_vec, pr_nx_vec), 3))\n",
    "# print(\"Spearman corr (Betweenness):\", round(spearman(vb_gt_vec, vb_nx_vec), 3))\n",
    "# print(\"Spearman corr (Closeness):  \", round(spearman(cl_gt_vec, cl_nx_vec), 3))\n",
    "\n",
    "# # show top nodes by PageRank + betweenness (graph-tool)\n",
    "# top_pr = np.argsort(-pr_gt_vec)[:5]\n",
    "# top_vb = np.argsort(-vb_gt_vec)[:5]\n",
    "\n",
    "# print(\"\\nTop-5 PageRank nodes (graph-tool):\")\n",
    "# for i in top_pr:\n",
    "#     print(f\"  node {i:>2}  PR={pr_gt_vec[i]:.4f}\")\n",
    "\n",
    "# print(\"\\nTop-5 betweenness nodes (graph-tool):\")\n",
    "# for i in top_vb:\n",
    "#     print(f\"  node {i:>2}  betw={vb_gt_vec[i]:.4f}\")\n",
    "\n",
    "# # simple visualization: size nodes by PageRank\n",
    "# pos = gt.sfdp_layout(g)\n",
    "# gt.graph_draw(\n",
    "#     g, pos=pos,\n",
    "#     vertex_size=gt.prop_to_size(pr_gt, mi=6, ma=18),\n",
    "#     vertex_fill_color=pr_gt,\n",
    "#     output_size=(700, 650)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37952f6a",
   "metadata": {},
   "source": [
    "# Appendix L: Motifs and motif significance (triangles are just the start)\n",
    "\n",
    "Motifs are small subgraph patterns (triads, tetrads, ...) that can be:\n",
    "\n",
    "- descriptive (\"this network has lots of feed-forward loops\")\n",
    "- inferential (\"is this motif over-represented compared to a null model?\")\n",
    "\n",
    "---\n",
    "\n",
    "## L.1 Counting motifs\n",
    "\n",
    "`graph-tool` can count node-induced subgraphs of size $k$ via:\n",
    "\n",
    "```python\n",
    "motifs, counts = gt.motifs(g, k)\n",
    "```\n",
    "\n",
    "The returned `motifs` are themselves tiny graphs (one per motif type), and `counts[i]` tells you how often motif `i` occurs as an induced subgraph.\n",
    "\n",
    "---\n",
    "\n",
    "## L.2 Motif significance as a z-score\n",
    "\n",
    "To quantify \"over-representation\", a common approach is to compare motif counts to a null ensemble (often degree-preserving shuffles).\n",
    "\n",
    "Let:\n",
    "\n",
    "- $N_i$ = motif count in the observed graph\n",
    "- $N_i^{(s)}$ = motif count in shuffle $s$\n",
    "- $\\mu_i = \\mathbb{E}[N_i^{(s)}]$\n",
    "- $\\sigma_i = \\mathrm{sd}(N_i^{(s)})$\n",
    "\n",
    "Then a standard z-score is:\n",
    "\n",
    "$$\n",
    "z_i = \\frac{N_i - \\mu_i}{\\sigma_i}.\n",
    "$$\n",
    "\n",
    "Large $|z_i|$ means motif $i$ is unusually common (positive) or unusually rare (negative) relative to the chosen null model.\n",
    "\n",
    "`graph-tool` computes this directly via:\n",
    "\n",
    "```python\n",
    "motifs, z = gt.motif_significance(g, k=3, n_shuffles=100, shuffle_model=\"configuration\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## L.3 Example: motifs of size 3 on Karate (and their significance)\n",
    "\n",
    "This is meant as a \"how to do it\" template — for real motif work, choose $k$ and $n_\\text{shuffles}$ carefully (runtime grows quickly with $k$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx_to_gt(nx.karate_club_graph())\n",
    "\n",
    "# Count 3-node induced motifs\n",
    "motifs_3, counts_3 = gt.motifs(g, k=3)\n",
    "counts_3 = np.asarray(counts_3)\n",
    "\n",
    "print(\"Number of distinct 3-node motifs found:\", len(motifs_3))\n",
    "print(\"Counts:\", counts_3)\n",
    "\n",
    "# A simple way to summarize motifs without drawing:\n",
    "# show number of edges in each motif graph\n",
    "edges_in_motif = np.array([m.num_edges() for m in motifs_3])\n",
    "summary = pd.DataFrame({\n",
    "    \"motif_id\": np.arange(len(motifs_3)),\n",
    "    \"edges_in_motif\": edges_in_motif,\n",
    "    \"count\": counts_3.astype(int)\n",
    "}).sort_values([\"edges_in_motif\", \"count\"], ascending=[True, False])\n",
    "\n",
    "summary\n",
    "\n",
    "# Motif significance profile (keep n_shuffles small for classroom runtime)\n",
    "motifs_sig, zscores = gt.motif_significance(\n",
    "    g, k=3,\n",
    "    n_shuffles=30,\n",
    "    shuffle_model=\"configuration\"\n",
    ")\n",
    "\n",
    "zscores = np.asarray(zscores)\n",
    "edges_sig = np.array([m.num_edges() for m in motifs_sig])\n",
    "\n",
    "sig = pd.DataFrame({\n",
    "    \"motif_id\": np.arange(len(motifs_sig)),\n",
    "    \"edges_in_motif\": edges_sig,\n",
    "    \"z\": zscores\n",
    "}).sort_values(\"z\", ascending=False)\n",
    "\n",
    "sig\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.stem(sig[\"z\"].values)\n",
    "plt.xlabel(\"Motif type (sorted by z)\")\n",
    "plt.ylabel(\"z-score\")\n",
    "plt.title(\"Motif significance profile (k=3, configuration shuffles)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699580b-a4b1-49c8-9f8a-6dacd5e79483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
